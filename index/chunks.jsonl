{"text": "Research-driven Data Engineer with experience across academic and industry settings. Skilled in designing and building scalable datapipelines, implementing data quality frameworks, and optimizing ELT processes. Proficient in Python, Spark, SQL, and AWS, with hands-on experience in large-scale data environments. Collaborates effectively across teams to enable efficient data access, analysis, anddecision-making. Fluent in English (C1) and certified B1 in German; strong communicator and team player.", "source": "CV:01_cv_public.md", "section": "Summary", "hash": "92744a70"}
{"text": "Languages: Python, TypeScript, SQL, Java, C, C++, R, PySpark Frameworks: FastAPI, Streamlit ML/AI: scikit-learn, PyTorch (basics), RAG (Retrieval-Augmented Generation), FAISS, embeddings, Hugging Face (LLM), Vector Databases, Data Parsing & Extraction, sentence-transformers, nbformat, BeautifulSoup Cloud/DevOps: Docker, CI/CD (GitHub Actions, GitLab CI), Linux, AWS (certified), Azure DevOps, REST API, Terraform, GitPython Project Management: JIRA, Trello, Agile Methodologies IDEs & Code Editors: Visual Studio Code (VS Code), Jupyter Notebook Data Visualization & Business Intelligence: MS PowerBI Deployment / Web App Skills: Streamlit apps deployment, Hugging Face Spaces hosting, end-to-end AI pipeline management (ingestion → embedding → chatbot response)", "source": "CV:01_cv_public.md", "section": "Skills", "hash": "1882be1b"}
{"text": "- Associate Consultant Data Engineer,Arcondis GmbH (Jun 2025 – Aug 2025) Engineered an automated data pipeline to compute 30+ KPIs, integrating data from Monday.com via REST API and internal Abacus database using JDBC. Built a helper automation to scan and isolate relevant tables within large-scale Abacus database—optimized execution time to ~3–4 minutes. Fully automated the workflow end-to-end, enabling scheduled KPI updates with zero manual intervention and improved reporting cadence. Interacted with stakeholders to understand the requirements and to give weekly updates. - Online Tutor,Self Employed (Jan 2024 – May 2025) Provide expert tutoring and academic supervision to bachelor's and master's students in Data Engineering, Cloud Computing, DevOps, and Databases. Design and deliver structured lesson plans and presentations to facilitate comprehensive learning experiences. Conduct collaborative sessions, including pair programming, to enhance student engagement and foster an interactive learning environment. - Data Engineer,Roche Diagnostics GmbH (Feb 2023 – Jul 2023 Penzberg, Germany) Built a Python ETL pipeline for clinical pathological datasets (OCR, PDF, text) to detect data drift. Applied NLP and text mining techniques for transformation of unstructured to structured data. Utilized ML algorithms, with Word2Vec enhancing results by 99.5%. Implemented Cosine similarity vectors to analyze and quantify differences across reports. Automated deployment processes using DevOps tools (Git, AWS, Docker), optimizing efficiency and scalability. - Apprentice Data Engineer,Roche Diagnostics GmbH (Jun 2022 – Aug 2022 Penzberg, Germany) Collaborated in Agile development through daily stand-ups and sprint planning. Established a cloud-based ETL data pipeline using AWS Glue, S3, and Athena. Utilized SQL optimization techniques in Athena for ad hoc data analysis. Used PySpark for ETL into a centralized data lake. Boosted uptime from 48% to 87% using Amazon CloudWatch monitoring. - Software Engineer,Fortress6 Technologies (Jun 2019 – May 2021, India) Collaborated with project managers, engineers, and stakeholders to support ongoing projects. Reduced data retrieval time by 60% through SQL optimization Automated AWS microservice deployment using Terraform (IaC) for 13+ ISPs. Implemented CI/CD pipelines using Git for 60+ clients. Mentored 5 junior developers, improving code quality by 25%. Enhanced backend development, positively impacting company performance.", "source": "CV:01_cv_public.md", "section": "Experience", "hash": "d86d50c2"}
{"text": "- MSc. Data Engineering, Jacobs (Constructor) University, Bremen, Germany - B.Tech Computer Science & Engineering, Uttarakhand Technical University, India", "source": "CV:01_cv_public.md", "section": "Education", "hash": "1989d89d"}
{"text": "- Recipient of 100% scholarship for MSc: Roche Cooperative Study Program - Academic merit scholarship: Jacobs University Bremen - AWS Cloud Computing and Deployment: WebTek Labs Pvt. Ltd. - A+ in MySQL training from Microsoft, ranking among the top 5%: Microsoft Technology Associate", "source": "CV:01_cv_public.md", "section": "AWARDS/ CERTIFICATIONS", "hash": "69710e1b"}
{"text": "from src.math_operations import add, sub def test_add(): assert add(2,3)==5 assert add(-4,5)==1 assert add(9,1)==10 def test_sub(): assert sub(2,3)==-1 assert sub(-4,5)==-9 assert sub(9,1)==8", "source": "Repo:CI_unit_test:tests/test_operations.py", "section": "CI_unit_test", "hash": "c099c4cf"}
{"text": "{ \"Products\": [ { \"Title\": \"Boots Sleepeaze Tablets 50 mg - 20s\", \"Price\": \"£5.79\", \"Price_Unit\": \"£\", \"Offer\": \"Save 10 percent on selected Boots Pharmacy Medicines - Advantage Card Holders only\" }, { \"Title\": \"Boots Sleepeaze Herbal Plus Tablets - 30 Tablets\", \"Price\": \"£5.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Tablets 25 mg - 20s\", \"Price\": \"£3.90\", \"Price_Unit\": \"£\", \"Offer\": \"Save 10 percent on selected Boots Pharmacy Medicines - Advantage Card Holders only\" }, { \"Title\": \"Feather and Down Perfect Partners\", \"Price\": \"£7.50\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Botanics Peaceful Night Sleep Duo Gift Set\", \"Price\": \"£8.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kalms Night One-a-Night - 28 Tablets\", \"Price\": \"£9.99\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Tisserand The Little Box of Sleep 3 x 10ml\", \"Price\": \"£13.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Bach Rescue Remedy Night Liquid Melts 28 Capsules\", \"Price\": \"£7.00\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Nytol One-A-Night 50mg Tablets - 20 Tablets\", \"Price\": \"£8.60\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Nytol Herbal Simply Sleep One-A-Night Tablets\", \"Price\": \"£7.35\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Kalms Night - 56 Tablets\", \"Price\": \"£5.99\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Feather & Down Sweet Dreams Sleeping Bag Gift Set\", \"Price\": \"£14.25\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Kira Restful Sleep Valerian Root Extract 300 mg Tablets - 25 Tablets\", \"Price\": \"£6.90\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Lavender Pillow Mist 100ml\", \"Price\": \"£6.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Rescue Remedy Gummies Night 60s\", \"Price\": \"£14.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Botanics Pillow Mist Lavender & Sweet Marjoram 100ml\", \"Price\": \"£7.50\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Oral Strips - 21 Strips\", \"Price\": \"£7.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Bach Rescue Night Dropper 20ml\", \"Price\": \"£8.33\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Champneys Sleep Pillow Mist 50ml\", \"Price\": \"£12.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Dreamland Intelliheat Warming Throw - Grey\", \"Price\": \"£60.00\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Dreamland Intelliheat Faux Fur Warming Throw - Alaskan Husky\", \"Price\": \"£99.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Boots Soft Silicone Earplugs - 3 Pairs\", \"Price\": \"£3.96\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Boots Mouldable Wax Earplugs - 5 Pairs\", \"Price\": \"£2.16\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Soap & Glory Perfect Zen Calming Bath Milk 500ml\", \"Price\": \"£6.99\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"This Works Deep Sleep™ Pillow Spray 75ml\", \"Price\": \"£21.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Dreamland Intelliheat Throw - Teal\", \"Price\": \"£59.99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"Lumie Sunrise Alarm\", \"Price\": \"£49.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Soap & Glory Perfect Zen Foaming Shower Oil 200ml\", \"Price\": \"£8.99\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Baylis & Harding Goodness Sleep Lavender & Bergamot Sleep Bath Soak 500ml\", \"Price\": \"£5.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 14ml\", \"Price\": \"£5.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Herringbone Throw - Emerald Green\", \"Price\": \"£74.99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"This Works Deep Sleep™ Pillow Spray 250ml\", \"Price\": \"£37.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Soap & Glory Perfect Zen Body Souffle 300ml\", \"Price\": \"£9.99\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Dreamland Hurry Home Warming Throw - Grey 160x120\", \"Price\": \"£74.99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"Soap & Glory Perfect Zen Warming Body Scrub 250ml\", \"Price\": \"£8.99\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Snoreeze Snoring Relief Oral Strips - 14 Applications\", \"Price\": \"£8.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Snugsie Giant Blanket Neutral\", \"Price\": \"£32.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Luxe Platinum/ Orchid\", \"Price\": \"£99.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Baylis & Harding Goodness Sleep Lavender & Bergamot Sleep Pillow Mist 100ml\", \"Price\": \"£7.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Feather & Down Sweet Dreams Sleep Essentials Set\", \"Price\": \"£14.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 42ml\", \"Price\": \"£14.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Soap & Glory The Rest Assured Sleep Mask\", \"Price\": \"£7.00\", \"Price_Unit\": \"£\", \"Offer\": \"Buy 1 get 2nd 1/2 price on selected Soap And Glory\" }, { \"Title\": \"Botanics Pure Essential Oil Lavender 10ml\", \"Price\": \"£6.50\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Naptime Warming Sherpa Blanket - Tartan Check Red.", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "728911d1"}
{"text": "{ \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 42ml\", \"Price\": \"£14.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Soap & Glory The Rest Assured Sleep Mask\", \"Price\": \"£7.00\", \"Price_Unit\": \"£\", \"Offer\": \"Buy 1 get 2nd 1/2 price on selected Soap And Glory\" }, { \"Title\": \"Botanics Pure Essential Oil Lavender 10ml\", \"Price\": \"£6.50\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Naptime Warming Sherpa Blanket - Tartan Check Red. Grey 180X135\", \"Price\": \"£89.99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"Feather & Down Sweet Dreams Melting Shower Cream 250ml\", \"Price\": \"£7.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Congestion Nasal Strips - 20 Strips\", \"Price\": \"£9.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Revive Me Multipurpose Heat Pad Standard Size\", \"Price\": \"£39.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hygge Days Faux Fur Warming Throw - Fallow Deer\", \"Price\": \"£99.99\", \"Price_Unit\": \"£\", \"Offer\": \"Only £99.99 on selected Dreamland\" }, { \"Title\": \"Snoreeze Snoring Relief Oral Device\", \"Price\": \"£37.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Feather & Down Pillow Spray 200ml\", \"Price\": \"£14.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Nytol Herbal Tablets 30 tablets\", \"Price\": \"£6.20\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Congestion Nasal Spray - 10ml\", \"Price\": \"£11.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Snoreeze Lozenges - 16 lozenges\", \"Price\": \"£9.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Herringbone Throw - Navy Blue\", \"Price\": \"£74.99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"BioEars Soft Silicone Earplugs with Activ Aloe - 3 pairs\", \"Price\": \"£5.55\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Feather & Down Soothing Sleep Butter 300ml\", \"Price\": \"£10.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Westlab Sleep Epsom Bath Salts with Lavender 1kg\", \"Price\": \"£7.49\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Botanics Pure Essential Oil Lavender 20ml\", \"Price\": \"£11.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Alpine Sleepdeep Sleeping Earplugs 1 Pair\", \"Price\": \"£12.95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Deep Sleep™ Bath Soak 200g\", \"Price\": \"£24.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Bach Rescue Remedy Night Spray 20ml – Flower Essences for Natural Night's Sleep\", \"Price\": \"£8.40\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Mute Medium - 3 pack (30 Night Supply).\", \"Price\": \"£19.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Rescue Balance & Positivity Capsules 30s\", \"Price\": \"£13.97\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Bach Rescue Peaceful Night Capsules 30s\", \"Price\": \"£10.63\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Snoreeze Snoring Relief Nasal Strips Small/Medium - 20 Applications\", \"Price\": \"£10.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Snoreeze Snoring Relief Nasal Spray 10ml\", \"Price\": \"£13.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Snugsie Wearable Blanket Silver\", \"Price\": \"£18.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Pillow Spray 30ml\", \"Price\": \"£19.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Silentnight Geltex Pillow\", \"Price\": \"£30.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw Pink 120X160Cm\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Roller Ball - 10ml\", \"Price\": \"£8.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Baylis & Harding Goodness Sleep Lavender & Bergamot Sleep Body Wash 500ml\", \"Price\": \"£5.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"BetterYou Magnesium Sleep Body Lotion - 180ml\", \"Price\": \"£9.99\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"This Works Deep Sleep Pillow Spray 35ml\", \"Price\": \"£14.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Nytol Original Tablets 25mg - 20 Tablets\", \"Price\": \"£6.30\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Alpine Sleepdeep Mini Sleeping Earplugs 1 Pair\", \"Price\": \"£12.95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Giant Blanket Green\", \"Price\": \"£32.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Silent Power Pure Air Fan Heater\", \"Price\": \"£119.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Silentnight Snugsie Wearable Blanket Blush\", \"Price\": \"£18.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"BetterYou Magnesium Sleep Flakes - 750g\", \"Price\": \"£6.99\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Aroma Active Laboratories Sleep Salt Soak 500g\", \"Price\": \"£7.50\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/2 price across the Aroma Active range - online only\" }, { \"Title\": \"Aroma Active Laboratories Sleep Over Night Recovery Face Oil 30ml\", \"Price\": \"£7.50\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/2 price across the Aroma Active range - online only\" }, { \"Title\": \"Bach Rescue Remedy Night Dropper 10ml - Flower Essences for Natural Night's Sleep\", \"Price\": \"£6.33\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Feather & Down Sweet Dreams Bath Essence 500ml\", \"Price\": \"£8.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Nytol Herbal Simply Sleep & Calm Elixir - 100ml\", \"Price\": \"£7.90\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Revive Me Back Heat Pad 5T 61X38 Cm\", \"Price\": \"£59.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hygge Days Faux Fur Warming Throw - Zebra\", \"Price\": \"£99.99\", \"Price_Unit\": \"£\", \"Offer\": \"Only £99.99 on selected Dreamland\" }, { \"Title\": \"This Works Deep Sleep™ Heavenly Candle\", \"Price\": \"£26.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Pillow Mist\", \"Price\": \"£13.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Snoreeze Snoring Relief Nasal Strips Large - 20 Applications\", \"Price\": \"£10.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Snugsie Wearable Blanket Charcoal\", \"Price\": \"£18.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Mattress Protector King\", \"Price\": \"£19.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Pillow Spray 100ml\", \"Price\": \"£35.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector King 2 Controls 200X150Cm\", \"Price\": \"£149.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer King 2 Controls 200X150Cm\", \"Price\": \"£124.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Double 1 Control 190X137Cm\", \"Price\": \"£99.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aroma Active Laboratories Sleep Pulsepoint Rollerball 6ml\", \"Price\": \"£4.00\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/2 price across the Aroma Active range - online only\" }, { \"Title\": \"Silentnight V Shaped Support Pillow\", \"Price\": \"£17.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Serenity Weighted Eye Mask\", \"Price\": \"£17.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kalms Rhodiola Tablets- 20 Tablets\", \"Price\": \"£10.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Bath Oil 200ml\", \"Price\": \"£16.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw - Mustard 120X160Cm\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Silent Power Eco Fan Heater\", \"Price\": \"£94.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Dreamland Peaceful Dreams Warming Over Blanket Double Dual 6T 180X180\", \"Price\": \"£114.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Alpine Sleepdeep Multi Size Sleeping Earplugs 2 Pairs\", \"Price\": \"£17.95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Pillow\", \"Price\": \"£29.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Healthspan Night Time CBD Oil 260mg 10ml\", \"Price\": \"£19.95\", \"Price_Unit\": \"£\", \"Offer\": \"£18.95with Advantage Card\" }, { \"Title\": \"Dreamland Silent Power Comfort\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Dreamland Peaceful Dreams Warming Over Blanket Single 6T 180X135 Cm\", \"Price\": \"£84.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Double 2 Controls 190X137Cm\", \"Price\": \"£114.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Peaceful Dreams Warming Over Blanket King Dual 6T 215X225\", \"Price\": \"£124.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Indulgent Days Soft Velvet Warming Throw Geometric Peacock 120 X 160 Cm\", \"Price\": \"£79.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Versa 3 Black\", \"Price\": \"£169.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Silentnight Snugsie Kids Glow In The Dark Hoody Denim\", \"Price\": \"£22.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Oversized Hoody Charocal\", \"Price\": \"£27.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Wearable Blanket Sage\", \"Price\": \"£18.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Sealy Side Sleeper Pillow\", \"Price\": \"£25.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Superking 2 Controls 200X180Cm\", \"Price\": \"£139.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Single 190X90Cm\", \"Price\": \"£79.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Deep Sleep™ Night Oil 120ml\", \"Price\": \"£26.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Feather & Down Breathe Well Pillow Spray\", \"Price\": \"£8.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Aromatherapy Associates Deep Relax Roller Ball 10ml\", \"Price\": \"£25.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector Double 1 Control 190X137Cm\", \"Price\": \"£124.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aromatherapy Associates Deep Relax Sleep Mist 50ml\", \"Price\": \"£32.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Klearvol Pillow Spray 100ml\", \"Price\": \"£9.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Klearvol Vapour Rub 50ml\", \"Price\": \"£4.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Dreamer Superior Cotton Mattress Warmer Single 150X80 Cm\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Supersoft Thermal Mattress Cover Double\", \"Price\": \"£19.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Heat Genie Self Heating Mattress Topper King\", \"Price\": \"£39.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Heat Genie Self Heating Mattress Topper Single\", \"Price\": \"£28.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Oversized Hoody Charocal Glitter\", \"Price\": \"£27.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Deep Sleep Duvet 13.5 Tog Double\", \"Price\": \"£34.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Klearvol Vapour Rub & Pillow Spray Bundle\", \"Price\": \"£13.48\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"NytEase Stress + Tension Support Pillow Spray - 100ml\", \"Price\": \"£7.33\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Serenity Sweet Dreams Gift Set - Eye Mask, Pillow Mist and Ear Plugs Set\", \"Price\": \"£14.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Alpine Soft Silicone Earplugs 3 Pairs\", \"Price\": \"£8.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Massage Relief 10ml\", \"Price\": \"£15.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Cowshed Sleepy Bath Salts 300g\", \"Price\": \"£24.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Dreamland Naptime Warming Sherpa Blanket Beige Tartan Check 180X135 Cm\", \"Price\": \"£109.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector Single 190X90Cm\", \"Price\": \"£99.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Squishy Body Support Pillow\", \"Price\": \"£28.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Cool Touch Pillow\", \"Price\": \"£30.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Body Pillow - Pure White\", \"Price\": \"£59.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Acid Reflux Wedge Pillow\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Headspace Mind Giftcard - 6 months Pre-Paid Membership\", \"Price\": \"£30.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Cowshed Sleepy Calming Pillow Mist 100ml\", \"Price\": \"£18.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Tisserand Aromatherapy Dream Bath Sleep Better Bathtime Collection\", \"Price\": \"£15.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector Superking 2 Controls 200X180Cm\", \"Price\": \"£169.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw - Navy 120X160Cm\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aromatherapy Associates Relax Body Oil 100ml\", \"Price\": \"£52.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw - Black 120X160Cm\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Vegan Pillow Spray 10ml\", \"Price\": \"£12.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Silentnight Heat Genie Self Heating Mattress Topper Super King\", \"Price\": \"£44.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Deep Sleep Duvet 13.5 Tog King\", \"Price\": \"£39.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Duvet - 7.5 Tog - Double\", \"Price\": \"£25.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Duvet - 7.5 Tog - Single\", \"Price\": \"£23.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Re-balance Pillow Pair\", \"Price\": \"£23.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Honeycomb Cooling Pillow\", \"Price\": \"£54.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Twinings Superblends Unwind Tea Bags 20s\", \"Price\": \"£2.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Headspace Sleep Giftcard - 6 months Pre-Paid Membership\", \"Price\": \"£30.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Silent Power Protection Fan Heater\", \"Price\": \"£94.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Kally Sleep Ultimate Side Sleeper Pillow\", \"Price\": \"£49.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector Double 2 Controls 190X137Cm\", \"Price\": \"£139.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hurry Home Warming Throw - Mustard 160X120\", \"Price\": \"£94.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Feather & Down Sweet Dreams Sleep Butter 300ml\", \"Price\": \"£10.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Alpine Sleepsoft Earplugs 1 Pair\", \"Price\": \"£11.95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aromatherapy Associates Deep Relax Sleep Mist 10ml\", \"Price\": \"£25.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Candle\", \"Price\": \"£20.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Cosy Up Silky Soft Faux Fur Warming Throw Pink 160X120 Cm\", \"Price\": \"£109.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Sense Lunar White\", \"Price\": \"£219.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Silentnight Luxury Anti-Snore Pillow\", \"Price\": \"£22.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aromatherapy Associates 3 Step Introduction to Sleep\", \"Price\": \"£45.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Heat Genie Self Heating Mattress Topper Double\", \"Price\": \"£37.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Kids Glow In The Dark Hoody Blush\", \"Price\": \"£22.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Oversized Hoody Blush\", \"Price\": \"£27.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Sealy Dual Comfort Pillow\", \"Price\": \"£40.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Sealy Airflow Pillow\", \"Price\": \"£37.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Mattress Topper 8cm Super King\", \"Price\": \"£64.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Mattress Topper 8cm King\", \"Price\": \"£59.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Mattress Topper 8cm Double\", \"Price\": \"£49.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Mattress Topper 8cm Single\", \"Price\": \"£42.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Support Pillow Pair\", \"Price\": \"£29.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnightt Impress Memory Foam Mattress Topper Super King\", \"Price\": \"£149.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Mattress Topper King\", \"Price\": \"£139.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Mattress Topper Double\", \"Price\": \"£129.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Mattress Topper Small Double\", \"Price\": \"£124.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Mattress Topper Single\", \"Price\": \"£109.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Deep Sleep Duvet 13.5 Tog Super King\", \"Price\": \"£44.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Deep Sleep Duvet 13.5 Tog Single\", \"Price\": \"£26.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy Pillow Pair\", \"Price\": \"£17.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy 15 Tog Duvet Super King\", \"Price\": \"£49.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Healthy Growth Waterproof Mattress Protector Double\", \"Price\": \"£17.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Healthy Growth Waterproof Mattress Protector Single\", \"Price\": \"£14.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Carmen Kingsize Heated Under Blanket\", \"Price\": \"£55.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Dreamer Superior Mattress Warmer King Dual 150x160 CM\", \"Price\": \"£119.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Dreamer Superior Cotton Mattress Warmer Double Dual 150X137 Cm\", \"Price\": \"£99.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Up Silky Soft Faux Fur Warming Throw Terracotta 160 X 120 Cm\", \"Price\": \"£109.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Summer Breeze Duvet - 2.5 Tog - King\", \"Price\": \"£24.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Summer Breeze Duvet - 2.5 Tog - Double\", \"Price\": \"£21.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Summer Breeze Duvet - 2.5 Tog - Single\", \"Price\": \"£18.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Cooler Summer Pillow - 2 Pack\", \"Price\": \"£18.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Protector - King\", \"Price\": \"£19.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Protector - Double\", \"Price\": \"£18.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Topper - King\", \"Price\": \"£28.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Protector - Small Double\", \"Price\": \"£17.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Topper - Double\", \"Price\": \"£25.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Topper - Single\", \"Price\": \"£20.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Duvet - 7.5 Tog - King\", \"Price\": \"£28.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Pillow - 2 Pack\", \"Price\": \"£18.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Lavender Scented Pillow\", \"Price\": \"£19.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Copper Pillow\", \"Price\": \"£19.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Anti-Snore Pillow\", \"Price\": \"£39.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Cooling Mattress Topper - King\", \"Price\": \"£89.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Cooling Pillow\", \"Price\": \"£49.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Sherpa Fleece Body Pillow - Pink\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Cooling Mattress Topper - Double\", \"Price\": \"£79.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Copper Mattress Topper - Double\", \"Price\": \"£64.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Sherpa Fleece Body Pillow - Grey\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Sherpa Fleece Body Pillow - Cream\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Dream Body Lotion 50ml\", \"Price\": \"£28.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Boots Soft Disposable Ear Plugs\", \"Price\": \"£5.60\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Soundasleep Speaker Pillow\", \"Price\": \"£29.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Deep Sleep™ Pillow Talk Set\", \"Price\": \"£27.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Silentnight Snugsie Giant Blanket Charcoal\", \"Price\": \"£32.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Massage & Body Oil 100ml\", \"Price\": \"£11.00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Revive Me Comfy Foot Warmer 5T 50 X 48\", \"Price\": \"£59.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Up Silky Soft Faux Fur Warming Throw - Cream 160X120 Cm\", \"Price\": \"£109.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Cooler Summer Duvet - 7.5 Tog - King\", \"Price\": \"£26.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kalms Night Valerian Root Extract 96mg - 50 Tablets\", \"Price\": \"£4.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Lumie Bodyclock Spark 100 wake-up light alarm clock\", \"Price\": \"£99.00\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Feather & Down Pillow Spray Duo\", \"Price\": \"£4.87\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Feather & Down Sweet Dreams Pillow Spray 100ml\", \"Price\": \"£8.00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Yankee Candle Sleep Diffuser - Silver Peaceful Dream\", \"Price\": \"£39.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Twinings Superblends Sleep - 30g\", \"Price\": \"£2.69\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Bach Rescue Peaceful Night Duo Capsule & Spray\", \"Price\": \"£10.00\", \"Price_Unit\": \"£\", \"Offer\": \"Clearance - when its gone its gone\" }, { \"Title\": \"EarHub Soft Foam Earplugs 10 Pairs\", \"Price\": \"£4.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Charge 4 - Rosewood\", \"Price\": \"£129.99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Snoreeze Snoring Relief Throat Spray 23.5ml\", \"Price\": \"£9.80\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Foam Ear Plugs - 20s\", \"Price\": \"£5.76\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Boots Foam Earplugs - 3 Pairs with Carry Case\", \"Price\": \"£2.80\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Silentnight Supersoft Thermal Mattress Cover King\", \"Price\": \"£20.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Supersoft Thermal Mattress Cover Single\", \"Price\": \"£17.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Mediflow The Water Pillow\", \"Price\": \"£39.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy 15 Tog Duvet King\", \"Price\": \"£42.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy 15 Tog Duvet Double\", \"Price\": \"£37.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy 15 Tog Duvet Single\", \"Price\": \"£32.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Mattress Protector Double & Pillow Pair\", \"Price\": \"£32.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Mattress Protector Single & Pillow Pair\", \"Price\": \"£26.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Pillow Protector Pair\", \"Price\": \"£14.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Mattress Protector Double\", \"Price\": \"£17.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Carmen Double Heated Under Blanket\", \"Price\": \"£49.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Cooler Summer Duvet - 7.5 Tog - Double\", \"Price\": \"£23.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Cooler Summer Duvet - 7.5 Tog - Single\", \"Price\": \"£21.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Serenity Cosy Up Gift Set - Mini Hot Water Bottle and Scented Candle Set\", \"Price\": \"£17.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Serenity Erase and Rewind Gift Set - Hot Water Bottle and Eye Mask Set\", \"Price\": \"£22.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Weighted Eye Mask\", \"Price\": \"£11.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Knee Pillow\", \"Price\": \"£29.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Neck Pain Pillow\", \"Price\": \"£49.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Pillow Heathered Grey\", \"Price\": \"£59.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 42ml\", \"Price\": \"£14.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 14ml\", \"Price\": \"£5.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Kally Sleep Body Pillow - Stone Blue\", \"Price\": \"£59.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Anti-Ageing Copper Pillow\", \"Price\": \"£29.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Copper Mattress Topper - King\", \"Price\": \"£74.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep TENCEL™ Cooling Pillows - Twin Pack\", \"Price\": \"£49.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Sports Recovery Pillow - Blue\", \"Price\": \"£69.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Withings Sleep Analyzer Under-Mattress\", \"Price\": \"£129.95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Skinnydip x Sophie Hannah Good Night Sleep Spray and Eyemask Set\", \"Price\": \"£16.00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Tisserand Sleep Better Diffuser Oil 9ml\", \"Price\": \"£9.50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Treets Wellbeing Calming Pillow Mist 130ml\", \"Price\": \"£4.99\", \"Price_Unit\": \"£\", \"Offer\": \"\" } ], \"Median\": 23.0 }", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "de6d6a15"}
{"text": "#!/usr/bin/env python # coding: utf-8 # In[1]: import unittest from bs4 import BeautifulSoup import json def calculate_median(prices): sorted_prices = sorted(prices) n = len(sorted_prices) if n % 2 == 0: median = (sorted_prices[n // 2 - 1] + sorted_prices[n // 2]) / 2 else: median = sorted_prices[n // 2] return median class TestWebScraping(unittest.TestCase): def setUp(self): # Load the HTML content from a sample file with open('orignal.html', 'r', encoding='utf-8') as file: self.html_content = file.read() # Parse the HTML content using BeautifulSoup self.soup = BeautifulSoup(self.html_content, 'html.parser') def test_scrape_data(self): result_list = [] prices = [] # Fetching all the products information from the webpage products_items = self.soup.find_all('div', class_='oct-grid__row oct-grid__row--full-width oct-listers-hits') # Loop to find required item from the product list for item in products_items: # Finding Title from the products_items try: item_titles = item.find_all('h3', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--h3--variant-2 oct-teaser__title oct-teaser-with-listers'}) except: print(\"\") # Finding Price from the products_items try: item_prices = item.find_all('p', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--p--variant-subtext oct-teaser__productPrice'}) except: print(\"\") # Finding Offer from the products_items try: item_offer = item.find_all('div', {'class': 'oct-teaser__wrap'}) except: print(\"\") # Running a loop to capture information of each product for title, price, offer in zip(item_titles, item_prices, item_offer): title_text = title.get_text(strip=True) price_text = price.get_text(strip=True) offer_text = offer.get_text(strip=True) # \"result_list\" accumulates these dictionaries for all products on the page result_list.append({'Title': title_text, 'Price': price_text, 'Price_Unit': '£', 'Offer': offer_text}) # Extract numerical prices for median calculation numeric_price = float(price_text.replace('£', '').replace(',', '')) prices.append(numeric_price) # Assertions self.assertTrue(result_list) # Check that result_list is not empty self.assertTrue(prices) # Check that prices list is not empty self.assertEqual(len(result_list), len(prices)) # Check that lengths match def test_calculate_median(self): # Test calculate_median function with various inputs self.assertEqual(calculate_median([1, 2, 3]), 2) self.assertEqual(calculate_median([1, 2, 3, 4]), 2.5) self.assertEqual(calculate_median([5, 2, 8, 1, 9]), 5) if __name__ == '__main__': unittest.main() # In[ ]:", "source": "Repo:Web-Scraping:test_web_scraping.py", "section": "Web-Scraping", "hash": "96505c32"}
{"text": "<h1>Boots Website Scraper</h1> <p>This Python script is designed to scrape product information from the Boots website. It utilizes the BeautifulSoup library for HTML parsing and requests to send HTTP requests to the server. Because the request for the website content was unsuccessful, a copy of the website in .html format was obtained and used instead.</p> <h2>Prerequisites</h2> <p>Before running the script, make sure you have the required Python libraries installed. You can install them using the following command:</p> <pre><code>pip install beautifulsoup4</code></pre> <h2>Usage</h2> <ol> <li>Clone the repository:</li> <pre><code>git clone https://github.com/your-username/Web-Scraping.git</code></pre> <li>Navigate to the project directory:</li> <pre><code>cd Web-Scraping</code></pre> <li>Run the script to download the JSON file directly:</li> <pre><code>python boot_web_scraping.py</code></pre> </ol> <h2>Functions</h2> <h3><code>calculate_median(prices)</code></h3> <p>Calculates the median value of a list of prices.</p> <h4>Parameters</h4> <ul> <li><code>prices</code> (list of float): A list of numerical values representing prices.</li> </ul> <h4>Returns</h4> <ul> <li><code>float</code>: The median value of the provided prices.</li> </ul> <h3><code>scrape_boots_website(url)</code></h3> <p>Scrapes product information from the Boots website.</p> <h4>Parameters</h4> <ul> <li><code>url</code> (str): The URL of the Boots website page to be scraped.</li> </ul> <h4>Returns</h4> <ul> <li><code>BeautifulSoup</code>: A BeautifulSoup object representing the parsed HTML content of the webpage.</li> </ul> <h2>Script Flow</h2> <ol> <li>The script opens the <code>orignal.html</code> file and retrieves the HTML content.</li> <li>The HTML content is then parsed using BeautifulSoup.</li> <li>Product information, including Title, Price, and Offer, is extracted from the parsed HTML. As the Price_Unit was hard coded, so no extraction needed there.</li> <li>The script calculates the median price from the extracted numerical prices.</li> <li>The results are saved in a JSON file named <code>output.json</code>.</li> </ol> <h2>File Input</h2> <p>The script expects an HTML file named <code>orignal.html</code> in the project directory. Make sure to provide the correct file path or name in the script.</p> <h2>File Output</h2> <p>The script generates a JSON file named <code>output.json</code> containing a list of products and their details, along with the calculated median price.</p> # Web Scraping Unit Test This unit test script is designed to validate the functionality of the Boots Website Scraper. It uses the `unittest` library to conduct tests on the scraping and calculation functions. ## Test Cases ### Test Case 1: `test_scrape_data` This test ensures that the scraping function retrieves data from the HTML content correctly. It checks the following: 1. `result_list` is not empty. 2. `prices` list is not empty. 3. The lengths of `result_list` and `prices` match. ### Test Case 2: `test_calculate_median` This test verifies the correctness of the `calculate_median` function by testing it with different inputs.", "source": "Repo:Web-Scraping:README.md", "section": "Web-Scraping", "hash": "61473870"}
{"text": "Scraper. It uses the `unittest` library to conduct tests on the scraping and calculation functions. ## Test Cases ### Test Case 1: `test_scrape_data` This test ensures that the scraping function retrieves data from the HTML content correctly. It checks the following: 1. `result_list` is not empty. 2. `prices` list is not empty. 3. The lengths of `result_list` and `prices` match. ### Test Case 2: `test_calculate_median` This test verifies the correctness of the `calculate_median` function by testing it with different inputs. ## How to Run Tests 1. Ensure you have the required Python libraries installed. You can install them using the following command: ```bash pip install beautifulsoup4 ``` 2. Clone the repository: ```bash git clone https://github.com/your-username/Web-Scraping.git ``` 3. Navigate to the project directory: ```bash cd Web-Scraping ``` 4. Run the unit tests: ```bash python test_web_scraping.py ``` ## Additional Information - The HTML content for testing is loaded from the `orignal.html` file in the project directory. - The script uses BeautifulSoup for HTML parsing and includes a `calculate_median` function for calculating the median value of a list of prices. - The results of the scraping are stored in a list named `result_list`. - The numerical prices are stored in a list named `prices`. - The script includes assertions to ensure the correctness of the scraping and calculation functions.", "source": "Repo:Web-Scraping:README.md", "section": "Web-Scraping", "hash": "b77b81c8"}
{"text": "## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell ```python # Calculate the median price median_price = calculate_median(prices) # Save the information in a JSON file output_data = { 'Products': result_list, 'Median': median_price } ``` ## Code Cell ```python # Dumping the JSON file, named \"output.json\" with open('output.json', 'w', encoding='utf-8') as output_file: json.dump(output_data, output_file, indent=2, ensure_ascii=False) ```", "source": "Repo:Web-Scraping:boot_web_scraping.ipynb", "section": "Web-Scraping", "hash": "8ce8f936"}
{"text": "#!/usr/bin/env python # coding: utf-8 # In[1]: from bs4 import BeautifulSoup # For collecting data import json # To create a JSON file import requests # For sending request to the server to get data # In[2]: # Define a function to calculate median value def calculate_median(prices): \"\"\" Calculates the median value of a list of prices. Parameters: - prices (list of float): A list of numerical values representing prices. Returns: - float: The median value of the provided prices. \"\"\" sorted_prices = sorted(prices) n = len(sorted_prices) if n % 2 == 0: median = (sorted_prices[n//2 - 1] + sorted_prices[n//2]) / 2 else: median = sorted_prices[n//2] return median # Use this function with a working URL def scrape_boots_website(url): \"\"\" Scrapes product information from the Boots website. Parameters: - url (str): The URL of the Boots website page to be scraped. Returns: - BeautifulSoup: A BeautifulSoup object representing the parsed HTML content of the webpage. Example: >>> url = \"https://www.boots.com/health-pharmacy/medicines-treatments/sleep?paging.index=0&amp;paging.size=24&amp;sortBy=mostRelevant&amp;criteria.category=wellness---sleep&amp;criteria.brand=Bach+Rescue+Remedy---Boots\" >>> soup = scrape_boots_website(url) >>> # Countinue with Ln [4] \"\"\" response = requests.get(url) soup = BeautifulSoup(response.text, 'html.parser') return soup # In[3]: # Give file name or path, depending on the location of your file file_path = 'orignal.html' # Read the contents of the HTML file with open(file_path, 'r', encoding='utf-8') as file: html_content = file.read() # Parsing the HTML content to extract different data's soup = BeautifulSoup(html_content, 'html.parser') # In[4]: # Creating two list for storing respected values result_list = [] # For appending all the items prices = [] # For appending all the \"numeric_price\" # Fetching all the products information from the webpage products_items = soup.find_all('div', class_='oct-grid__row oct-grid__row--full-width oct-listers-hits') # Loop to find required item from the product list for item in products_items: # Finding Title from the products_items try: item_titles = item.find_all('h3', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--h3--variant-2 oct-teaser__title oct-teaser-with-listers'}) except: print(\"\") # Finding Price from the products_items try: item_prices = item.find_all('p', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--p--variant-subtext oct-teaser__productPrice'}) except: print(\"\") # Finding Offer from the products_items try: item_offer = item.find_all('div', {'class': 'oct-teaser__wrap'}) except: print(\"\") # Running a loop to capture information of each product for title, price, offer in zip(item_titles, item_prices, item_offer): title_text = title.get_text(strip=True) price_text = price.get_text(strip=True) offer_text = offer.get_text(strip=True) # \"result_list\" accumulates these dictionaries for all products on the page result_list.append({'Title': title_text, 'Price': price_text, 'Price_Unit': '£', 'Offer': offer_text}) # Extract numerical prices for median calculation numeric_price = float(price_text.replace('£', '').replace(',', '')) prices.append(numeric_price) # In[5]:", "source": "Repo:Web-Scraping:boot_web_scraping.py", "section": "Web-Scraping", "hash": "b30e290b"}
{"text": "oct-aem-text oct-aem-text--p--variant-subtext oct-teaser__productPrice'}) except: print(\"\") # Finding Offer from the products_items try: item_offer = item.find_all('div', {'class': 'oct-teaser__wrap'}) except: print(\"\") # Running a loop to capture information of each product for title, price, offer in zip(item_titles, item_prices, item_offer): title_text = title.get_text(strip=True) price_text = price.get_text(strip=True) offer_text = offer.get_text(strip=True) # \"result_list\" accumulates these dictionaries for all products on the page result_list.append({'Title': title_text, 'Price': price_text, 'Price_Unit': '£', 'Offer': offer_text}) # Extract numerical prices for median calculation numeric_price = float(price_text.replace('£', '').replace(',', '')) prices.append(numeric_price) # In[5]: # Calculate the median price median_price = calculate_median(prices) # Save the information in a JSON file output_data = { 'Products': result_list, 'Median': median_price } # In[6]: # Dumping the JSON file, named \"output.json\" with open('output.json', 'w', encoding='utf-8') as output_file: json.dump(output_data, output_file, indent=2, ensure_ascii=False)", "source": "Repo:Web-Scraping:boot_web_scraping.py", "section": "Web-Scraping", "hash": "f05cafcd"}
{"text": "from dotenv import load_dotenv load_dotenv() import base64 import streamlit as st import os import io from PIL import Image import pdf2image import google.generativeai as genai genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\")) def get_gemini_response(input,pdf_cotent,prompt): model=genai.GenerativeModel('gemini-pro-vision') response=model.generate_content([input,pdf_content[0],prompt]) return response.text def input_pdf_setup(uploaded_file): if uploaded_file is not None: ## Convert the PDF to image images=pdf2image.convert_from_bytes(uploaded_file.read()) first_page=images[0] # Convert to bytes img_byte_arr = io.BytesIO() first_page.save(img_byte_arr, format='JPEG') img_byte_arr = img_byte_arr.getvalue() pdf_parts = [ { \"mime_type\": \"image/jpeg\", \"data\": base64.b64encode(img_byte_arr).decode() # encode to base64 } ] return pdf_parts else: raise FileNotFoundError(\"No file uploaded\") ## Streamlit App st.set_page_config(page_title=\"ATS Resume EXpert\") st.header(\"ATS Tracking System\") input_text=st.text_area(\"Job Description: \",key=\"input\") uploaded_file=st.file_uploader(\"Upload your resume(PDF)...\",type=[\"pdf\"]) if uploaded_file is not None: st.write(\"PDF Uploaded Successfully\") submit1 = st.button(\"Tell Me About the Resume\") #submit2 = st.button(\"How Can I Improvise my Skills\") submit3 = st.button(\"Percentage match\") input_prompt1 = \"\"\" You are an experienced Technical Human Resource Manager,your task is to review the provided resume against the job description. Please share your professional evaluation on whether the candidate's profile aligns with the role. Highlight the strengths and weaknesses of the applicant in relation to the specified job requirements. \"\"\" input_prompt3 = \"\"\" You are an skilled ATS (Applicant Tracking System) scanner with a deep understanding of data science and ATS functionality, your task is to evaluate the resume against the provided job description. give me the percentage of match if the resume matches the job description. First the output should come as percentage and then keywords missing and last final thoughts. \"\"\" if submit1: if uploaded_file is not None: pdf_content=input_pdf_setup(uploaded_file) response=get_gemini_response(input_prompt1,pdf_content,input_text) st.subheader(\"The Repsonse is\") st.write(response) else: st.write(\"Please uplaod the resume\") elif submit3: if uploaded_file is not None: pdf_content=input_pdf_setup(uploaded_file) response=get_gemini_response(input_prompt3,pdf_content,input_text) st.subheader(\"The Repsonse is\") st.write(response) else: st.write(\"Please uplaod the resume\")", "source": "Repo:ATS:main.py", "section": "ATS", "hash": "cfeee7f5"}
{"text": "#!/usr/bin/env python # coding: utf-8 # ### Problem 1 [3 points] # Load the data provided in universities.csv from the moodle course page. The data is about economic and # demographic characteristics of various US universities. # Load the data from tuition_income.csv from the same page. # Select from the universities dataset data for all universities located in the following states: New Mexico, # Utah, Montana, Kentucky, South Carolina, Alabama, Idaho, West Virginia, Arkansas, Mississipi. Use this # restricted data set as basis for the entire exam (and modify accordingly in the following questions). # In[1]: import pandas as pd import plotly.express as px from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import scale from sklearn.linear_model import LinearRegression from sklearn.svm import SVR, SVC from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix pd.set_option('max_columns', None) import warnings warnings.filterwarnings('ignore') from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_curve,auc,plot_confusion_matrix # In[2]: # Taking University dataset uni= pd.read_csv(\"universities.csv\") uni # In[3]: # Taking Tuition Income datatset dftution= pd.read_csv(\"tuition_income.csv\") # ### 1. How many universities are in the resulting dataset? [1] # In[4]: df = uni df = df.loc[df['state'].isin([\"New Mexico\", \"Utah\", \"Montana\", \"Kentucky\", \"South Carolina\", \"Alabama\", \"Idaho\", \"West Virginia\", \"Arkansas\", \"Mississipi\"])] df = df[df[\"name\"].str.contains('University')] df # Solution: There in total 99 Universities in the resulting Dataset. # ### 2. Graduates of how many universities have an estimated early career pay greater than 50000 dollars per year? [1] # In[5]: # ECP is estimated early career pay ECP = df[df['early_career_pay']>50000] ECP # Solution: 15 Universities Graduates are having an estimated early career pay greater than 50000 dollars # per year? # ### 3. How many universities are not public? [1] # In[6]: # Not public = Private + For Profit pvt = df[df['type']!= 'Public'] print(pvt.type.value_counts()) pvt # Solution: 38 Universities are not public . # ### Problem 2: Making sense of your data [15 points] # In[7]: df.dtypes", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "d37d27ea"}
{"text": "# In[5]: # ECP is estimated early career pay ECP = df[df['early_career_pay']>50000] ECP # Solution: 15 Universities Graduates are having an estimated early career pay greater than 50000 dollars # per year? # ### 3. How many universities are not public? [1] # In[6]: # Not public = Private + For Profit pvt = df[df['type']!= 'Public'] print(pvt.type.value_counts()) pvt # Solution: 38 Universities are not public . # ### Problem 2: Making sense of your data [15 points] # In[7]: df.dtypes # ### 1. Take a quick look at the data structure. Describe the dataset in a few sentences. [2] # # Most of the columns in our University data are of type \"int64\". This means that they are 64 bit integers. But type \"float64\" column is a floating point value which means it contains decimals. Some datatypes are of \"object\", which means it contain numeric or strings or both vaues. # ### 2. Which facts or principles do you need to know about US university system to make a good sense of this dataset? [2] # # According to my evaluation, name of the university, state, total enrollment is important as it will give us an idea of the total number of students getting admission each year. Henceforth, the state code, type of university is also informative to know whether the university is public, private or for profit. Walking down the road, degree length gives the duration of the course plus, the private in_state_tuition, out_of_state_tuition and private in_state_total, out_of_state_total accounts for same data, so maybe we can merge it into one column to reduce number of columns. Early career pay, mid career pay gives a solid idea of how much they are going to earn during and after getting a degree. Make_world_better_percent also importantly contribute to the dataset as it gives an idea as how much we are contributing towars world with our knowledge. # ### 3. Which things about the data you have at hand you do not know for sure and have to assume? [2] # # Some of the data's I have less idea about thier existance are: # # n_asian # n_black # n_hispanic # n_pacific # n_total_minority # n_multiracial # n_unknown # n_white # room_and_board # stem_percent", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "76092324"}
{"text": "also importantly contribute to the dataset as it gives an idea as how much we are contributing towars world with our knowledge. # ### 3. Which things about the data you have at hand you do not know for sure and have to assume? [2] # # Some of the data's I have less idea about thier existance are: # # n_asian # n_black # n_hispanic # n_pacific # n_total_minority # n_multiracial # n_unknown # n_white # room_and_board # stem_percent # ### 4. In this project, you will have to predict out-of-state tuition fee. Do students choose university solely based on the cost? Which other factors might be important? [2] # # No, student have not selected university only on the basis of cost. If we will compare total_enrollment of row no. 70 and 84, the out_of_state tuition fee of row number 70 is 15848 and total_enrollment = 12002, also out_of_state tuition fee of row number 84 is 12870 and total_enrollment = 3128. As we can see the university in row 70 costs higher and also have higher enrollment of student than university in row number 84, which is cheaper, it can deduce that students are not choosing university on the basis of cost. # # Other factors which might be important other than the University are, type of university, early_career_pay and mid_career_pay. # ### 5. To whom is this cost variable more important than the other three? Explain. [2] # # # Other than those three the cost variable are important room_and_board. As we can see that the total cost whether it be in_state_tuition or out_of_state_tuition, the room_and_board are adding up in the total cost of both of these. # ### 6. Formulate a reasonable business goal, execution of which would require analysis of out-of-state tuition fee in this dataset. [1] # # ### 7. Which variable would you have to optimize for this goal? What variables would you have to constrain for it to be reasonable? [1] # The variable which I will optimize will be total_enrollment. I will put constrain on mid_career_pay. # ### 8. Which data manipulations would you have to perform for an analysis or an ML system for this goal? What would you have to predict? Would classification or regression be more suitable, and why? [3] # # ### Problem 3: EDA and Data preprocessing [10 points] # In[8]:", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "99e227e8"}
{"text": "goal? What variables would you have to constrain for it to be reasonable? [1] # The variable which I will optimize will be total_enrollment. I will put constrain on mid_career_pay. # ### 8. Which data manipulations would you have to perform for an analysis or an ML system for this goal? What would you have to predict? Would classification or regression be more suitable, and why? [3] # # ### Problem 3: EDA and Data preprocessing [10 points] # In[8]: # Number of time different states occurs in dataset. print(df.state.value_counts()) # In[9]: # Number of time different states_codes occurs in dataset. print(df.state_code.value_counts()) # In[10]: # Different types of University print(df.type.value_counts()) # In[11]: # Different length period print(df.degree_length.value_counts()) # In[12]: # Counting all categorical variables in dataset columns = ['state', 'state_code', 'type', 'degree_length'] for cols in columns: samples = df[cols].nunique() print(\"Total number of categories in\", cols , \" is \", samples) # ### 1. Check how many categorical features exist and for each categorical feature see how many samples belong to each category. [2] # # There are four categorical features which exist, those are: state, state_code, type and degree.length. # # No. of samples belong to each category are: # # state = 9; # state_code = 9; # type = 3; # degree.length = 2; # ### 2. Visualize the distributions of all features in the data set and summarize your findings. [6] # In[13]: get_ipython().run_line_magic('matplotlib', 'inline') import seaborn as sns import matplotlib.pyplot as plt import numpy as np df.plot.hist(subplots= True,bins=100,figsize=(15,30),edgecolor='black');", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "6c72f7e6"}
{"text": "[2] # # There are four categorical features which exist, those are: state, state_code, type and degree.length. # # No. of samples belong to each category are: # # state = 9; # state_code = 9; # type = 3; # degree.length = 2; # ### 2. Visualize the distributions of all features in the data set and summarize your findings. [6] # In[13]: get_ipython().run_line_magic('matplotlib', 'inline') import seaborn as sns import matplotlib.pyplot as plt import numpy as np df.plot.hist(subplots= True,bins=100,figsize=(15,30),edgecolor='black'); # ### Summary # # Unnamed: We don't see much distriution as it is equal to the Serial number <br><br> # total_enrollment: There we experience a right skewed plot. Which means number of students getting enrolled are as high as 5000, hence declintion is observed until 35000.<br><br> # n_native: Few thousand native students are only enrolled in university <br><br> # n_asian: This also contrubute as same as n_native, concluding that few students from asian countries went to USA for studying. <br><br> # n_black: This shows a minor right skewed graph which means that we will find these category student on a average, <br><br> # n_hispanic: It shows distribution almost equal to what we have seen in n_asian. Contributing only few thousands of students <br><br> # n_pacific: It shows distribution almost equal to what wehave seen in n_asian. Contributing only few thousands of students<br><br> # n_nonresident: It shows distribution almost equal to what wehave seen in n_asian. Contributing only few thousands of students<br><br> # n_total_minority: Here we see a right skewed, indicating high frequescy of students coming from minorities. <br><br> # n_multiracial: Multiracial shows excalty same distribution as n_pacific. Contributing few thousands of students. <br><br> # n_unknown: This growth is similar to n_hispanic distribution. <br><br> # n_white: This shows a right skewed showing high number of student in each intervals upto 45000 approx. <br><br> # n_women: This is also a right skewed which show high number during 5000 thousand and then there is a certain drop gofing forward. <br><br> # room_and_board: This histogram shows a rise and then fall of frequency before and after 10000's<br><br> # in_state_tuition: Leading at approx 8000, in state tution shows quite a distribution of mean value in each frequency level.<br><br> # in_state_total: The distribution shows a gradual incease and gradual decreas in the frequency, shooting up again at approx 32000, extending the mean upto 63000 approximate.<br><br> # out_of_state_tuition: Show a little co-relation with in_state_total where the average frequency is below 5.<br><br> # out_of_state_total: This represent a left skewed graph. Extending upto 62000 with highest frequency at 35000.<br><br> # early_career_pay: Shows an average frequency distribution of below 10.<br><br> # mid_career_pay: Shows a right skewed, with frequency of 1.0 approaching to 80000. Starting its mean value at 65000 mean approx and extending above 100000. <br><br> # make_world_better_percent: This also contrubute as same as n_native. <br><br> # stem_percent: This also contrubute as same as n_native", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "85d96db3"}
{"text": "with in_state_total where the average frequency is below 5.<br><br> # out_of_state_total: This represent a left skewed graph. Extending upto 62000 with highest frequency at 35000.<br><br> # early_career_pay: Shows an average frequency distribution of below 10.<br><br> # mid_career_pay: Shows a right skewed, with frequency of 1.0 approaching to 80000. Starting its mean value at 65000 mean approx and extending above 100000. <br><br> # make_world_better_percent: This also contrubute as same as n_native. <br><br> # stem_percent: This also contrubute as same as n_native # ### 3. Split the data set into a training (70%) and a test (30%) set with the help of stratified sampling based on the degree length. Report mean and standard deviation for out_of_state_tuition in train and test data set. [2] # In[14]: # Splitting data into test and train dataset from sklearn.model_selection import train_test_split X = df.loc[:, df.columns != 'out_of_state_tuition'] y = df['out_of_state_tuition'] X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=df['degree_length'], test_size=0.3) # In[15]: y_train.describe() # In[16]: y_test.describe() # ### Problem 4: Data Visualization [16 points] # In[17]: from scipy import stats, optimize, interpolate columns = ['n_native', 'n_asian', 'n_black', 'n_hispanic', 'n_pacific', 'n_nonresident', 'n_total_minority', 'n_multiracial', 'n_unknown', 'n_white', 'n_women'] for col in columns: ax = sns.scatterplot(data =df[(np.abs(stats.zscore(df[col])) < 3)] , x=col, y='total_enrollment', hue='out_of_state_tuition', s=100) plt.style.use('fivethirtyeight') plt.title(col +' vs total_enrollment') plt.xlabel(col) plt.ylabel('total_enrollment') plt.legend(title = 'out_of_state_tuition' ) plt.gcf().set_size_inches(7, 7) plt.show() # ### 1. Describe what can be seen and interpret it. [5] # # The scatterplot between n_native and enrollment shows an association that is positive, linear, and appears to be somewhat strong with a few outliers. We can also find good number of native students with the total enrollment of 32000 and few around 350 native students with enrollment approx 60000. <br><br> # # The scatterplot between n_asian and enrollment shows an association that is positive, linear, and appears to be somewhat strong with a medium outliers. We can also find high density of asian students upto total_enrollment of 5000. <br><br> # # The scatterplot between n_black and enrollment shows an association that is positive, non-linear, and appears to be somewhat strongly scatters. We can also find high density of black students during first few total enrollment of 5000. But this graph also depicts there are quite a number of students scattered around each interval of total_enrollment.<br><br> # # The scatterplot between n_hispanic and enrollment shows an association that is positive, linear, and appears to be somewhat strong with a few outliers.", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "5ca6a177"}
{"text": "# The scatterplot between n_black and enrollment shows an association that is positive, non-linear, and appears to be somewhat strongly scatters. We can also find high density of black students during first few total enrollment of 5000. But this graph also depicts there are quite a number of students scattered around each interval of total_enrollment.<br><br> # # The scatterplot between n_hispanic and enrollment shows an association that is positive, linear, and appears to be somewhat strong with a few outliers. The number of hispanic students are approx 2500 in the total enrollment of 30000 and below, and only few around 4000 with total enrollment of 60000.<br><br> # # # The scatterplot between n_pacific and enrollment shows an association that is positive, linear, and appears to be somewhat strong with few outliers. Maximum of approx 38 pacific students can be observed for the enrollment upto 30000 and few students with medium out of state tuition are represented in the outliers.<br><br> # # The scatterplot between n_resident and enrollment shows an association that is positive, curve, and appears to be somewhat strong with few outliers. A gap can br observed in nonresident students between around 800 to 1600. However, we can also find few hundered students in the total enrollment of around 60000.<br><br> # # The scatterplot between n_minority and enrollment shows an association that is positive, non-linear, and appears to be somewhat strong with a medium outliers. Maximum number of students are getting chance in university with high enrollment of above 2000. <br><br> # # The scatterplot between n_multiracial and enrollment shows an association that is positive, linear, and appears to be somewhat strong with a few outliers. The linear growth represent with the increase in total enrollment the number of student in multiracial are also getting increased.<br><br> # # The scatterplot between n_unknown and enrollment shows an association that is positive, non-linear, and appears to be somewhat strong with a medium outliers. We can find good number of unknown students with the total enrollment of 30000 and few around 1700 unknown students with enrollment approx 60000.<br><br> # # The scatterplot between n_white and enrollment shows an association that is positive, linear, and appears to be somewhat light with a few outliers.", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "937c5a47"}
{"text": "in multiracial are also getting increased.<br><br> # # The scatterplot between n_unknown and enrollment shows an association that is positive, non-linear, and appears to be somewhat strong with a medium outliers. We can find good number of unknown students with the total enrollment of 30000 and few around 1700 unknown students with enrollment approx 60000.<br><br> # # The scatterplot between n_white and enrollment shows an association that is positive, linear, and appears to be somewhat light with a few outliers. We can find a good linear growth of white students with high number of enrolled students.<br><br> # # The scatterplot between n_women and enrollment shows an association that is positive, linear with no outliers. We can find a directly proportionality of women students with the number of enrolled students. # # # ### 2. Which demographic characteristics are more pronounced for more expensive universities? [3] # There is a high competition in almost all demographics for more expensive university. The most pronounced one I will consider all of them as we can see from the graph that on an average out_of_state_tuition=30000 dollar for all demogrpahics. Moving forward we can also observe high out_of_state_tuition= 40000 for few demographics such as n_asian ad n_white. # ### 3. Write down your assumptions about why universities with lower tuition fees tend to attract certain groups more than more expensive universities. [3] # It is very natural that the University with low tuition fees tends to attract certian group of people because moving to anothter countries for education not only add up tuition fee on a student, but also adds up staying and fooding price. Hence it is also see that the native students are also a part of low tuition fees and reason could be that the native student belongs to that country and hence has to pay low, whereas, which is not in case with non native or students. # ### 4. Which important considerations might these visualization conceal, rather than reveal? Produce visualizations that illustrate the findings that might be unobvious from scatterplots above. [5] # Problem 4 part 4 # ### Problem 5: Correlation [15 points] # Create and look at the correlation plots between the variables. # ### 1. Which feature has the strongest correlation to the target feature? Plot the correlation between them exclusively. [2] # In[18]:", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "fd653d2c"}
{"text": "not in case with non native or students. # ### 4. Which important considerations might these visualization conceal, rather than reveal? Produce visualizations that illustrate the findings that might be unobvious from scatterplots above. [5] # Problem 4 part 4 # ### Problem 5: Correlation [15 points] # Create and look at the correlation plots between the variables. # ### 1. Which feature has the strongest correlation to the target feature? Plot the correlation between them exclusively. [2] # In[18]: def corr(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); corr(df,'out_of_state_tuition') # The strongest correlation with respect to out_of_state_tuition is out_of_state_total. # ### 2. Describe and interpret what you can see in this correlation plot. Any unusual occurrences? [3] # In the pearson correlation heatmap, we can observe the most correlated or variables which are highly proportional to out_of_state_tuition are out_of_state_total, in_state_total, in_state_tuition, room_and_board which are relted upto 60%. The reason could be, as these corresponds to the dollars which are costing to the students, and it do also calls for university profit of tuiton fees and room and board.<br> Further going there a drop of atleast 25% in comaparision to mid_career_pay. We can also observe n_nonresident and early_career_pay corresponds equal that is 30% correlation with out_of_state_tuition. <br>Henceforth, there is a sharp drop of values from 27% to 9.5% when comparision hits different type of student in the dataframe.<br> We can also see negative correlation of values to -1.3% to -21%. Which means that out_of_state_tuition are not much related with values lesser than 0. # ### 3. Which three features correlate the most with make_world_better_percent, a percentage of alumni who make the world a better place? [1] # In[19]: def corr1(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); corr1(df,'make_world_better_percent') # Three features correlate the most with make_world_better_percent are out_of_state_total, room_and_board and n_white. These three features are inversely correlated with make_world_better_place. # ### 4. Choose the strongest of these three correlations and propose four hypotheses about the nature of the link between these variables. [4]", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "84fe0bc9"}
{"text": "three features correlate the most with make_world_better_percent, a percentage of alumni who make the world a better place? [1] # In[19]: def corr1(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); corr1(df,'make_world_better_percent') # Three features correlate the most with make_world_better_percent are out_of_state_total, room_and_board and n_white. These three features are inversely correlated with make_world_better_place. # ### 4. Choose the strongest of these three correlations and propose four hypotheses about the nature of the link between these variables. [4] # 4 Hypothesis are:<br><br> # Hypothesis 1 - Lower is the cost of room_and_board, will have a positive impact on make_world_better_percent.<br><br> # Hypothesis 2 - More students belonging from different ethenicity, less will be the discrimination and it will also give a postive impact on make_world_better_percent. As there will young talent from all around the world. <br><br> # Hypothesis 3 - Lower the overall total cost of studying in University, more student will take addmission and probability of educated youth will increase which will make a positive impact on make_world_better_percent.<br><br> # Hypothesis 4 - More students means more research will be done by University which ultimately impact on make_world_better_percent.<br><br> # ### 5. Which hypothesis do you find the most plausible? Which sources are there supporting it? [2] # The most plausible hypothesis I found is Hypothesis 4. The sources which support it are the number of students from different Ethenicity contributes high percentage of make_world_better_percent. # ### 6. Which features do you lack in this dataset, which would have helped you determine whether your hypothesis is likely true? [1] # The feature which is lacking in dataset which would have determine my hypothesis to be true is \"Research dataset\". The number of research done by each University would have greatly contribute to prove my hypothesis true. # ### 7. Explain the difference between Pearson and Spearman correlation coefficients. Which of them have you just used in this problem? Which of them would be more feasible for the analysis you are doing?[2]", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "4e5a7339"}
{"text": "determine whether your hypothesis is likely true? [1] # The feature which is lacking in dataset which would have determine my hypothesis to be true is \"Research dataset\". The number of research done by each University would have greatly contribute to prove my hypothesis true. # ### 7. Explain the difference between Pearson and Spearman correlation coefficients. Which of them have you just used in this problem? Which of them would be more feasible for the analysis you are doing?[2] # In Pearson correlation the variables are directly proportional to each other, or we can say there is a linear relationship between two values. As the one change in one variable occurs, proportional change in other variable also occurs.<br><br> # # In Spearman, there is monotonic relationship between two variables. Variable change togeather but not necessatily at constant rate.<br><br> # # I have used Pearson correlation.<br><br> # # Spearman correlation may have a better analysis. # ### Problem 6: Correlation 2 [16 points] # # Create new attributes by combining different features with each other. # In[20]: dfethnicity = df.n_native+df.n_asian+df.n_black+df.n_hispanic+df.n_pacific+df.n_nonresident+df.n_total_minority+df.n_multiracial+df.n_unknown+df.n_white+df.n_women dfethnicity # In[21]: dfmid = pd.merge(df, dfethnicity.to_frame(),left_index=True, right_index=True) dfmid = dfmid.drop([\"n_native\",\"n_asian\",\"n_black\",\"Unnamed: 0\",\"n_pacific\",\"n_nonresident\",\"n_total_minority\",\"n_multiracial\",\"stem_percent\",\"n_unknown\",\"n_white\",\"n_women\",\"state_code\",\"room_and_board\",\"in_state_tuition\",\"n_hispanic\"], axis=1) dfmid.columns.values[11] = \"ethnicity\" dfmid # ### 1. Explain why you think a combination will be useful before you create them. [2] # The newly created combination \"dfmid\" is useful as I have combined all the different ethnicity into one which reduces the size and complexity of dataset drastically. Moving forward I removed more features as there were already combined into one. This combination now give a proper visualization of datas which may postively impact the dataset. # ### 2. Check their correlation with the out_of_state_tuition in comparison with the other features from before. Show the correlations with the target feature in a descending order. [2] # In[22]: # Heat map of newly created dataset with respect to out_of_state_tuition def make_corr(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); make_corr(dfmid,'out_of_state_tuition') # ### 3. Do your newly created features have higher correlation with the target feature? [1] # Yes, as represented from the above heatmap, I find these features to have higher correlation with out_of_state_tuition.", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "f0cf5af1"}
{"text": "comparison with the other features from before. Show the correlations with the target feature in a descending order. [2] # In[22]: # Heat map of newly created dataset with respect to out_of_state_tuition def make_corr(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); make_corr(dfmid,'out_of_state_tuition') # ### 3. Do your newly created features have higher correlation with the target feature? [1] # Yes, as represented from the above heatmap, I find these features to have higher correlation with out_of_state_tuition. # ### 4. Take a look at the data from tuition_income dataset and decide which features or combinations of features you think will also be beneficial for prediction. Repeat steps 1-3 for them. Note that you may need to make some extra transformations to add them. [4] # In[23]: dftution.head() dfmerge = pd.merge(dfmid,dftution,how=\"inner\",on = \"name\") dftuiton_filter = dftution[dftution[\"name\"].isin(dfmerge.name.unique().tolist())] dftuiton_filter = pd.concat([dftuiton_filter,pd.get_dummies(dftuiton_filter.income_lvl)],axis = 1).drop(\"income_lvl\",axis=1) dftuiton_filter_merge = pd.DataFrame() low = [] mid = [] high = [] veryhigh = [] highest = [] total_price = [] year = [] campus = [] net_cost=[] income_lvl = [] for i in dftuiton_filter.name.unique(): temp = dftuiton_filter[dftuiton_filter[\"name\"]==i] total_price.append(temp[\"total_price\"].median()) year.append(temp['year'].mode(dropna = True).iloc[0]) campus.append(temp['campus'].mode(dropna = True).iloc[0]) net_cost.append(temp['net_cost'].median()) low.append(temp[\"0 to 30,000\"].sum()) mid.append(temp[\"30,001 to 48,000\"].sum()) high.append(temp[\"48_001 to 75,000\"].sum()) veryhigh.append(temp[\"75,001 to 110,000\"].sum()) highest.append(temp['Over 110,000'].sum()) dftuiton_filter_merge[\"name\"] = dftuiton_filter.name.unique().tolist() dftuiton_filter_merge[\"year\"] = year dftuiton_filter_merge[\"total_price\"] = total_price dftuiton_filter_merge[\"net_cost\"] = net_cost dftuiton_filter_merge[\"0 to 30,000\"] = low dftuiton_filter_merge[\"30,001 to 48,000\"] = mid dftuiton_filter_merge[\"48,001 to 75,000\"] = high dftuiton_filter_merge[\"75,001 to 110,000\"] = veryhigh dftuiton_filter_merge[\"Over 110,000\"] = highest dftuiton_filter_merge[\"campus\"] = campus # In[24]: dfuni_tuition_combined = pd.merge(dfmid,dftuiton_filter_merge,how=\"inner\",on = \"name\") # In[25]: # Displaying merge dataset of university and tuition income. dfuni_tuition_combined # The combination is useful because it narrow down data to more specific dataset which contain all the useful information needed. # In[26]: # Correlation heatmap of combine datatset with out_of_state_tuition def corr(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); corr1(dfuni_tuition_combined,'out_of_state_tuition') # Yes, my newly created features are having higher correlation with the target feature that is out_of_state_tuition.", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "a519a915"}
{"text": "In[24]: dfuni_tuition_combined = pd.merge(dfmid,dftuiton_filter_merge,how=\"inner\",on = \"name\") # In[25]: # Displaying merge dataset of university and tuition income. dfuni_tuition_combined # The combination is useful because it narrow down data to more specific dataset which contain all the useful information needed. # In[26]: # Correlation heatmap of combine datatset with out_of_state_tuition def corr(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); corr1(dfuni_tuition_combined,'out_of_state_tuition') # Yes, my newly created features are having higher correlation with the target feature that is out_of_state_tuition. # ### 8. Do any of your new features shine the new light on possible determinants of what makes students feel they are making the world the better place? Explain your insights. [2] # # If we compare both the heat map that is <b>Correlation heatmap of combine datatset with out_of_state_tuition</b> and <b> Heat map of newly created dataset with respect to out_of_state_tuition</b>, we will not see much difference on make_world_better_percent. In Correlation heatmap of combine datatset with out_of_state_tuition the value is -0.26 and in Heat map of newly created dataset with respect to out_of_state_tuition is -0.21 i.e.; there is only a growth of -0.05 units value. # ### Problem 7: Data cleaning [10 points] # ### 1. Find out which variables contain missing values. If there are any, how many values are missing? [2] # In[27]: dfuni_tuition_combined.isnull().sum() * 100 / len(dfuni_tuition_combined) # ### 2. Which approaches exist for handling missing data? Describe two approaches of how to handle them. Write one advantage and one disadvantage for each of those methods. [4] # Approaches exist for handling missing data are: Multiple Imputatuion and K Nearest Neighbors(KNN)<br><br> # Multiple Imputation: It is mainly for large dataset. In this uses subsituting data in place of missing data.<br> # Advantage: Results are readily interpreted.<br> # Disadvantage:It assumes a random data in place of missing data.<br><br> # KNN: This method uses Eucliden distance between neighbouring data cordinates to know how similar data is.<br> # Advantage: It do not assumes data.<br> # Disadvamtage: Since it stores all training data, hence it requires large memory capacity. # # ### 3. Handle the missing data by methods you find the most suitable. Explain how you chose the method for each column. [3] # In[28]: # Handling missing datas. dfuni_tuition_combined_imputation = dfuni_tuition_combined.fillna(dfuni_tuition_combined.mean()) # In[29]: dfuni_tuition_combined_imputation.head()", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "121a4f08"}
{"text": "in place of missing data.<br><br> # KNN: This method uses Eucliden distance between neighbouring data cordinates to know how similar data is.<br> # Advantage: It do not assumes data.<br> # Disadvamtage: Since it stores all training data, hence it requires large memory capacity. # # ### 3. Handle the missing data by methods you find the most suitable. Explain how you chose the method for each column. [3] # In[28]: # Handling missing datas. dfuni_tuition_combined_imputation = dfuni_tuition_combined.fillna(dfuni_tuition_combined.mean()) # In[29]: dfuni_tuition_combined_imputation.head() # While cleaning the data we observed 3 features that is are early_career_pay, mid_career_pay, make_world_better_percent denotes missing values. So I implemented Multiple Imputation technique to fill the missing values as thats is the best technique for large dataset. # In[30]: # Splitting values into train and test set with 70% versus 30% ratio, respectively. dfuni_tuition_combined_imputation.drop([\"name\"],inplace=True,axis=1) dfuni_tuition_combined_imputation[\"year\"] = dfuni_tuition_combined_imputation.year.apply(str) X = dfuni_tuition_combined_imputation.loc[:, dfuni_tuition_combined_imputation.columns != 'out_of_state_tuition'] y = dfuni_tuition_combined_imputation['out_of_state_tuition'] X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=dfuni_tuition_combined_imputation['degree_length'], test_size=0.3,random_state = 131) # ### 4. Handle the categorical data with one-hot-encoding. [1] # In[31]: X_train = pd.get_dummies(X_train) X_test = pd.get_dummies(X_test) # ### Problem 8: Feature scaling [9 points] # # ### 1. What feature scaling methods exist? Name five methods and write a short description for three of them. Pick one to use for this data set. [6] # Feature Scaling is the process by which we transform data into a better version. It is use to normalize the features in the dataset into a finite range.<br><br> # Five Feature Scaling methods are:<br><br> # <b>Absolute Maximum Scaling:</b> In this scaling, datas are scaled into maximum value and the results varies approximately within the range of -1 to 1<br><br> # <b>Min-Max Scaling:</b> It is feature pre-processing technique in which data are scaled in fixed range of 0 to 1.<br><br> # <b>Normalization:</b> It is the feature by which numeric columns in dataset changes to common scale.<br><br> # <b>Standardization:</b> In this method the datas are converted into uniform format which is further used to different operations.<br><br> # <b>Robust Scaling:</b> In this the algorithm scale features that are robust to outliers. # In[32]: dfuni_tuition_combined_imputation.head() # In[33]: # Performing Standardization method for my dataset. from sklearn.preprocessing import StandardScaler def scal_independent(DataFrame): scale = StandardScaler() cols = ['total_enrollment','in_state_total', 'out_of_state_total','early_career_pay', 'mid_career_pay', 'make_world_better_percent','total_price', 'net_cost','ethnicity'] DataFrame[cols] = scale.fit_transform(DataFrame[cols]) return DataFrame def scal_dependent(DataFrame): dataframe = scale(DataFrame) return dataframe X_train = scal_independent(X_train) X_test = scal_independent(X_test) y_train = scal_dependent(y_train) y_test = scal_dependent(y_test) # In[34]:", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "0578521f"}
{"text": "are converted into uniform format which is further used to different operations.<br><br> # <b>Robust Scaling:</b> In this the algorithm scale features that are robust to outliers. # In[32]: dfuni_tuition_combined_imputation.head() # In[33]: # Performing Standardization method for my dataset. from sklearn.preprocessing import StandardScaler def scal_independent(DataFrame): scale = StandardScaler() cols = ['total_enrollment','in_state_total', 'out_of_state_total','early_career_pay', 'mid_career_pay', 'make_world_better_percent','total_price', 'net_cost','ethnicity'] DataFrame[cols] = scale.fit_transform(DataFrame[cols]) return DataFrame def scal_dependent(DataFrame): dataframe = scale(DataFrame) return dataframe X_train = scal_independent(X_train) X_test = scal_independent(X_test) y_train = scal_dependent(y_train) y_test = scal_dependent(y_test) # In[34]: # Dropping non-numeric features from imputed dataset. X_train.drop(list(set(X_train.columns.tolist())- set(X_test.columns.tolist())),axis=1,inplace=True) # In[35]: X_test.drop('state_Montana',inplace = True, axis = 1) # ### 2. Find the feature importance with a quick Random Forest and show them in a plot. What insights do you get out of it? [3] # In[36]: from sklearn.datasets import make_regression X, y = make_regression(n_samples=165, n_features=52, n_informative=5, random_state=1) mdl = RandomForestRegressor() mdl.fit(X_train, y_train) feature_scores = pd.Series(mdl.feature_importances_, index=X_train.columns).sort_values(ascending=False) f, ax = plt.subplots(figsize=(30, 24)) ax = sns.barplot(x=feature_scores, y=feature_scores.index, data=df) ax.set_title(\"Visualize feature scores of the features\") ax.set_yticklabels(feature_scores.index) ax.set_xlabel(\"Feature importance score\") ax.set_ylabel(\"Features\") plt.show() # I can observe that not much feature shows up with the Random Forest feature scaling method. # ### Problem 9: Test data [6 points] # # Perform the same data cleaning steps from questions 7 and 8 for the test data set. (Replace missing # values, handle the categorical data, add the new features, and scale the features) [6] # # #### Already perform above in question 7 & 8. # ### Problem 10: Regression [19 points] # # Select and train the following regression models on the training set. Linear model, support vector # regression, and random forest. # ### 1. Evaluate the three regression models on the test set. Which model performs best? [9] # In[37]: # Performing Linear Regression. r =LinearRegression() model = r.fit(X_train, y_train) y_pred = r.predict(X_test) residuals = y_test - y_pred mae = round(metrics.mean_absolute_error(y_test, y_pred),2) mse = round(metrics.mean_squared_error(y_test, y_pred),2) rmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2) mape = round(np.mean(abs(residuals/y_test)),2) accuracy = str(100-(round(np.mean(abs(residuals/y_test)),4)*100)) +' %' nRMSE = round((rmse/70.127909),2) aError = round(np.mean(abs(residuals)),2) error_df = pd.DataFrame(data = [[mae, mse, rmse,mape, accuracy, nRMSE,aError]], columns = ['MAE', 'MSE', 'RMSE','MAPE','Model Accuracy','Normalized RMSE','Average Error'], index = ['Linear Regression']) print(error_df) # In[38]:", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "1156bbd8"}
{"text": "models on the test set. Which model performs best? [9] # In[37]: # Performing Linear Regression. r =LinearRegression() model = r.fit(X_train, y_train) y_pred = r.predict(X_test) residuals = y_test - y_pred mae = round(metrics.mean_absolute_error(y_test, y_pred),2) mse = round(metrics.mean_squared_error(y_test, y_pred),2) rmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2) mape = round(np.mean(abs(residuals/y_test)),2) accuracy = str(100-(round(np.mean(abs(residuals/y_test)),4)*100)) +' %' nRMSE = round((rmse/70.127909),2) aError = round(np.mean(abs(residuals)),2) error_df = pd.DataFrame(data = [[mae, mse, rmse,mape, accuracy, nRMSE,aError]], columns = ['MAE', 'MSE', 'RMSE','MAPE','Model Accuracy','Normalized RMSE','Average Error'], index = ['Linear Regression']) print(error_df) # In[38]: # Performing Random Forest Regression r = RandomForestRegressor() model = r.fit(X_train, y_train) y_pred = r.predict(X_test) residuals = y_test - y_pred mae = round(metrics.mean_absolute_error(y_test, y_pred),2) mse = round(metrics.mean_squared_error(y_test, y_pred),2) rmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2) mape = round(np.mean(abs(residuals/y_test)),2) accuracy = str(100-(round(np.mean(abs(residuals/y_test)),4)*100)) +' %' nRMSE = round((rmse/70.127909),2) aError = round(np.mean(abs(residuals)),2) error_df = pd.DataFrame(data = [[mae, mse, rmse,mape, accuracy, nRMSE,aError]], columns = ['MAE', 'MSE', 'RMSE','MAPE','Model Accuracy','Normalized RMSE','Average Error'], index = ['Random Forest Regression']) print(error_df) # In[39]: # Performing Support Vector Regression r =SVR () model = r.fit(X_train, y_train) y_pred = r.predict(X_test) residuals = y_test - y_pred mae = round(metrics.mean_absolute_error(y_test, y_pred),2) mse = round(metrics.mean_squared_error(y_test, y_pred),2) rmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2) mape = round(np.mean(abs(residuals/y_test)),2) accuracy = str(100-(round(np.mean(abs(residuals/y_test)),4)*100)) +' %' nRMSE = round((rmse/70.127909),2) aError = round(np.mean(abs(residuals)),2) error_df = pd.DataFrame(data = [[mae, mse, rmse,mape, accuracy, nRMSE,aError]], columns = ['MAE', 'MSE', 'RMSE','MAPE','Model Accuracy','Normalized RMSE','Average Error'], index = ['SVR']) print(error_df) # ##### Solution: # Random forest model performs the best. # ### 2. Explain one approach about how the models can be further optimized. [3] # One approach by which a model can be further be optimize is through isotonic regression. Isotonic Regression is a technique by which a free-form line is fitted to a sequence of observation in such a way that fitted line is neither non-decreasing or non-increasing everywhere rather should be close to the observation as much as possible. # ### 3. Based on the data you have at hand and your background knowledge, which problems might you encounter when trying use your best model to predict out-of-state tuition fee for the next year? [2] # # ### 4. Name and explain two accuracy metrics for regression other than MSE or RMSE. Are any of them better suited for evaluating your models, given the goal you have formulated earlier? [2]", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "20bdf1de"}
{"text": "be close to the observation as much as possible. # ### 3. Based on the data you have at hand and your background knowledge, which problems might you encounter when trying use your best model to predict out-of-state tuition fee for the next year? [2] # # ### 4. Name and explain two accuracy metrics for regression other than MSE or RMSE. Are any of them better suited for evaluating your models, given the goal you have formulated earlier? [2] # Two accuracy model except MSE and MAPE are: <br><br> # MAE (Mean Absolute Error): It is the average measure of errors in a set of models without taking vector into account.<br><br> # MAPE (Mean Absolute Percentage Error): It is the percentage of the average difference between forecasted and true values.<br><br> # Yes they both sucessfully suited for the evaluation of my model. # # ### 5. What has to change in real life to completely invalidate the results you get and accuracy of your model? I.e., what laws, natural events, societal changes have to happen to make conclusions based on this dataset inadmissible for further decision-making? [3] # If out_of_state_tuition and in_state_total feature gets invalidate, the results and accuracy of the whole model will become inaccurate. If such a law is passed that no fees will be taken by the students the dataset will become inadmissible for further decision making. # ### Problem 11: Classification [21 points] # # Categorize the target variable (out_of_state_tuition) into five categories and build a classification model # for the above pre-processed data. # ### 1. Train the following classification models on the training set for classification and evaluate the models on the test set: SVM, k-NN, and Random Forest. [9] # In[40]: # Function for classification. def classification(Xtrain, Xtest, ytrain, ytest, classifier): cls = classifier cls.fit(Xtrain, ytrain) ypred = cls.predict(Xtest) print(f\"Accuracy of the classifier is: {accuracy_score(ytest, ypred)}\") print(f\"Precision Score of the classifier is: {precision_score(ytest, ypred,average='macro')}\") print(f\"Recall Score of the classifier is: {recall_score(ytest, ypred,average='macro')}\") print(f\"F1 Score of the classifier is: {f1_score(ytest, ypred,average='macro')}\") plot_confusion_matrix(classifier, Xtest, ytest) # In[41]: # Creating bins for y_train and y_test y_train_class = pd.cut(y_train, bins = [-3,-1,0,1,3,4], include_lowest = True, labels=[1,2,3,4,5]) y_test_class = pd.cut(y_test, bins = [-3,-1,0,1,3,4], include_lowest = True, labels=[1,2,3,4,5]) # In[42]: # Plotting Confusion matrix for model k-NN classification(X_train, X_test, y_train_class, y_test_class, KNeighborsClassifier()) # In[43]: # Plotting Confusion matrix for model SVM classification(X_train, X_test, y_train_class, y_test_class, SVC()) # In[44]:", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "883539fa"}
{"text": "{precision_score(ytest, ypred,average='macro')}\") print(f\"Recall Score of the classifier is: {recall_score(ytest, ypred,average='macro')}\") print(f\"F1 Score of the classifier is: {f1_score(ytest, ypred,average='macro')}\") plot_confusion_matrix(classifier, Xtest, ytest) # In[41]: # Creating bins for y_train and y_test y_train_class = pd.cut(y_train, bins = [-3,-1,0,1,3,4], include_lowest = True, labels=[1,2,3,4,5]) y_test_class = pd.cut(y_test, bins = [-3,-1,0,1,3,4], include_lowest = True, labels=[1,2,3,4,5]) # In[42]: # Plotting Confusion matrix for model k-NN classification(X_train, X_test, y_train_class, y_test_class, KNeighborsClassifier()) # In[43]: # Plotting Confusion matrix for model SVM classification(X_train, X_test, y_train_class, y_test_class, SVC()) # In[44]: # Plotting Confusion matrix for model Random Forest classification(X_train, X_test, y_train_class, y_test_class, RandomForestClassifier()) # ### 3. Which model performs best based on which evaluation method? [2] # Random Forest perform best on our model. The can be seen by the accuracy of the classifier which is 88.4615% approx # ### 4. Explain the evaluation method you used. [2] # The evaluation method I have used here are: <br><br> # <b>SVM:</b> SVM is a supervised learning method. These are used in high dimenation spaces.<br><br><b>k-NN:</b> This method does not make assumption on underlying datas. It perform action at the time of classifcation, hence it is also known as lazy lerner algorithm.<br><br>and,<br><br> <b>Random Forest:</b> It uses ensemble learning that is it combines classifiers to perform solution to complex problem. # ### 5. For which applications would classification into these artificial categories be more useful than regression? Name at least two. [2] # Application where classification will be more useful than regression is when there will discrete value. In these artificial category we can opt for \"type\" and \"degree_length\". # ### 6. For which applications would regression on original values be more useful? Name at least two. [2] # Regression is used in contunues values. Here we can use in \"out_of_state_total\" and \"total_enrollment\" # ### Part 2 # # ### Problem 12: Text Mining [40 points] # # The dataset GoodReads.csv contains book descriptions. Load this dataset and draw random sample of size 5000 from it, setting the seed to 45. # In[45]: Goodreads = pd.read_csv('GoodReads.csv') Goodreads.head() # In[46]: # dfgr is the name of the data frame of Good Reads book dfgr = Goodreads.sample(n=5000, random_state=45) dfgr.head() # ### 1. Plot and describe the distribution of average ratings of the books.[2] # In[47]: fig = px.histogram(x = dfgr['rating'], width=800, marginal='box', labels={'col':'col'}) fig.update_layout(title = 'Average Ratings of books') fig.show()", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "635f1bea"}
{"text": "The dataset GoodReads.csv contains book descriptions. Load this dataset and draw random sample of size 5000 from it, setting the seed to 45. # In[45]: Goodreads = pd.read_csv('GoodReads.csv') Goodreads.head() # In[46]: # dfgr is the name of the data frame of Good Reads book dfgr = Goodreads.sample(n=5000, random_state=45) dfgr.head() # ### 1. Plot and describe the distribution of average ratings of the books.[2] # In[47]: fig = px.histogram(x = dfgr['rating'], width=800, marginal='box', labels={'col':'col'}) fig.update_layout(title = 'Average Ratings of books') fig.show() # ### 2. Clean the data in your dataframe. Explain your reasoninng for selecting the cleaning methods you decided to use.[3] # In[48]: dfgr = dfgr.drop(['img', 'isbn', 'isbn13', 'link'], axis = 1) dfgr.head() # I decided to drop image, link and isbn, because, img and link are the web-links for the picture and the PDF file of the book, I also ommitted the isbn, which will not be using. # ### 3. Categorize the rating variable into several categories. Explain how many categories you decided to have, where you drew the line, and why you decided to do it this way[5] # In[49]: category = pd.cut(dfgr['rating'], bins = [0, 1, 2, 3, 4, 5.1], include_lowest = True, labels=['Very Poor', 'Poor', 'Neutral', 'Good', 'Very Good']) dfgr['Rating Category'] = category dfgr.head() # ### 4. Get a feel about the distribution of text lengths of the book descriptions by adding a new feature for the length of each message. Check the statistical values. [2] # In[50]: msgLen = dfgr['desc'].str.len() dfgr['Description Length'] = msgLen dfgr.head() # ### 5. Visualize the distribution of text lengths with a histogram, where the colouring is according to the rating category. [2] # In[51]: x1 = dfgr.loc[dfgr['Rating Category'] == 'Very Poor', 'Description Length'] x2 = dfgr.loc[dfgr['Rating Category'] == 'Poor', 'Description Length'] x3 = dfgr.loc[dfgr['Rating Category'] == 'Neutral', 'Description Length'] x4 = dfgr.loc[dfgr['Rating Category'] == 'Good', 'Description Length'] x5 = dfgr.loc[dfgr['Rating Category'] == 'Very Good', 'Description Length'] kwargs = dict(alpha = 0.5, bins = 80) plt.figure(figsize=(20,9)) plt.hist(x1, **kwargs, color='#465362', label='Very Poor') plt.hist(x2, **kwargs, color='#011936', label='Poor') plt.hist(x3, **kwargs, color='#C2EABD', label='Neutral') plt.hist(x4, **kwargs, color='#F9DC5C', label='Good') plt.hist(x5, **kwargs, color='#ED254E', label='Very Good') plt.gca().set(title = 'Frequency Histogram of Text Length', ylabel = 'Frequency', xlabel = 'Text Length') plt.legend(loc = 'upper right', prop = {'size': 20}); # ### 6. Create a random stratified training and test split (70/30 split). Verify the correct proportions of the splitted data sets by creating proportion table. [1] # In[52]:", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "88b9de28"}
{"text": "dict(alpha = 0.5, bins = 80) plt.figure(figsize=(20,9)) plt.hist(x1, **kwargs, color='#465362', label='Very Poor') plt.hist(x2, **kwargs, color='#011936', label='Poor') plt.hist(x3, **kwargs, color='#C2EABD', label='Neutral') plt.hist(x4, **kwargs, color='#F9DC5C', label='Good') plt.hist(x5, **kwargs, color='#ED254E', label='Very Good') plt.gca().set(title = 'Frequency Histogram of Text Length', ylabel = 'Frequency', xlabel = 'Text Length') plt.legend(loc = 'upper right', prop = {'size': 20}); # ### 6. Create a random stratified training and test split (70/30 split). Verify the correct proportions of the splitted data sets by creating proportion table. [1] # In[52]: X = dfgr.loc[:, dfgr.columns != 'Description Length'] y = dfgr['Description Length'] X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=dfgr['Rating Category'],test_size=0.3) grStr = {'X': [len(X_train)/(len(X_train)+len(X_test)),len(X_test)/(len(X_train)+len(X_test))], 'Y': [len(y_train)/(len(y_train)+len(y_test)),len(y_test)/(len(y_train)+len(y_test))]} grProportion = pd.DataFrame(grStr) grProportion.head() # In[53]: # Train dataset Xtrain_tab = pd.crosstab(index = X_train['Rating Category'], columns = 'Count') Xtrain_tab # In[54]: # Test dataset Xtest_tab = pd.crosstab(index = X_test['Rating Category'], columns = 'Count') Xtest_tab # ### 7. Tokenize the descriptions with help of the quanteda package and illustrate the effect of each action by showing the content of some of the reviews in the training data set: # # Remove the numbers, punctuations, symbols, and hyphens. Turn the texts in the reviews into lower case. Remove the stopwords in the modified training set (the tokens) with the predefined stopword list of quanteda. Perform stemming on the tokens. [5] # In[55]: import nltk from nltk.tokenize import sent_tokenize nltk.download('punkt') nltk.download('stopwords') import string from nltk.corpus import stopwords from nltk.stem.snowball import SnowballStemmer # In[56]: # Getting rid of NaN in the description column, to prepare it for tokenization dfgr['desc']=dfgr['desc'].fillna('') dfgr.head(40) # In[57]: # Creating the new string with lowercase desctiption dfgr['Lower Case'] = dfgr['desc'].str.lower() dfgr.head(25) # In[58]: # Getting rid of the punctuations in the Lower Case variable dfgr['Lower Case'] = dfgr['Lower Case'].str.replace('[^\\w\\s]', '') dfgr.head() # In[59]: # Deleting stopwords stop = stopwords.words('english') dfgr['Without Stopwords'] = dfgr['Lower Case'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) dfgr.head() # In[60]: # Create a new column with tokenized lowercase column dfgr['Tokenized Description'] = dfgr.apply(lambda row: nltk.word_tokenize(row['Without Stopwords']), axis = 1) dfgr.head() # In[61]: # Stemming stemmer = SnowballStemmer('english') dfgr['Stemmed Token'] = dfgr['Tokenized Description'].apply(lambda x: [stemmer.stem(y) for y in x]) dfgr.head() # ### 8. Create a bag-of-words model and add bi-grams to the normal feature matrix. [2] # In[62]: vocab = [] for i in dfgr['Stemmed Token']: for x in i: vocab.append(x) vocab # In[63]: # Getting rid of duplicated values vocabDupl = list(dict.fromkeys(vocab)) vocabDupl # In[64]:", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "cf006404"}
{"text": "with tokenized lowercase column dfgr['Tokenized Description'] = dfgr.apply(lambda row: nltk.word_tokenize(row['Without Stopwords']), axis = 1) dfgr.head() # In[61]: # Stemming stemmer = SnowballStemmer('english') dfgr['Stemmed Token'] = dfgr['Tokenized Description'].apply(lambda x: [stemmer.stem(y) for y in x]) dfgr.head() # ### 8. Create a bag-of-words model and add bi-grams to the normal feature matrix. [2] # In[62]: vocab = [] for i in dfgr['Stemmed Token']: for x in i: vocab.append(x) vocab # In[63]: # Getting rid of duplicated values vocabDupl = list(dict.fromkeys(vocab)) vocabDupl # In[64]: def convert(org_list, separator = ' '): return separator.join(org_list) sentences = [] for j in dfgr['Stemmed Token']: sentence = '' sentence = convert(j) sentences.append(sentence) sentences # In[65]: sentence100 = sentences[:100] print(sentence100) # In[66]: # Transforming given text into vectors from sklearn.feature_extraction.text import CountVectorizer import itertools count_vec = CountVectorizer() cdf = count_vec.fit_transform(sentence100) bag = pd.DataFrame(cdf.toarray(), columns = count_vec.get_feature_names()) bag # In[67]: # Bi-Gram for 100 sentences bi_gram = [(x, i.split()[j + 1]) for i in sentence100 for j, x in enumerate(i.split()) if j < len(i.split()) - 1] print (\"The BiGram are :\\n\\n \" + str(bi_gram)) # ### 9. Build a function for relative term frequency (TF) and another one to calculate the inverse document frequency (IDF). # In[68]: # Calculating TF def tf(wordDict, bow): tfDict = {} bowCount = len(bow) for word, count in wordDict.items(): tfDict[word] = count/float(bowCount) return tfDict # In[69]: # Calculating IDF def idf(docList): import math idfDict = dict() n = len(docList) idfDict = dict.fromkeys(docList[0].keys(), 0) for doc in docList: for word, val in doc.items(): if val == 0: idfDict[word] += 1.0 for word, val in idfDict.items(): idfDict[word] = math.log10(n / float(val)) return idfDict # In[70]: import __future__ from __future__ import division # Calculating TFIDF def computeTFIDF(tfBow,idfs): tfidf = {} for word, val in tfBow.items(): tfidf[word] = val * idfs[word] return tfidf # In[71]: # Create a dictionary, to count the words in a certain second wordDict1 = dict.fromkeys(vocabDupl, 0) wordDict2 = dict.fromkeys(vocabDupl, 0) # Counting words for word in dfgr['Stemmed Token'].values[0]: wordDict1[word] += 1 for word in dfgr['Stemmed Token'].values[1]: wordDict1[word] += 1 wordDict1 # In[72]: # Convert into Data Frame pd.DataFrame([wordDict1]) # In[74]: tfBow1 = tf(wordDict1, dfgr['Stemmed Token'].values[0]) tfBow2 = tf(wordDict1, dfgr['Stemmed Token'].values[1]) tfBow1 # In[75]: idfs = idf([wordDict1, wordDict2]) # In[76]: tfidfBow1 = computeTFIDF(tfBow1, idfs) tfidfBow2 = computeTFIDF(tfBow2, idfs) pd.DataFrame([tfidfBow1, tfidfBow2])", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "92fd1879"}
{"text": "# KNN-ML-project # iPhone Purchase Prediction with K-Nearest Neighbors (KNN) This project utilizes the K-Nearest Neighbors algorithm to predict whether a person will purchase an iPhone based on their gender, age, and salary. The dataset contains information about individuals, and the goal is to build a model that accurately predicts iPhone purchases. ## Dataset Overview The dataset 'iphone.csv' includes columns for Gender, Age, Salary, and the target variable 'Purchase Iphone'. Initial exploration shows no missing values. ## Data Preprocessing - **Handling Categorical Data**: Gender information is converted into numerical values using LabelEncoder. - **Feature Scaling**: StandardScaler is used to standardize the independent variables for the KNN algorithm. ## Model Building - **K-NN Classifier**: A K-Neighbors Classifier is trained with n_neighbors set to 5. - **Training Accuracy**: Achieves an accuracy of 91.67%. ## Model Evaluation ### Confusion Matrix Metrics - **True Negatives (TN)**: 64 - **False Positives (FP)**: 4 - **False Negatives (FN)**: 3 - **True Positives (TP)**: 29 ### Performance Metrics - **Accuracy Score**: 93% - **Precision Score**: 87.88% - **Recall Score**: 90.62% ## Prediction The model predicts whether a 'Female' aged 47 with a salary of 30000 will purchase an iPhone as '1' (indicating purchase). ## Determining Optimal n_neighbors - Plotting training and testing accuracy against varying n_neighbors (1-20). - Observed overfitting for lower n_neighbors (1,2) and achieved convergence at n_neighbors = 3. - Optimal n_neighbors found at 3, as both training and testing accuracy stabilize around 93%. ## Conclusion The model best operates with n_neighbors = 3 for this dataset, providing a stable prediction accuracy of 93% without overfitting.", "source": "Repo:KNN-ML-project:README.md", "section": "KNN-ML-project", "hash": "4e81ffbe"}
{"text": "## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted]python # Making Predictions y_pred = c.predict(x_test_scalar) # It will do the prediction of new data and give us the O/P [Long code block omitted] ## Markdown Cell <b>Accuracy Score</b> = (TP+TN)/(TP+TN+FP+FN)<br><br> <b>Recall Score</b> = TP/(TP+FN)<br><br> <b>Precision Score</b> = TP/(TP+FP) ## Markdown Cell We get an accuracy of 93% and only 7 that is 3+4 incorrect predictions were made.<br><br> 64 --> True Negative i.e,; Person has not bought an iPhone and predicted value also says the same<br> 4 --> Flase Positive i.e,; Person has not bought an iPhone but the predicted value says they did buy<br> 3 --> Flase Negative i.e,; Person has bought an iPhone but the predicted value says they did not buy<br> 29 --> True Positive i.e,; Person has bought an iPhone and the predicted value also says they bought<br> ## Code Cell [Long code block omitted] ## Markdown Cell After running this code I got the training and testing accuracy for different n_neighbors ## Code Cell ```python # Plotting test, train accuracy with n_neighbors plt.figure(figsize=(10,5)) plt.plot(range(1,21), testing_accuracy, label = \"Testing Accuracy\") plt.plot(range(1,21), training_accuracy, label = \"Training Accuracy\") plt.title('Training vs Testing Accuracy') plt.xlabel('n_neighbors') plt.ylabel('Accuracy') plt.legend(loc = 'best') plt.show() ``` ## Markdown Cell #### Analyzing the graph above In the beginning when n_neighbor were 1,2 training accuracy was lot higher than the testing accuracy. So, the model was suffering from overfitting.<br><br> After that the training and testing accuracy become closer and thats the spot that we wanted. <br><br> Moving forward, when the n_neighbor got higher, the testing accuracy acquire a constant value of about 93% and training accuracy gradually went down from n_neighbour = 5; which we do not need.<br><br> Therefore, from the above graph the n_neighbour for this particular dataset and model should be 3.", "source": "Repo:KNN-ML-project:kNN Classifier iPhone proj.ipynb", "section": "KNN-ML-project", "hash": "7c726022"}
{"text": "## Code Cell [Long code block omitted]python df.shape [Long code block omitted]python features = ['Charge_Capacity(Ah)', 'Discharge_Capacity(Ah)', 'Internal_Resistance(Ohm)', 'Voltage(V)'] scaler = MinMaxScaler() normalized_features = scaler.fit_transform(df_modify[features]) # Define weights for each feature weights = [0.3, 0.3, 0.2, 0.2] # Calculate the battery health metric battery_health_metric = (normalized_features * weights).sum(axis=1) plt.figure(figsize=(8, 6)) plt.hist(battery_health_metric, bins=20, color='blue') plt.xlabel('Battery Health Metric') plt.ylabel('Frequency') plt.title('Distribution of Battery Health Metric') plt.grid(True) plt.show() # Calculate threshold based on histogram threshold_low = 0.2 threshold_high = 0.8 # Calculate battery health status based on thresholds battery_health_status = [] for value in battery_health_metric: if value < threshold_low: battery_health_status.append(\"Unhealthy\") elif value < threshold_high: battery_health_status.append(\"Intermediate\") else: battery_health_status.append(\"Healthy\") df_modify['Battery_Health_Status'] = battery_health_status [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted]", "source": "Repo:NASA-Battery-project:Li ion.ipynb", "section": "NASA-Battery-project", "hash": "ae8a95c9"}
{"text": "# NASA-Battery-project ## Overview The dataset 'CS2_37_1_18_11.xlsx' contains information about a battery undergoing various testing procedures. It provides multiple parameters and measurements that can be utilized for understanding the battery's health, efficiency, and behavior during different steps and cycles of testing. ### General Dataset Information - The dataset comprises 9871 entries and 17 columns representing different battery parameters. - Various features such as Test_Time, Date_Time, Step_Time, Current, Voltage, Capacity, Energy, and Resistance, among others, are included. - Initial inspection indicates no missing values and diverse data types, primarily float and integer types. ## Exploratory Data Analysis ### Battery Health Metric Calculation - Data normalization and feature weighting were applied to derive a 'Battery Health Metric.' - A histogram is plotted to depict the distribution of the calculated metric. - Categorized battery health status into 'Unhealthy,' 'Intermediate,' and 'Healthy.' ### Battery Health Status Visualization - A pie chart visualizes the distribution of different battery health statuses. ### Step and Cycle-Based Aggregations - Calculated statistical measures (mean, median, and standard deviation) of Voltage and Current for both steps and cycles. - Graphically represented step-based and cycle-based aggregations, illustrating the trends across different steps and cycles. ### Discharge Efficiency Analysis - Discharge efficiency was computed from charge and discharge energy values. - A histogram was plotted to demonstrate the distribution of discharge efficiency. ### Time-Series Analysis of Discharge Efficiency - Utilized time-series analysis to observe the discharge efficiency trends concerning battery health status over time. - Plotted these trends to visualize how discharge efficiency fluctuates concerning the battery's health status. ## Insights and Observations - The battery health metric calculation allowed for better understanding of battery health status. - Aggregations provided insights into step and cycle-based trends of voltage and current. - Discharge efficiency analysis revealed a distribution, showing the variance in efficiency measurements. ## Conclusion The analysis provides a comprehensive understanding of the battery's health, efficiency, and behavior during testing procedures. The visualizations and calculations offer insights into the battery's performance, guiding further analysis or actions.", "source": "Repo:NASA-Battery-project:README.md", "section": "NASA-Battery-project", "hash": "8e4f6fb9"}
{"text": "## Code Cell [Long code block omitted] ## Markdown Cell ## Train Dataset ## Code Cell [Long code block omitted] ## Markdown Cell ## Out_of_sample Dataset ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted]python # Load the saved threshold with open(\"threshold(tfidf).pkl\", \"rb\") as f: threshold = pickle.load(f)# Create a grouped box plot data = [mean_similarity_In_Sample, mean_similarity_Out_Of_Sample, mean_similarity_foreign] labels = ['In Sample Test', 'Out Sample Test', 'Foreign Test'] # Add a dashed line for the threshold plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold') plt.boxplot(data, labels=labels) plt.title(\"Box Plot of Mean Cosine Similarities(TF-IDF)\") plt.ylabel(\"Mean Cosine Similarity\") plt.legend() plt.show() [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted]", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.ipynb", "section": "Master-Thesis-Data-Drift", "hash": "bd803c36"}
{"text": "# Project Overview This repository comprises a suite of functions dedicated to text data preprocessing, TF-IDF vectorization, cosine similarity computation, data visualization, and analysis. The primary focus is on text similarity calculations and comparisons. ## Table of Contents 1. [General Functions](#general-functions) 2. [Text Preprocessing](#text-preprocessing) 3. [TF-IDF Calculation](#tf-idf-calculation) 4. [Cosine Similarity Calculation](#cosine-similarity-calculation) 5. [Data Visualization](#data-visualization) 6. [Word2Vec Analysis](#word2vec-analysis) 7. [Thresholds Analysis](#thresholds-analysis) --- ## General Functions <a name=\"general-functions\"></a> The repository offers a set of general functions specifically designed for managing text data: - `create_dataframe(path)`: Creates a Pandas DataFrame from text files located at a specified path. - `preprocess_text(df, column)`: Handles text data preprocessing within a DataFrame for a specified column. - `tfidf(train, column_train, test, column_test)`: Computes TF-IDF vectorization for both training and test data. - `calculate_cosine_similarity(train, test)`: Determines the cosine similarity between two sets of vectors. - `heatmapvis(argument)`: Generates a heatmap plot for improved data visualization. - `histogramvis(argument)`: Produces a histogram plot for visualizing data distributions. - `top10(cosinesimilarity)`: Identifies the top 10 most similar documents based on cosine similarity. - `percentile(cosinesimilarity)`: Calculates the 10th, 50th, and 75th percentiles for the distribution of cosine similarity within the dataset. - `word2vec(df, column)`: Computes Word2Vec representation for a given dataset. --- ## Text Preprocessing <a name=\"text-preprocessing\"></a> The `preprocess_text()` function operates on a DataFrame and performs various preprocessing tasks, including special character and number removal, text conversion to lowercase, elimination of single alphabets, tokenization, stopword removal, lemmatization, and more. --- ## TF-IDF Calculation <a name=\"tf-idf-calculation\"></a> The `tfidf()` function calculates TF-IDF vectorization for the provided training and test datasets using the TfidfVectorizer from scikit-learn. --- ## Cosine Similarity Calculation <a name=\"cosine-similarity-calculation\"></a> The `calculate_cosine_similarity()` function computes the cosine similarity between two sets of vectors, a crucial tool for text similarity analysis. --- ## Data Visualization <a name=\"data-visualization\"></a> The repository includes functions like `heatmapvis()` and `histogramvis()` for visualizing cosine similarity through heatmap plots and histogram distributions, enhancing data interpretation. --- ## Word2Vec Analysis <a name=\"word2vec-analysis\"></a> The repository also offers functions dedicated to Word2Vec analysis. `word2vec()` computes Word2Vec representations for a given dataset. --- ## Thresholds Analysis <a name=\"thresholds-analysis\"></a> The analysis involves loading previously saved thresholds and creating box plots for mean cosine similarities obtained from both TF-IDF and Word2Vec analysis for different test datasets. For comprehensive details on usage and functions, please refer to the provided code.", "source": "Repo:Master-Thesis-Data-Drift:README.md", "section": "Master-Thesis-Data-Drift", "hash": "b92aab43"}
{"text": "Master Thesis February 21, 2024 [1]: import re import os import nltk import string import pickle import gensim import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from nltk.corpus import stopwords from gensim.models import Word2Vec from nltk.stem import WordNetLemmatizer from sklearn.metrics.pairwise import cosine_similarity from sklearn.feature_extraction.text import TfidfVectorizer 0.1 General function used in project [2]: def create_dataframe(path): \"\"\" Creating a dataframe of the text file. Args: path: Path in which the text file exists. Return: df: Text file extracted in the dataframe for further computations. \"\"\" content = [] file_names = [] for filename in os.listdir(path): with open(os.path.join(path, filename), 'r') as file: content.append(file.read()) file_names.append(filename) 1 df = pd.DataFrame({'Filename': file_names, 'Content': content}) return df \"\"\" Preprocessing the text dataset which is acquired. Args: df: Dataframe of whose preprocessing is to be done for a particular␣ ↪column. column: Column name for which preprocessing is to be done. Return: df: Preprocessed dataframe \"\"\" def preprocess_text(df, column): # Remove special characters and numbers df[column] = df[column].apply(lambda text: re.sub(r'[^a-zA-Z\\s]', '',␣ ↪str(text))) # Convert to lowercase df[column] = df[column].str.lower() # Remove multiple occurrences of 'i' characters df[column] = df[column].apply(lambda text: re.sub(r'(i{2,})', 'i',␣ ↪str(text))) # Remove single alphabets df[column] = df[column].apply(lambda text: re.sub(r'\\b[a-zA-Z]\\b', '',␣ ↪str(text))) # Tokenize the text df[column] = df[column].apply(lambda text: nltk.word_tokenize(text)) # Remove stopwords stop_words = set(stopwords.words('english')) df[column] = df[column].apply(lambda tokens: [token for token in tokens if␣ ↪token not in stop_words]) # Lemmatize the tokens lemmatizer = WordNetLemmatizer() df[column] = df[column].apply(lambda tokens: [lemmatizer.lemmatize(token)␣ ↪for token in tokens]) 2 # Remove 'mmddyyy' df[column] = df[column].apply(lambda text: re.sub(r'(\\b[mmddyyy]+\\b)', '',␣ ↪str(text))) # Remove extra whitespace df[column] = df[column].apply(lambda text: re.sub(r'\\s+', '', str(text))) # Join the tokens back into a string df[column] = df[column].apply(lambda tokens: ''.join(tokens)) return df \"\"\" Calculate TF-IDF vectorization between train and test dataset. Args: train: Train dataframe. test: Test dataframe- column_train: Column of intrest from the training dataframe. column_test: Column of intrest from the testing dataframe. Returns: train: Array-like or sparse matrix representing the training vectors. test: Array-like or sparse matrix representing the test vectors. \"\"\" def tfidf(train, column_train, test, column_test): # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(train[column_train]) # Apply the same vectorizer on the test documents test = vectorizer.transform(test[column_test]) print(train.shape) print(test.shape) return train, test 3", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.pdf", "section": "Master-Thesis-Data-Drift", "hash": "91f1fbe8"}
{"text": "Test dataframe- column_train: Column of intrest from the training dataframe. column_test: Column of intrest from the testing dataframe. Returns: train: Array-like or sparse matrix representing the training vectors. test: Array-like or sparse matrix representing the test vectors. \"\"\" def tfidf(train, column_train, test, column_test): # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(train[column_train]) # Apply the same vectorizer on the test documents test = vectorizer.transform(test[column_test]) print(train.shape) print(test.shape) return train, test 3 def calculate_cosine_similarity(train, test): \"\"\" Calculate cosine similarity between two sets of vectors. Args: train: Array-like or sparse matrix representing the training vectors. test: Array-like or sparse matrix representing the test vectors. Returns: cos_sim: Array representing the cosine similarity between the training␣ ↪and test vectors. \"\"\" cos_sim = cosine_similarity(train, test) return cos_sim #------------Data visualization--------------------# \"\"\" Plotting Heatmap and Histogram form better data visualization. Args: argument: Array cosine similarity between training and testing vectors. \"\"\" def heatmapvis(argument): # Create a heatmap plot plt.figure(figsize=(6, 4)) sns.heatmap(argument, cmap='cool') plt.title('Cosine Similarity') plt.xlabel('Test Documents') plt.ylabel('Train Documents') plt.show() def histogramvis(argument): plt.hist(argument.flatten(), bins=20) plt.xlabel('Cosine Similarity') plt.ylabel('Frequency') plt.title('Distribution of Cosine Similarity') plt.show() \"\"\" 4 Calculate top 10 document which is most similar according to Cosine␣ ↪similarity. Args: cosinesimilarity: Array-like cosine similarity between training and␣ ↪testing vectors. Return: top_similarities: Document number which is most similar in both the␣ ↪data. \"\"\" def top10(cosinesimilarity): doc_index = 0 # Index of the document to find similarities for N = 10 # Number of top similarities to retrieve top_similarities = sorted(range(len(cosinesimilarity[doc_index])),␣ ↪key=lambda i: cosinesimilarity[doc_index][i], reverse=True)[:N] return top_similarities \"\"\" Calculate pecentile 25th, 50th and 75th percentile to find distribution of␣ ↪the dataset w.r.t cosine similarity of TF-IDF vectorization. Args: cosinesimilarity: Array-like cosine similarity between training and␣ ↪testing vectors. \"\"\" def percentile(cosinesimilarity): percentiles = np.percentile(cosinesimilarity, [10, 50, 75]) print(\"10th percentile:\", percentiles[0]*100) print(\"50th percentile:\", percentiles[1]*100) print(\"75th percentile:\", percentiles[2]*100) ␣ ↪ \"\"\" Calculate the Word2Vec of dataset 5", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.pdf", "section": "Master-Thesis-Data-Drift", "hash": "38f73e76"}
{"text": "to find similarities for N = 10 # Number of top similarities to retrieve top_similarities = sorted(range(len(cosinesimilarity[doc_index])),␣ ↪key=lambda i: cosinesimilarity[doc_index][i], reverse=True)[:N] return top_similarities \"\"\" Calculate pecentile 25th, 50th and 75th percentile to find distribution of␣ ↪the dataset w.r.t cosine similarity of TF-IDF vectorization. Args: cosinesimilarity: Array-like cosine similarity between training and␣ ↪testing vectors. \"\"\" def percentile(cosinesimilarity): percentiles = np.percentile(cosinesimilarity, [10, 50, 75]) print(\"10th percentile:\", percentiles[0]*100) print(\"50th percentile:\", percentiles[1]*100) print(\"75th percentile:\", percentiles[2]*100) ␣ ↪ \"\"\" Calculate the Word2Vec of dataset 5 Args: df: Name of the dataframe who comparision is tobe done with train␣ ↪dataset. column: Column name for which vectorization is to be done with training␣ ↪dataset. Return: similarity_matrix_name: Array like similarity between trian and chossen␣ ↪dataset. \"\"\" def word2vec(df,column): # Train Word2Vec model model = gensim.models.Word2Vec(df[column], vector_size=100, window=5,␣ ↪min_count=1, workers=4) return model 0.2 Train Dataset [3]: df_train = create_dataframe(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/ ↪train_set\") [4]: #df_train [5]: df_train = preprocess_text(df_train, \"Content\") [6]: #df_train[\"Content\"][0] 0.3 In_sample Dataset [7]: df_in_test = create_dataframe(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/ ↪In_sample_test\") [8]: df_in_test = preprocess_text(df_in_test, \"Content\") 0.4 Out_of_sample Dataset [9]: df_out_test = create_dataframe(\"/mnt/c/Users/alamm9/Desktop/ ↪text_file_train_test/Out_of_sample_test\") [10]: df_out_test = preprocess_text(df_out_test, \"Content\") 6 0.5 Foreign Dataset [11]: df_foreign = pd.read_csv(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake. ↪csv\") [12]: #df_foreign.head() [13]: selected_text = df_foreign['text'].sample(n=500, random_state=42) # Create a new DataFrame with the selected text selected_df = pd.DataFrame(selected_text, columns=['text']) [14]: selected_df = preprocess_text(selected_df,\"text\") [15]: # selected_df [16]: selected_df.shape [16]: (500, 1) [ ]: 0.6 Calculating TF-IDF on Train and In_Sample, Out_of_Sample & Foregin Dataset [17]: # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_in_test[\"Content\"]) cos1 = calculate_cosine_similarity(train, test) mean_similarity_In_Sample = np.mean(cos1, axis=1) [18]: heatmapvis(cos1) 7 [19]: top10(cos1) [19]: [304, 300, 302, 301, 303, 282, 138, 19, 241, 79] [20]: # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_out_test[\"Content\"]) cos2= calculate_cosine_similarity(train, test) mean_similarity_Out_Of_Sample = np.mean(cos2, axis=1) [21]: heatmapvis(cos2) 8 [22]: top10(cos2) [22]: [22, 23, 35, 32, 37, 24, 25, 26, 40, 28] [23]: # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(selected_df[\"text\"]) cos3 = calculate_cosine_similarity(train, test) mean_similarity_foreign = np.mean(cos3, axis=1) [24]: heatmapvis(cos3) 9", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.pdf", "section": "Master-Thesis-Data-Drift", "hash": "d3dec3e7"}
{"text": "= vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_out_test[\"Content\"]) cos2= calculate_cosine_similarity(train, test) mean_similarity_Out_Of_Sample = np.mean(cos2, axis=1) [21]: heatmapvis(cos2) 8 [22]: top10(cos2) [22]: [22, 23, 35, 32, 37, 24, 25, 26, 40, 28] [23]: # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(selected_df[\"text\"]) cos3 = calculate_cosine_similarity(train, test) mean_similarity_foreign = np.mean(cos3, axis=1) [24]: heatmapvis(cos3) 9 [25]: top10(cos3) [25]: [95, 7, 85, 54, 368, 234, 240, 22, 433, 494] [26]: # Load the saved threshold with open(\"threshold(tfidf).pkl\", \"rb\") as f: threshold = pickle.load(f)# Create a grouped box plot data = [mean_similarity_In_Sample, mean_similarity_Out_Of_Sample,␣ ↪mean_similarity_foreign] labels = ['In Sample Test', 'Out Sample Test', 'Foreign Test'] # Add a dashed line for the threshold plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold') plt.boxplot(data, labels=labels) plt.title(\"Box Plot of Mean Cosine Similarities(TF-IDF)\") plt.ylabel(\"Mean Cosine Similarity\") plt.legend() plt.show() 10 [27]: # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5,␣ ↪min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token␣ ↪in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the␣ ↪training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in␣ ↪model.wv], axis=0) for doc_tokens in df_in_test[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) # Calculate mean similarity for each row mean_similarity_In_Sample = np.mean(similarity_matrix, axis=1) similarity_matrix (1626, 307) 11 [28]: heatmapvis(similarity_matrix) [29]: top10(similarity_matrix ) [29]: [110, 18, 295, 153, 296, 8, 95, 145, 254, 282] [30]: # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5,␣ ↪min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token␣ ↪in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the␣ ↪training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in␣ ↪model.wv], axis=0) for doc_tokens in df_out_test[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) 12", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.pdf", "section": "Master-Thesis-Data-Drift", "hash": "f2e1e2c9"}
{"text": "145, 254, 282] [30]: # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5,␣ ↪min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token␣ ↪in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the␣ ↪training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in␣ ↪model.wv], axis=0) for doc_tokens in df_out_test[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) 12 # Calculate mean similarity for each row mean_similarity_Out_Of_Sample = np.mean(similarity_matrix, axis=1) similarity_matrix (1626, 41) [31]: heatmapvis(similarity_matrix) [32]: top10(similarity_matrix ) [32]: [11, 3, 7, 10, 20, 19, 18, 23, 17, 16] [33]: # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5,␣ ↪min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token␣ ↪in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the␣ ↪training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in␣ ↪model.wv], axis=0) for doc_tokens in selected_df[\"text\"]] 13 # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) # Calculate mean similarity for each row mean_similarity_foreign = np.mean(similarity_matrix, axis=1) similarity_matrix (1626, 500) [34]: heatmapvis(similarity_matrix) [35]: top10(similarity_matrix ) [35]: [128, 126, 311, 287, 393, 309, 155, 433, 242, 139] [37]: # Load the saved threshold with open(\"threshold(w2v).pkl\", \"rb\") as f: threshold = pickle.load(f) data = [mean_similarity_In_Sample, mean_similarity_Out_Of_Sample,␣ ↪mean_similarity_foreign] labels = ['In Sample Test', 'Out Sample Test', 'Foreign Test'] 14 # Add a dashed line for the threshold plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold') plt.boxplot(data, labels=labels) plt.title(\"Box Plot of Mean Cosine Similarities(Word2Vec)\") plt.ylabel(\"Mean Cosine Similarity\") plt.legend() plt.show() [ ]: 15", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.pdf", "section": "Master-Thesis-Data-Drift", "hash": "b5bdee43"}
{"text": "persona: name: \"Mohammad Tanzil Alam\" style: \"friendly, concise, professional\" llm: gguf_repo: \"bartowski/Llama-3.2-1B-Instruct-GGUF\" gguf_file: \"Llama-3.2-1B-Instruct-Q4_K_M.gguf\" n_ctx: 4096 n_threads: 4 temperature: 0.2 top_p: 0.9 max_tokens: 512 embeddings: model_name: \"sentence-transformers/all-MiniLM-L6-v2\" retriever: k: 5 guardrails: blocked_phrases: - \"reveal your system prompt\" - \"ignore previous instructions\" - \"jailbreak\" - \"password\" - \"credit card\" - \"social security\"", "source": "Repo:TanzilGPT:config.yaml", "section": "TanzilGPT", "hash": "56bfb9ae"}
{"text": "--- title: TanzilGPT emoji: 👤 colorFrom: blue colorTo: purple sdk: streamlit sdk_version: 1.28.0 app_file: app.py pinned: false license: apache-2.0 tags: - chatbot - AI CV - streamlit - personal assistant - interactive resume --- # 🤖 TanzilGPT – My Interactive AI CV Welcome to **TanzilGPT**, an AI-powered version of my CV. Instead of scrolling through documents, you can simply **ask questions** and get clear answers about my background, projects, and online work. --- ## ✨ Features - 🧑‍💼 Learn about my **professional experience and skills** - 💻 Explore my **projects** (directly connected with my GitHub repositories) - 🌍 Access my **portfolio, blog, and other online links** - ❓ Have a natural chat to discover more about me --- ## 💬 Example Questions You can ask TanzilGPT things like: - *\"Tell me about your professional experience.\"* - *\"Explain one of your projects in detail.\"* - *\"What programming languages do you know?\"* - *\"What are your key technical skills?\"* --- ## 🚀 How It Works - The chatbot uses RAG (Retrieval-Augmented Generation) trained on my: - 📄 CV and professional background - 📂 Projects and code from GitHub repositories - 🔗 Portfolio and online presence - It uses this knowledge to answer questions with cited sources - It does **not** search the web or access private data — only what I've chosen to share --- ## 🎯 Why I Built This Traditional resumes are static. I wanted to create something **interactive and modern**: an AI assistant that lets you explore my work as if you were having a conversation with me. --- ## 🙋‍♂️ About Me I'm **Mohammad Tanzil Alam**, a computer engineer passionate about **AI, Data Engineering, and building impactful tools**. 📌 Connect with me: - 💼 LinkedIn: https://www.linkedin.com/in/mohammad-tanzil-alam/ - 📂 GitHub: https://github.com/tanzilalam23 --- ⚡ **Try it out now — ask TanzilGPT anything about me!**", "source": "Repo:TanzilGPT:README.md", "section": "TanzilGPT", "hash": "4c2d65ce"}
{"text": "import os import re import nbformat import json import faiss import numpy as np import yaml from pathlib import Path from bs4 import BeautifulSoup from sentence_transformers import SentenceTransformer from git import Repo, InvalidGitRepositoryError import fitz import hashlib from typing import List, Tuple, Dict # Configuration CONTENT_DIR = \"content\" REPO_DIR = \"repos\" INDEX_DIR = \"index\" CONFIG_FILE = \"config.yaml\" def load_config(): \"\"\"Load configuration from YAML file\"\"\" try: with open(CONFIG_FILE, \"r\", encoding=\"utf-8\") as f: return yaml.safe_load(f) except FileNotFoundError: print(f\"❌ {CONFIG_FILE} not found!\") return None def ensure_directories(): \"\"\"Create necessary directories\"\"\" for directory in [REPO_DIR, INDEX_DIR]: os.makedirs(directory, exist_ok=True) print(f\"📁 Ensured directory exists: {directory}\") def extract_github_urls(content_dir: str) -> List[str]: \"\"\"Extract GitHub URLs from markdown files in content directory\"\"\" repo_urls = set() # Use set to avoid duplicates if not os.path.exists(content_dir): print(f\"⚠️ Content directory '{content_dir}' not found!\") return [] for md_file in Path(content_dir).glob(\"*.md\"): try: with open(md_file, \"r\", encoding=\"utf-8\") as f: content = f.read() # Find GitHub URLs urls = re.findall(r\"https://github\\.com/[^\\s)]+\", content) repo_urls.update(urls) print(f\"📄 Found {len(urls)} GitHub URLs in {md_file.name}\") except Exception as e: print(f\"⚠️ Error reading {md_file}: {e}\") return list(repo_urls) def fetch_repository(url: str) -> Path: \"\"\"Clone or update a GitHub repository\"\"\" try: # Create a safe directory name from URL repo_name = url.replace(\"https://github.com/\", \"\").replace(\"/\", \"-\") local_path = Path(REPO_DIR) / repo_name if local_path.exists(): try: # Try to update existing repo repo = Repo(local_path) if repo.remotes: repo.remotes.origin.pull() print(f\"🔄 Updated {url}\") else: print(f\"⚠️ No remote found for {local_path}\") except (InvalidGitRepositoryError, Exception) as e: # If update fails, remove and re-clone print(f\"⚠️ Update failed ({e}), re-cloning...\") import shutil shutil.rmtree(local_path) repo = Repo.clone_from(url, local_path, depth=1) print(f\"📦 Cloned {url}\") else: # Clone fresh repo repo = Repo.clone_from(url, local_path, depth=1) print(f\"📦 Cloned {url}\") return local_path except Exception as e: print(f\"❌ Failed to fetch {url}: {e}\") return None def extract_text_from_file(filepath: Path) -> str: \"\"\"Extract text from various file formats\"\"\" try: if filepath.suffix.lower() in [\".md\", \".py\", \".txt\", \".js\", \".css\", \".yml\", \".yaml\", \".json\"]: with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: return f.read() elif filepath.suffix.lower() == \".ipynb\": nb = nbformat.read(filepath, as_version=4) text_parts = [] for cell in nb.cells: if cell.cell_type in (\"markdown\", \"code\") and cell.source.strip(): text_parts.append(f\"# {cell.cell_type.upper()} CELL\\n{cell.source}\\n\") return \"\\n\".join(text_parts) elif filepath.suffix.lower() in [\".html\", \".htm\"]: with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: soup = BeautifulSoup(f.read(), \"html.parser\") return soup.get_text(separator=\"\\n\", strip=True) elif filepath.suffix.lower() == \".pdf\": with fitz.open(filepath) as pdf: text_parts = [] for page_num in range(len(pdf)): page = pdf[page_num] text_parts.append(page.get_text()) return \"\\n\".join(text_parts) return \"\" except Exception as e: print(f\"⚠️ Error extracting text from {filepath}: {e}\") return \"\"", "source": "Repo:TanzilGPT:prep_index.py", "section": "TanzilGPT", "hash": "8d49362f"}
{"text": "== \".ipynb\": nb = nbformat.read(filepath, as_version=4) text_parts = [] for cell in nb.cells: if cell.cell_type in (\"markdown\", \"code\") and cell.source.strip(): text_parts.append(f\"# {cell.cell_type.upper()} CELL\\n{cell.source}\\n\") return \"\\n\".join(text_parts) elif filepath.suffix.lower() in [\".html\", \".htm\"]: with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: soup = BeautifulSoup(f.read(), \"html.parser\") return soup.get_text(separator=\"\\n\", strip=True) elif filepath.suffix.lower() == \".pdf\": with fitz.open(filepath) as pdf: text_parts = [] for page_num in range(len(pdf)): page = pdf[page_num] text_parts.append(page.get_text()) return \"\\n\".join(text_parts) return \"\" except Exception as e: print(f\"⚠️ Error extracting text from {filepath}: {e}\") return \"\" def extract_texts_from_directory(directory_path: Path, source_name: str) -> List[Tuple[str, str]]: \"\"\"Extract texts from all files in a directory\"\"\" texts = [] valid_extensions = {\".md\", \".py\", \".ipynb\", \".html\", \".htm\", \".pdf\", \".txt\", \".js\", \".css\", \".yml\", \".yaml\", \".json\"} skip_dirs = {\".git\", \"venv\", \"node_modules\", \"__pycache__\", \".pytest_cache\", \"dist\", \"build\"} for root, dirs, files in os.walk(directory_path): # Skip certain directories dirs[:] = [d for d in dirs if d not in skip_dirs] for filename in files: filepath = Path(root) / filename # Skip files that are too large (>5MB) if filepath.stat().st_size > 5 * 1024 * 1024: continue if filepath.suffix.lower() in valid_extensions: text = extract_text_from_file(filepath) if text.strip() and len(text.strip()) > 50: # Only include substantial content relative_path = str(filepath.relative_to(directory_path)) texts.append((text, f\"{source_name}:{relative_path}\")) return texts def collect_all_documents(content_dir: str, repo_urls: List[str]) -> List[Dict[str, str]]: \"\"\"Collect all documents from CV files and repositories\"\"\" documents = [] # 1. Process CV markdown files if os.path.exists(content_dir): for md_file in Path(content_dir).glob(\"*.md\"): try: with open(md_file, \"r\", encoding=\"utf-8\") as f: content = f.read() if content.strip(): documents.append({ \"text\": content, \"source\": f\"CV:{md_file.name}\", \"section\": md_file.stem }) print(f\"📄 Added CV file: {md_file.name}\") except Exception as e: print(f\"⚠️ Error reading {md_file}: {e}\") # 2. Process GitHub repositories for url in repo_urls: repo_path = fetch_repository(url) if repo_path and repo_path.exists(): repo_name = url.split(\"/\")[-1] repo_texts = extract_texts_from_directory(repo_path, f\"Repo:{repo_name}\") for text, source in repo_texts: documents.append({ \"text\": text, \"source\": source, \"section\": repo_name }) print(f\"📦 Added {len(repo_texts)} documents from {repo_name}\") print(f\"✅ Total collected documents: {len(documents)}\") return documents def smart_chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]: \"\"\"Intelligently chunk text by sentences and paragraphs\"\"\" # First try to split by double newlines (paragraphs) paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()] chunks = [] current_chunk = [] current_size = 0 for paragraph in paragraphs: words = paragraph.split() # If paragraph itself is too long, split by sentences if len(words) > chunk_size: sentences = [s.strip() for s in re.split(r'[.!?]+', paragraph) if s.strip()] for sentence in sentences: sentence_words = sentence.split()", "source": "Repo:TanzilGPT:prep_index.py", "section": "TanzilGPT", "hash": "4844d2e1"}
{"text": "= 500, overlap: int = 50) -> List[str]: \"\"\"Intelligently chunk text by sentences and paragraphs\"\"\" # First try to split by double newlines (paragraphs) paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()] chunks = [] current_chunk = [] current_size = 0 for paragraph in paragraphs: words = paragraph.split() # If paragraph itself is too long, split by sentences if len(words) > chunk_size: sentences = [s.strip() for s in re.split(r'[.!?]+', paragraph) if s.strip()] for sentence in sentences: sentence_words = sentence.split() if current_size + len(sentence_words) > chunk_size and current_chunk: # Save current chunk chunk_text = ' '.join(current_chunk) if len(chunk_text.split()) > 10: # Only save substantial chunks chunks.append(chunk_text) # Start new chunk with overlap overlap_words = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk current_chunk = overlap_words + sentence_words current_size = len(current_chunk) else: current_chunk.extend(sentence_words) current_size += len(sentence_words) else: # Paragraph fits in chunk size if current_size + len(words) > chunk_size and current_chunk: # Save current chunk chunk_text = ' '.join(current_chunk) if len(chunk_text.split()) > 10: chunks.append(chunk_text) # Start new chunk with overlap overlap_words = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk current_chunk = overlap_words + words current_size = len(current_chunk) else: current_chunk.extend(words) current_size += len(words) # Add final chunk if current_chunk: chunk_text = ' '.join(current_chunk) if len(chunk_text.split()) > 10: chunks.append(chunk_text) return chunks def create_knowledge_base(documents: List[Dict[str, str]], config: dict): \"\"\"Create embeddings and FAISS index from documents\"\"\" print(\"🔄 Creating knowledge base...\") # Chunk all documents all_chunks = [] chunk_metadata = [] for doc in documents: doc_chunks = smart_chunk_text(doc[\"text\"]) for chunk in doc_chunks: if len(chunk.split()) < 5: # Skip very short chunks continue all_chunks.append(chunk) chunk_metadata.append({ \"text\": chunk, \"source\": doc[\"source\"], \"section\": doc[\"section\"], \"hash\": hashlib.md5(chunk.encode()).hexdigest()[:8] # For deduplication }) print(f\"✂️ Created {len(all_chunks)} text chunks\") if not all_chunks: print(\"❌ No chunks created! Check your content.\") return False # Initialize embedder try: model_name = config[\"embeddings\"][\"model_name\"] print(f\"🧠 Loading embedding model: {model_name}\") embedder = SentenceTransformer(model_name) except Exception as e: print(f\"❌ Failed to load embedding model: {e}\") return False # Create embeddings try: print(\"🔄 Creating embeddings...\") embeddings = embedder.encode( all_chunks, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True ) print(f\"✅ Created embeddings: {embeddings.shape}\") except Exception as e: print(f\"❌ Failed to create embeddings: {e}\") return False # Create and save FAISS index try: dim = embeddings.shape[1] index = faiss.IndexFlatL2(dim) index.add(embeddings.astype('float32')) index_path = os.path.join(INDEX_DIR, \"faiss.index\") faiss.write_index(index, index_path) print(f\"💾 Saved FAISS index: {index_path}\") except Exception as e: print(f\"❌ Failed to create FAISS index: {e}\") return False", "source": "Repo:TanzilGPT:prep_index.py", "section": "TanzilGPT", "hash": "8a7a12c2"}
{"text": "as e: print(f\"❌ Failed to load embedding model: {e}\") return False # Create embeddings try: print(\"🔄 Creating embeddings...\") embeddings = embedder.encode( all_chunks, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True ) print(f\"✅ Created embeddings: {embeddings.shape}\") except Exception as e: print(f\"❌ Failed to create embeddings: {e}\") return False # Create and save FAISS index try: dim = embeddings.shape[1] index = faiss.IndexFlatL2(dim) index.add(embeddings.astype('float32')) index_path = os.path.join(INDEX_DIR, \"faiss.index\") faiss.write_index(index, index_path) print(f\"💾 Saved FAISS index: {index_path}\") except Exception as e: print(f\"❌ Failed to create FAISS index: {e}\") return False # Save chunk metadata try: chunks_path = os.path.join(INDEX_DIR, \"chunks.jsonl\") with open(chunks_path, \"w\", encoding=\"utf-8\") as f: for chunk_meta in chunk_metadata: f.write(json.dumps(chunk_meta, ensure_ascii=False) + \"\\n\") print(f\"💾 Saved chunk metadata: {chunks_path}\") except Exception as e: print(f\"❌ Failed to save chunks: {e}\") return False return True def main(): \"\"\"Main indexing pipeline\"\"\" print(\"🚀 Starting knowledge base creation...\") # Load configuration config = load_config() if not config: print(\"❌ Cannot proceed without configuration file\") return False # Ensure directories exist ensure_directories() # Extract GitHub URLs repo_urls = extract_github_urls(CONTENT_DIR) print(f\"🔗 Found {len(repo_urls)} GitHub repositories\") if not repo_urls and not os.path.exists(CONTENT_DIR): print(\"❌ No content found! Please ensure you have either:\") print(\" 1. Markdown files in 'content/' directory\") print(\" 2. GitHub URLs in your markdown files\") return False # Collect all documents documents = collect_all_documents(CONTENT_DIR, repo_urls) if not documents: print(\"❌ No documents collected! Check your content sources.\") return False # Create knowledge base success = create_knowledge_base(documents, config) if success: print(\"\\n🎉 Knowledge base creation complete!\") print(f\"📊 Statistics:\") print(f\" • Documents processed: {len(documents)}\") print(f\" • GitHub repositories: {len(repo_urls)}\") print(f\" • Index saved to: {INDEX_DIR}/\") print(\"\\n✅ Your AI CV chatbot is ready! Run 'streamlit run app.py' to start.\") else: print(\"❌ Knowledge base creation failed!\") return success if __name__ == \"__main__\": main()", "source": "Repo:TanzilGPT:prep_index.py", "section": "TanzilGPT", "hash": "9445de0c"}
{"text": "# MUST BE FIRST - Page config import streamlit as st st.set_page_config( page_title=\"Chat with Mohammad Tanzil Alam\", page_icon=\"🤖\", layout=\"wide\" ) import json import os import yaml import faiss import numpy as np from sentence_transformers import SentenceTransformer from llama_cpp import Llama from huggingface_hub import hf_hub_download from typing import List, Dict import time # ---------- Load config ---------- @st.cache_data def load_config(): try: with open(\"config.yaml\", \"r\", encoding=\"utf-8\") as f: return yaml.safe_load(f) except FileNotFoundError: st.error(\"❌ config.yaml not found!\") st.stop() CFG = load_config() # ---------- Download and load model ---------- @st.cache_resource def load_model(): \"\"\"Download and load Llama model with better error handling\"\"\" models_dir = \"models\" os.makedirs(models_dir, exist_ok=True) model_filename = CFG[\"llm\"][\"gguf_file\"] local_model_path = os.path.join(models_dir, model_filename) # Download model if missing if not os.path.exists(local_model_path): st.info(f\"🔄 Downloading {model_filename}... This may take a few minutes.\") progress_bar = st.progress(0) status_text = st.empty() try: downloaded_path = hf_hub_download( repo_id=CFG[\"llm\"][\"gguf_repo\"], filename=CFG[\"llm\"][\"gguf_file\"], local_dir=\"models\", local_dir_use_symlinks=False, token=os.getenv(\"HF_TOKEN\") ) if os.path.exists(downloaded_path): local_model_path = downloaded_path progress_bar.progress(100) status_text.success(\"✅ Model downloaded successfully!\") time.sleep(1) progress_bar.empty() status_text.empty() else: st.error(\"❌ Model download failed - file not found\") st.stop() except Exception as e: st.error(f\"❌ Model download failed: {e}\") st.info(\"💡 Try using a smaller model or check your internet connection\") st.stop() # Load Llama model try: st.info(\"🤖 Loading language model...\") llm = Llama( model_path=local_model_path, n_ctx=CFG[\"llm\"].get(\"n_ctx\", 4096), n_threads=CFG[\"llm\"].get(\"n_threads\", 4), n_gpu_layers=0, # CPU only for compatibility verbose=False, use_mlock=False, # Reduce memory usage n_batch=128 # Smaller batch size ) st.success(\"✅ Language model loaded successfully!\") return llm except Exception as e: st.error(f\"❌ Failed to load model: {e}\") st.info(\"💡 Try restarting the app or using a smaller model\") st.stop() # ---------- Load embeddings & FAISS index ---------- @st.cache_resource def load_retrieval_components(): try: st.info(\"🔄 Loading embedding model...\") embedder = SentenceTransformer(CFG[\"embeddings\"][\"model_name\"]) index_path = \"index/faiss.index\" chunks_path = \"index/chunks.jsonl\" if not os.path.exists(index_path) or not os.path.exists(chunks_path): st.error(\"❌ Index files not found! Run prep_index.py first.\") st.stop() st.info(\"🔄 Loading FAISS index...\") index = faiss.read_index(index_path) chunks = [] with open(chunks_path, \"r\", encoding=\"utf-8\") as f: for line in f: if line.strip(): chunks.append(json.loads(line.strip())) st.success(f\"✅ Loaded embeddings and {len(chunks)} chunks!\") return embedder, index, chunks except Exception as e: st.error(f\"❌ Failed to load retrieval components: {e}\") st.stop() # Initialize all components llm = load_model() embedder, index, CHUNKS = load_retrieval_components() # ---------- Core RAG functions ---------- def _embed(text: str) -> np.ndarray: \"\"\"Create embedding for query text\"\"\" try: embedding = embedder.encode([text], normalize_embeddings=True) return embedding[0] except Exception as e: st.error(f\"❌ Embedding failed: {e}\") return np.array([])", "source": "Repo:TanzilGPT:app.py", "section": "TanzilGPT", "hash": "6aaf1ea8"}
{"text": "with open(chunks_path, \"r\", encoding=\"utf-8\") as f: for line in f: if line.strip(): chunks.append(json.loads(line.strip())) st.success(f\"✅ Loaded embeddings and {len(chunks)} chunks!\") return embedder, index, chunks except Exception as e: st.error(f\"❌ Failed to load retrieval components: {e}\") st.stop() # Initialize all components llm = load_model() embedder, index, CHUNKS = load_retrieval_components() # ---------- Core RAG functions ---------- def _embed(text: str) -> np.ndarray: \"\"\"Create embedding for query text\"\"\" try: embedding = embedder.encode([text], normalize_embeddings=True) return embedding[0] except Exception as e: st.error(f\"❌ Embedding failed: {e}\") return np.array([]) def retrieve(query: str, k: int = 5) -> List[Dict]: \"\"\"Retrieve relevant chunks using FAISS semantic search\"\"\" try: query_embedding = _embed(query) if query_embedding.size == 0: return [] # Search FAISS index distances, indices = index.search( query_embedding.reshape(1, -1).astype('float32'), k ) results = [] for i, (distance, idx) in enumerate(zip(distances[0], indices[0])): if idx < len(CHUNKS): chunk = CHUNKS[idx].copy() chunk[\"similarity\"] = float(1 - distance) chunk[\"rank\"] = i + 1 results.append(chunk) return results except Exception as e: st.error(f\"❌ Retrieval failed: {e}\") return [] # ---------- LLM Generation ---------- SYSTEM_PROMPT = f\"\"\"You are {CFG['persona']['name']}, an AI assistant representing Mohammad Tanzil Alam's professional profile. Your personality: {CFG['persona']['style']} Instructions: - Answer questions about Mohammad's background, skills, projects, and experience - Use the provided context to give accurate, specific answers - If you don't know something, say so honestly - Keep responses professional but approachable - Focus on Mohammad's technical skills, projects, and career journey - Be conversational and helpful Context will be provided below. Use it to answer questions accurately.\"\"\" def check_guardrails(question: str) -> bool: \"\"\"Check if question contains blocked phrases\"\"\" blocked = CFG.get(\"guardrails\", {}).get(\"blocked_phrases\", []) question_lower = question.lower() for phrase in blocked: if phrase.lower() in question_lower: return False return True def generate_answer(question: str, context: str) -> str: \"\"\"Generate answer using local Llama model\"\"\" try: # Build prompt prompt = f\"\"\"{SYSTEM_PROMPT} Context from Mohammad's profile: {context} Question: {question} Answer:\"\"\" # Generate response response = llm( prompt, max_tokens=CFG[\"llm\"].get(\"max_tokens\", 512), temperature=CFG[\"llm\"].get(\"temperature\", 0.2), top_p=CFG[\"llm\"].get(\"top_p\", 0.9), stop=[\"Question:\", \"Context:\", \"\\n\\n---\"], echo=False ) answer = response[\"choices\"][0][\"text\"].strip() # Clean up response if \"Answer:\" in answer: answer = answer.split(\"Answer:\")[-1].strip() return answer except Exception as e: st.error(f\"❌ Generation failed: {e}\") return \"I apologize, but I'm having trouble generating a response right now. Please try asking your question again.\" # ---------- Streamlit UI ---------- def main(): st.title(f\"💬 Chat with {CFG['persona']['name']}\") st.markdown(\"Ask me anything about Mohammad's background, skills, and experience!\")", "source": "Repo:TanzilGPT:app.py", "section": "TanzilGPT", "hash": "6bc3ea4f"}
{"text": "# Generate response response = llm( prompt, max_tokens=CFG[\"llm\"].get(\"max_tokens\", 512), temperature=CFG[\"llm\"].get(\"temperature\", 0.2), top_p=CFG[\"llm\"].get(\"top_p\", 0.9), stop=[\"Question:\", \"Context:\", \"\\n\\n---\"], echo=False ) answer = response[\"choices\"][0][\"text\"].strip() # Clean up response if \"Answer:\" in answer: answer = answer.split(\"Answer:\")[-1].strip() return answer except Exception as e: st.error(f\"❌ Generation failed: {e}\") return \"I apologize, but I'm having trouble generating a response right now. Please try asking your question again.\" # ---------- Streamlit UI ---------- def main(): st.title(f\"💬 Chat with {CFG['persona']['name']}\") st.markdown(\"Ask me anything about Mohammad's background, skills, and experience!\") # Status indicators col1, col2, col3 = st.columns(3) with col1: st.metric(\"🤖 LLM Status\", \"✅ Loaded\") with col2: st.metric(\"📚 Knowledge Base\", f\"{len(CHUNKS)} chunks\") with col3: st.metric(\"🔍 Search Type\", \"Semantic (FAISS)\") # Initialize chat history if \"messages\" not in st.session_state: st.session_state.messages = [ {\"role\": \"assistant\", \"content\": f\"Hi! I'm {CFG['persona']['name']}'s AI assistant. I can answer questions about Mohammad's background, skills, projects, and experience using advanced language AI. What would you like to know?\"} ] # Display chat messages for message in st.session_state.messages: with st.chat_message(message[\"role\"]): st.markdown(message[\"content\"]) # Chat input if prompt := st.chat_input(\"Ask about Mohammad's background...\"): # Check guardrails if not check_guardrails(prompt): st.warning(\"⚠️ Please ask appropriate questions about professional topics.\") return # Add user message st.session_state.messages.append({\"role\": \"user\", \"content\": prompt}) with st.chat_message(\"user\"): st.markdown(prompt) # Generate response with st.chat_message(\"assistant\"): with st.spinner(\"🔍 Searching knowledge base...\"): # Retrieve relevant context using semantic search retrieved_chunks = retrieve(prompt, k=CFG[\"retriever\"][\"k\"]) if retrieved_chunks: context = \"\\n\\n\".join([ f\"Source: {chunk['source']}\\nContent: {chunk['text']}\" for chunk in retrieved_chunks ]) # Show retrieved sources with st.expander(f\"📚 Retrieved {len(retrieved_chunks)} relevant sources\"): for i, chunk in enumerate(retrieved_chunks, 1): st.write(f\"**{i}.** {chunk['source']} (similarity: {chunk['similarity']:.3f})\") st.write(f\"_{chunk['text'][:200]}..._\") # Generate LLM response with st.spinner(\"🤖 Generating AI response...\"): answer = generate_answer(prompt, context) st.markdown(answer) st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer}) else: fallback = \"I couldn't find specific information about that in Mohammad's profile. Could you try asking about his skills, projects, education, or work experience?\" st.markdown(fallback) st.session_state.messages.append({\"role\": \"assistant\", \"content\": fallback}) # Sidebar with st.sidebar: st.header(\"🤖 AI System Info\") st.success(\"**Full LLM + RAG System**\") st.write(\"✅ Semantic Search (FAISS)\") st.write(\"✅ Language Model (Llama)\") st.write(\"✅ Context-Aware Responses\") st.markdown(\"---\") st.header(\"ℹ️ About\") st.write(f\"Advanced AI assistant for **{CFG['persona']['name']}**\") st.markdown(\"---\") st.write(\"**Available Topics:**\") st.write(\"• 🛠️ Technical Skills\") st.write(\"• 📦 Projects & Work\") st.write(\"• 🎓 Education\") st.write(\"• 💼 Professional Experience\") if __name__ == \"__main__\": main()", "source": "Repo:TanzilGPT:app.py", "section": "TanzilGPT", "hash": "218d2ea9"}
{"text": "# Projects ### Unit Testing Sample Built with Python Repo: https://github.com/tanzilalam23/CI_unit_test ### DASCM Project Personal portfolio made with React and Tailwind. Repo: https://github.com/tanzilalam23/DASCM ### Data Drift Detection Repo: https://github.com/tanzilalam23/Master-Thesis-Data-Drift ### Data Analysis Project Repo: https://github.com/tanzilalam23/Data-Analysis-Project ### ATS AI-powered chatbot using Gemini Pro. Repo: https://github.com/tanzilalam23/ATS ### Web Scrapping Repo: https://github.com/tanzilalam23/Web-Scraping ### Drug Classification Repo: https://github.com/tanzilalam23/Drug-Classification ### NASA Battery Analysis Repo: https://github.com/tanzilalam23/NASA-Battery-project ### KNN Classifier Repo: https://github.com/tanzilalam23/KNN-ML-project ### Automated Data drift Repo: https://github.com/tanzilalam23/Detecting-Data-Drift-_Automated ### Data Analysis Repo: https://github.com/tanzilalam23/Data_Analytics ### Chatbot CV Assistant AI-powered chatbot version of me, running on Hugging Face. Repo: https://github.com/tanzilalam23/Master-Thesis-Data-Drift", "source": "Repo:TanzilGPT:content/02_projects.md", "section": "TanzilGPT", "hash": "7048b536"}
{"text": "# Summary Research-driven Data Engineer with experience across academic and industry settings. Skilled in designing and building scalable datapipelines, implementing data quality frameworks, and optimizing ELT processes. Proficient in Python, Spark, SQL, and AWS, with hands-on experience in large-scale data environments. Collaborates effectively across teams to enable efficient data access, analysis, anddecision-making. Fluent in English (C1) and certified B1 in German; strong communicator and team player. # Skills Languages: Python, TypeScript, SQL, Java, C, C++, R, PySpark Frameworks: FastAPI, Streamlit ML/AI: scikit-learn, PyTorch (basics), RAG (Retrieval-Augmented Generation), FAISS, embeddings, Hugging Face (LLM), Vector Databases, Data Parsing & Extraction, sentence-transformers, nbformat, BeautifulSoup Cloud/DevOps: Docker, CI/CD (GitHub Actions, GitLab CI), Linux, AWS (certified), Azure DevOps, REST API, Terraform, GitPython Project Management: JIRA, Trello, Agile Methodologies IDEs & Code Editors: Visual Studio Code (VS Code), Jupyter Notebook Data Visualization & Business Intelligence: MS PowerBI Deployment / Web App Skills: Streamlit apps deployment, Hugging Face Spaces hosting, end-to-end AI pipeline management (ingestion → embedding → chatbot response) # Experience - Associate Consultant Data Engineer,Arcondis GmbH (Jun 2025 – Aug 2025) Engineered an automated data pipeline to compute 30+ KPIs, integrating data from Monday.com via REST API and internal Abacus database using JDBC. Built a helper automation to scan and isolate relevant tables within large-scale Abacus database—optimized execution time to ~3–4 minutes. Fully automated the workflow end-to-end, enabling scheduled KPI updates with zero manual intervention and improved reporting cadence. Interacted with stakeholders to understand the requirements and to give weekly updates. - Online Tutor,Self Employed (Jan 2024 – May 2025) Provide expert tutoring and academic supervision to bachelor's and master's students in Data Engineering, Cloud Computing, DevOps, and Databases. Design and deliver structured lesson plans and presentations to facilitate comprehensive learning experiences. Conduct collaborative sessions, including pair programming, to enhance student engagement and foster an interactive learning environment. - Data Engineer,Roche Diagnostics GmbH (Feb 2023 – Jul 2023 Penzberg, Germany) Built a Python ETL pipeline for clinical pathological datasets (OCR, PDF, text) to detect data drift. Applied NLP and text mining techniques for transformation of unstructured to structured data. Utilized ML algorithms, with Word2Vec enhancing results by 99.5%. Implemented Cosine similarity vectors to analyze and quantify differences across reports. Automated deployment processes using DevOps tools (Git, AWS, Docker), optimizing efficiency and scalability.", "source": "Repo:TanzilGPT:content/01_cv_public.md", "section": "TanzilGPT", "hash": "d8851b89"}
{"text": "and foster an interactive learning environment. - Data Engineer,Roche Diagnostics GmbH (Feb 2023 – Jul 2023 Penzberg, Germany) Built a Python ETL pipeline for clinical pathological datasets (OCR, PDF, text) to detect data drift. Applied NLP and text mining techniques for transformation of unstructured to structured data. Utilized ML algorithms, with Word2Vec enhancing results by 99.5%. Implemented Cosine similarity vectors to analyze and quantify differences across reports. Automated deployment processes using DevOps tools (Git, AWS, Docker), optimizing efficiency and scalability. - Apprentice Data Engineer,Roche Diagnostics GmbH (Jun 2022 – Aug 2022 Penzberg, Germany) Collaborated in Agile development through daily stand-ups and sprint planning. Established a cloud-based ETL data pipeline using AWS Glue, S3, and Athena. Utilized SQL optimization techniques in Athena for ad hoc data analysis. Used PySpark for ETL into a centralized data lake. Boosted uptime from 48% to 87% using Amazon CloudWatch monitoring. - Software Engineer,Fortress6 Technologies (Jun 2019 – May 2021, India) Collaborated with project managers, engineers, and stakeholders to support ongoing projects. Reduced data retrieval time by 60% through SQL optimization Automated AWS microservice deployment using Terraform (IaC) for 13+ ISPs. Implemented CI/CD pipelines using Git for 60+ clients. Mentored 5 junior developers, improving code quality by 25%. Enhanced backend development, positively impacting company performance. # Education - MSc. Data Engineering, Jacobs (Constructor) University, Bremen, Germany - B.Tech Computer Science & Engineering, Uttarakhand Technical University, India # LANGUAGE - English: Fluent - Hindi: Native - German: B1+ # AWARDS/ CERTIFICATIONS - Recipient of 100% scholarship for MSc: Roche Cooperative Study Program - Academic merit scholarship: Jacobs University Bremen - AWS Cloud Computing and Deployment: WebTek Labs Pvt. Ltd. - A+ in MySQL training from Microsoft, ranking among the top 5%: Microsoft Technology Associate", "source": "Repo:TanzilGPT:content/01_cv_public.md", "section": "TanzilGPT", "hash": "63b2150e"}
{"text": "## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell ```python #tfidf(df_train,df_validation) ``` ## Code Cell ```python #w2v(df_train,df_validation) ```", "source": "Repo:Detecting-Data-Drift-_Automated:train_module.ipynb", "section": "Detecting-Data-Drift-_Automated", "hash": "a7262c05"}
{"text": "## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted] ## Code Cell [Long code block omitted]", "source": "Repo:Detecting-Data-Drift-_Automated:word2vec.ipynb", "section": "Detecting-Data-Drift-_Automated", "hash": "b4d7596a"}
{"text": "# Detecting-Data-Drift-_Automated # Data Drift Analysis ## Abstract Data drift refers to a shift in the distribution of unseen input data compared to the training data. In the context of deploying a production pipeline, maintaining similarity between processed reports and the trained data is crucial. Analyzing reports from different domains (e.g., radiotherapy) poses a risk of poor quality abstractions, potentially leading to incorrect clinical inferences and risking patient safety. Data drift may also stem from differences in report layouts. This project aims to assess data drift in unseen reports compared to the training dataset. ## Workflow The project primarily involves processing OCR reports, converting them into text files, and conducting operations to detect data drift. The pipeline includes the following steps: 1. **Analysis Module**: Imports `datadrift_module`. 2. **Datadrift Module**: Imports `preprocessing_module`. Utilizes `threshold`, `train`, and `model` from `train_module`. 3. **Train Module**: Calculates thresholds, models, train embeddings, and vectors for TF-IDF and Word2Vec vectorization. ## Steps/Operations This section describes the functions and features of four core modules: ### 1. Train Module This module calculates and saves thresholds, models, train embeddings, and vectors for TF-IDF and Word2Vec vectorization using the training and validation datasets. #### a. Word2Vec Function - Calculates the Word2Vec vectorizer model and train_embeddings. - Computes cosine mean similarity and determines the 10th percentile as the threshold. #### b. TF-IDF Function - Computes TF-IDF vectorization. - Calculates cosine mean similarity and establishes the 10th percentile as the threshold. ### 2. Preprocessing Module This module preprocesses both the training and unseen datasets to prepare the text data for further operations. Customization is possible based on specific requirements. ### 3. Datadrift Module The central module for data drift calculation. The `predict_data_drift` function takes unseen dataset and a boolean result (for TF-IDF or Word2Vec calculation). - Preprocesses the unseen dataset and checks if TF-IDF or Word2Vec calculation is required. - Utilizes previously saved variables (thresholds, vectorizers, train data) from the Train Module to compute data drift. - Notifies if data drift exists between each unseen document and the training document. ### 4. Analysis Module Importing the Datadrift Module, this module calls the `predict_data_drift` function with parameters (i.e., location of the unseen dataset) to analyze and report the results. ## Note Modules can be adapted and customized as needed to fit specific project requirements. The descriptions provide an overview of their responsibilities and intended functionalities.", "source": "Repo:Detecting-Data-Drift-_Automated:README.md", "section": "Detecting-Data-Drift-_Automated", "hash": "1cc8a0a5"}
{"text": "## Code Cell [Long code block omitted]python df.info() [Long code block omitted] ## Markdown Cell We see that Na_to_K is right skewed, so performing log tranformation to look like normal distribution ## Code Cell [Long code block omitted]python # Categorical variables categorical_cols=df.select_dtypes(include=object).columns.to_list() categorical_cols [Long code block omitted] ## Code Cell [Long code block omitted]python from sklearn.model_selection import cross_val_score result=cross_val_score(k,x_train,y_train,cv=10) print(result) print(\"Average Result\", result.mean()) [Long code block omitted] ## Markdown Cell We get an accuracy of 80%. ## Markdown Cell #### Now checking the performance of testing and training dataset for different n_neighbors values. I will choose 1-40. ## Code Cell [Long code block omitted]", "source": "Repo:Drug-Classification:KNN in drug classification.ipynb", "section": "Drug-Classification", "hash": "67e2bdff"}
{"text": "# Drug Classification Analysis ## Overview This project focuses on analyzing a drug classification dataset. The analysis includes data preprocessing, model implementation, and performance evaluation. ## Dataset - **Source:** `drug.csv` - **Description:** The dataset contains information about drug classifications based on attributes like Age, Sex, Blood Pressure (BP), Cholesterol, Sodium to Potassium Ratio (Na_to_K), and the corresponding drug label. ## Data Preprocessing - Checked for missing values (None found). - Encoded categorical variables using Label Encoding. - Explored unique values for each categorical attribute. ## Univariate Analysis - Conducted univariate analysis for numerical attributes: 'Age' and 'Na_to_K'. - Applied log transformation to 'Na_to_K' to achieve a normal distribution. ## Model Implementation - Utilized K-Nearest Neighbors (KNN) classifier for drug classification. - Split the dataset into training and testing sets. - Applied Standard Scaling for feature scaling. - Explored different 'n_neighbors' values in KNN. - Evaluated the model using confusion matrices and performance metrics. ## Results - Achieved an accuracy of 80%. - Investigated the impact of 'n_neighbors' on model performance.", "source": "Repo:Drug-Classification:README.md", "section": "Drug-Classification", "hash": "0e6fdbab"}
{"text": "KNN in drug classification In [1]: import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns In [2]: df = pd . read_csv ( \"drug.csv\" ) # Taking dataset df . head () Out[2]: Age Sex BP Cholesterol Na_to_K Drug 0 23 F HIGH HIGH 25.355 DrugY 1 47 M LOW HIGH 13.093 drugC 2 47 M LOW HIGH 10.114 drugC 3 28 F NORMAL HIGH 7.798 drugX 4 61 F LOW HIGH 18.043 DrugY Data Preprocessing ¶ In [3]: # Finding null values df . isnull () . sum () Out[3]: Age 0 Sex 0 BP 0 Cholesterol 0 Na_to_K 0 Drug 0 dtype: int64 In [4]: df [ 'Drug' ] . unique () Out[4]: array(['DrugY', 'drugC', 'drugX', 'drugA', 'drugB'], dtype=object) In [5]: df . shape # Checking number of rows and columns Out[5]: (200, 6) In [6]: df . isna () . sum () #Checking for NaN values Out[6]: Age 0 Sex 0 BP 0 Cholesterol 0 Na_to_K 0 Drug 0 dtype: int64 In [7]: df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 200 entries, 0 to 199 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Age 200 non-null int64 1 Sex 200 non-null object 2 BP 200 non-null object 3 Cholesterol 200 non-null object 4 Na_to_K 200 non-null float64 5 Drug 200 non-null object dtypes: float64(1), int64(1), object(4) memory usage: 9.5+ KB In [8]: # Checking for unique values df . nunique () Out[8]: Age 57 Sex 2 BP 3 Cholesterol 2 Na_to_K 198 Drug 5 dtype: int64 So there are 4 categorical variables that is Sex, BP, Cholestrol & Drug And; 2 numerical variable Univariate Analysis ¶ In [9]: # Creating a variable that holds the numeric variable and storing it as a list numerical_cols = df . select_dtypes ( include = np . number ) . columns . to_list () numerical_cols Out[9]: ['Age', 'Na_to_K'] In [10]: for col in numerical_cols : plt . figure ( figsize = ( 14 , 5 )) sns . histplot ( data = df , x = col , bins = 10 , kde = True ) plt . title ( f \"Distribution of { col } \" ) We see that Na_to_K is right skewed, so performing log tranformation to look like normal distribution In [11]: df [ 'Na_to_K' ] = np .", "source": "Repo:Drug-Classification:KNN in drug classification.html", "section": "Drug-Classification", "hash": "92bac5c1"}
{"text": "() numerical_cols Out[9]: ['Age', 'Na_to_K'] In [10]: for col in numerical_cols : plt . figure ( figsize = ( 14 , 5 )) sns . histplot ( data = df , x = col , bins = 10 , kde = True ) plt . title ( f \"Distribution of { col } \" ) We see that Na_to_K is right skewed, so performing log tranformation to look like normal distribution In [11]: df [ 'Na_to_K' ] = np . log ( df [ 'Na_to_K' ]) In [12]: plt . figure ( figsize = ( 14 , 5 )) sns . histplot ( data = df , x = df [ 'Na_to_K' ], bins = 10 , kde = True ) plt . title ( f \"Distribution of Na_to_K\" ) Out[12]: Text(0.5, 1.0, 'Distribution of Na_to_K') In [13]: sns . histplot ( data = df , x = df [ 'Drug' ]) Out[13]: <AxesSubplot:xlabel='Drug', ylabel='Count'> There is an imbalance in the target labels i.e.; Drug Y is abundantly present. Hence to compensate this and ignore the baising of the result, I will be using class weights to balance this situation by putting weights= \"uniform\" during knn classification here . ¶ In [14]: # Categorical variables categorical_cols = df . select_dtypes ( include = object ) . columns . to_list () categorical_cols Out[14]: ['Sex', 'BP', 'Cholesterol', 'Drug'] In [15]: # Encoding Categorical Variable from sklearn.preprocessing import LabelEncoder le = LabelEncoder () for n in categorical_cols : df [ n ] = le . fit_transform ( df [ n ]) df . head () Out[15]: Age Sex BP Cholesterol Na_to_K Drug 0 23 0 0 0 3.232976 0 1 47 1 1 0 2.572078 3 2 47 1 1 0 2.313921 3 3 28 0 2 0 2.053867 4 4 61 0 1 0 2.892758 0 In [16]: # Distribution of dependent and independent variable x = df . iloc [:,: 5 ] y = df . iloc [:, - 1 ] print ( x . head (), \" \\n \" ) print ( y . head ()) Age Sex BP Cholesterol Na_to_K 0 23 0 0 0 3.232976 1 47 1 1 0 2.572078 2 47 1 1 0 2.313921 3 28 0 2 0 2.053867 4 61 0 1 0 2.892758", "source": "Repo:Drug-Classification:KNN in drug classification.html", "section": "Drug-Classification", "hash": "ccd741ae"}
{"text": "0 2.892758 0 In [16]: # Distribution of dependent and independent variable x = df . iloc [:,: 5 ] y = df . iloc [:, - 1 ] print ( x . head (), \" \\n \" ) print ( y . head ()) Age Sex BP Cholesterol Na_to_K 0 23 0 0 0 3.232976 1 47 1 1 0 2.572078 2 47 1 1 0 2.313921 3 28 0 2 0 2.053867 4 61 0 1 0 2.892758 0 0 1 3 2 3 3 4 4 0 Name: Drug, dtype: int64 In [17]: # Splitting into train and test set from sklearn.model_selection import train_test_split x_train , x_test , y_train , y_test = train_test_split ( x , y , random_state = 0 ) #Splitting test size to 25% as default value x_train . shape , y_train . shape , x_test . shape , y_test . shape # Checking the size of each test and train set Out[17]: ((150, 5), (150,), (50, 5), (50,)) In [18]: # Feature Scaling from sklearn.preprocessing import StandardScaler sc = StandardScaler () x_train = sc . fit_transform ( x_train ) x_test = sc . transform ( x_test ) In [19]: from sklearn.neighbors import KNeighborsClassifier # Using Euclidean distance # Using weights to ignore biasing of target variable k = KNeighborsClassifier ( n_neighbors = 5 , metric = \"minkowski\" , p = 2 , weights = \"uniform\" ) Implementing cross validation to train model on multiple train-test splits ¶ In [20]: from sklearn.model_selection import cross_val_score result = cross_val_score ( k , x_train , y_train , cv = 10 ) print ( result ) print ( \"Average Result\" , result . mean ()) [0.86666667 0.93333333 0.73333333 0.93333333 0.73333333 0.66666667 0.66666667 1. 0.86666667 0.86666667] Average Result 0.8266666666666668 As seen from the above result after applying K fold cross validation the lower accuracy of the model is 66.67% and the highest accuracy is 100, which can also be seen from the graph here In [21]: k . fit ( x_train , y_train ) # Training models Out[21]: KNeighborsClassifier() In [22]: # Making prediction y_pred = k . predict ( x_test ) # It will do the prediction of new data and give us the O/P In [23]: # Creating Confusion metrics from sklearn import metrics cm = metrics . confusion_matrix ( y_test , y_pred ) #To compare predicted result with the actual result sns .", "source": "Repo:Drug-Classification:KNN in drug classification.html", "section": "Drug-Classification", "hash": "14330f76"}
{"text": "can also be seen from the graph here In [21]: k . fit ( x_train , y_train ) # Training models Out[21]: KNeighborsClassifier() In [22]: # Making prediction y_pred = k . predict ( x_test ) # It will do the prediction of new data and give us the O/P In [23]: # Creating Confusion metrics from sklearn import metrics cm = metrics . confusion_matrix ( y_test , y_pred ) #To compare predicted result with the actual result sns . heatmap ( cm , annot = True ) accuracy = metrics . accuracy_score ( y_test , y_pred ) # Checking accuracy of a model print ( \"Accuracy score:\" , accuracy ) print ( \"Precision Score : \" , metrics . precision_score ( y_test , y_pred , average = 'macro' )) print ( \"Recall Score : \" , metrics . recall_score ( y_test , y_pred , average = 'macro' )) Accuracy score: 0.8 Precision Score : 0.7779411764705882 Recall Score : 0.7838333333333333 We get an accuracy of 80%. Now checking the performance of testing and training dataset for different n_neighbors values. I will choose 1-40. ¶ In [24]: training_accuracy = [] testing_accuracy = [] for i in range ( 1 , 40 ): knn = KNeighborsClassifier ( n_neighbors = i ) knn . fit ( x_train , y_train ) training_accuracy . append ( knn . score ( x_train , y_train )) testing_accuracy . append ( knn . score ( x_test , y_test )) # accuracy(y_test, y_pred) Graph fro 1-40 knn values ¶ In [25]: # Plotting test, train accuracy with n_neighbors plt . figure ( figsize = ( 10 , 5 )) plt . plot ( range ( 1 , 40 ), testing_accuracy , label = \"Testing Accuracy\" ) plt . plot ( range ( 1 , 40 ), training_accuracy , label = \"Training Accuracy\" ) plt . title ( 'Training vs Testing Accuracy' ) plt . xlabel ( 'n_neighbors' ) plt . ylabel ( 'Accuracy' ) plt . legend ( loc = 'best' ) plt . show ()", "source": "Repo:Drug-Classification:KNN in drug classification.html", "section": "Drug-Classification", "hash": "0088f087"}
