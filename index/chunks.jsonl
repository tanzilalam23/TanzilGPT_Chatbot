{"text": "# Projects ### Unit Testing Sample Built with Python Repo: https://github.com/tanzilalam23/CI_unit_test ### DASCM Project Personal portfolio made with React and Tailwind. Repo: https://github.com/tanzilalam23/DASCM ### Data Drift Detection Repo: https://github.com/tanzilalam23/Master-Thesis-Data-Drift ### Data Analysis Project Repo: https://github.com/tanzilalam23/Data-Analysis-Project ### ATS AI-powered chatbot using Gemini Pro. Repo: https://github.com/tanzilalam23/ATS ### Web Scrapping Repo: https://github.com/tanzilalam23/Web-Scraping ### Drug Classification Repo: https://github.com/tanzilalam23/Drug-Classification ### NASA Battery Analysis Repo: https://github.com/tanzilalam23/NASA-Battery-project ### KNN Classifier Repo: https://github.com/tanzilalam23/KNN-ML-project ### Automated Data drift Repo: https://github.com/tanzilalam23/Detecting-Data-Drift-_Automated ### Data Analysis Repo: https://github.com/tanzilalam23/Data_Analytics ### Chatbot CV Assistant AI-powered chatbot version of me, running on Hugging Face. Repo: https://github.com/tanzilalam23/Master-Thesis-Data-Drift", "source": "CV:02_projects.md", "section": "02_projects", "hash": "5355279b"}
{"text": "# Links - GitHub: https://github.com/tanzilalam23 - LinkedIn: https://www.linkedin.com/in/mohammad-tanzil-alam/ - Email: mohammadtanzilalam@gmail.com", "source": "CV:03_links.md", "section": "03_links", "hash": "3adfc1cb"}
{"text": "# Summary Research-driven Data Engineer with experience across academic and industry settings. Skilled in designing and building scalable datapipelines, implementing data quality frameworks, and optimizing ELT processes. Proficient in Python, Spark, SQL, and AWS, with hands-on experience in large-scale data environments. Collaborates effectively across teams to enable efficient data access, analysis, anddecision-making. Fluent in English (C1) and certified B1 in German; strong communicator and team player. # Skills Languages: Python, TypeScript, SQL, Java, C, C++, R, PySpark Frameworks: FastAPI, Streamlit ML/AI: scikit-learn, PyTorch (basics), RAG (Retrieval-Augmented Generation), FAISS, embeddings, Hugging Face (LLM), Vector Databases, Data Parsing & Extraction, sentence-transformers, nbformat, BeautifulSoup Cloud/DevOps: Docker, CI/CD (GitHub Actions, GitLab CI), Linux, AWS (certified), Azure DevOps, REST API, Terraform, GitPython Project Management: JIRA, Trello, Agile Methodologies IDEs & Code Editors: Visual Studio Code (VS Code), Jupyter Notebook Data Visualization & Business Intelligence: MS PowerBI Deployment / Web App Skills: Streamlit apps deployment, Hugging Face Spaces hosting, end-to-end AI pipeline management (ingestion → embedding → chatbot response) # Experience - Associate Consultant Data Engineer,Arcondis GmbH (Jun 2025 – Aug 2025) Engineered an automated data pipeline to compute 30+ KPIs, integrating data from Monday.com via REST API and internal Abacus database using JDBC. Built a helper automation to scan and isolate relevant tables within large-scale Abacus database—optimized execution time to ~3–4 minutes. Fully automated the workflow end-to-end, enabling scheduled KPI updates with zero manual intervention and improved reporting cadence. Interacted with stakeholders to understand the requirements and to give weekly updates. - Online Tutor,Self Employed (Jan 2024 – May 2025) Provide expert tutoring and academic supervision to bachelor's and master's students in Data Engineering, Cloud Computing, DevOps, and Databases. Design and deliver structured lesson plans and presentations to facilitate comprehensive learning experiences. Conduct collaborative sessions, including pair programming, to enhance student engagement and foster an interactive learning environment. - Data Engineer,Roche Diagnostics GmbH (Feb 2023 – Jul 2023 Penzberg, Germany) Built a Python ETL pipeline for clinical pathological datasets (OCR, PDF, text) to detect data drift. Applied NLP and text mining techniques for transformation of unstructured to structured data. Utilized ML algorithms, with Word2Vec enhancing results by 99.5%. Implemented Cosine similarity vectors to analyze and quantify differences across reports. Automated deployment processes using DevOps tools (Git, AWS, Docker), optimizing efficiency and scalability. - Apprentice Data Engineer,Roche Diagnostics GmbH (Jun 2022 – Aug 2022 Penzberg, Germany) Collaborated in Agile development through daily stand-ups and sprint planning. Established a cloud-based ETL data pipeline using AWS Glue, S3, and Athena. Utilized SQL optimization techniques in Athena for ad hoc data analysis. Used PySpark for ETL into a centralized data lake. Boosted uptime from 48% to 87% using Amazon CloudWatch monitoring.", "source": "CV:01_cv_public.md", "section": "01_cv_public", "hash": "5b9848d5"}
{"text": "Agile development through daily stand-ups and sprint planning. Established a cloud-based ETL data pipeline using AWS Glue, S3, and Athena. Utilized SQL optimization techniques in Athena for ad hoc data analysis. Used PySpark for ETL into a centralized data lake. Boosted uptime from 48% to 87% using Amazon CloudWatch monitoring. - Software Engineer,Fortress6 Technologies (Jun 2019 – May 2021, India) Collaborated with project managers, engineers, and stakeholders to support ongoing projects. Reduced data retrieval time by 60% through SQL optimization Automated AWS microservice deployment using Terraform (IaC) for 13+ ISPs. Implemented CI/CD pipelines using Git for 60+ clients. Mentored 5 junior developers, improving code quality by 25%. Enhanced backend development, positively impacting company performance. # Education - MSc. Data Engineering, Jacobs (Constructor) University, Bremen, Germany - B.Tech Computer Science & Engineering, Uttarakhand Technical University, India # LANGUAGE - English: Fluent - Hindi: Native - German: B1+ # AWARDS/ CERTIFICATIONS - Recipient of 100% scholarship for MSc: Roche Cooperative Study Program - Academic merit scholarship: Jacobs University Bremen - AWS Cloud Computing and Deployment: WebTek Labs Pvt. Ltd. - A+ in MySQL training from Microsoft, ranking among the top 5%: Microsoft Technology Associate", "source": "CV:01_cv_public.md", "section": "01_cv_public", "hash": "0d708714"}
{"text": "# CODE CELL import streamlit as st import os from PIL import Image import pdf2image import base64 import io import google.generativeai as genai genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\")) def get_gemini_response(input,pdf_cotent,prompt): model=genai.GenerativeModel('gemini-pro-vision') response=model.generate_content([input,pdf_content[0],prompt]) return response.text def input_pdf_setup(uploaded_file): if uploaded_file is not None: ## Convert the PDF to image images=pdf2image.convert_from_bytes(uploaded_file.read()) first_page=images[0] # Convert to bytes img_byte_arr = io.BytesIO() first_page.save(img_byte_arr, format='JPEG') img_byte_arr = img_byte_arr.getvalue() pdf_parts = [ { \"mime_type\": \"image/jpeg\", \"data\": base64.b64encode(img_byte_arr).decode() # encode to base64 } ] return pdf_parts else: raise FileNotFoundError(\"No file uploaded\") ## Streamlit App st.set_page_config(page_title=\"ATS Resume EXpert\") st.header(\"ATS Tracking System\") input_text=st.text_area(\"Job Description: \",key=\"input\") uploaded_file=st.file_uploader(\"Upload your resume(PDF)...\",type=[\"pdf\"]) if uploaded_file is not None: st.write(\"PDF Uploaded Successfully\") submit1 = st.button(\"Tell Me About the Resume\") submit2 = st.button(\"Percentage match\") input_prompt1 = \"\"\" You are an experienced Technical Human Resource Manager,your task is to review the provided resume against the job description. Please share your professional evaluation on whether the candidate's profile aligns with the role. Highlight the strengths and weaknesses of the applicant in relation to the specified job requirements. \"\"\" input_prompt2 = \"\"\" You are an skilled ATS (Applicant Tracking System) scanner with a deep understanding of data science and ATS functionality, your task is to evaluate the resume against the provided job description. give me the percentage of match if the resume matches the job description. First the output should come as percentage and then keywords missing and last final thoughts. \"\"\" if submit1: if uploaded_file is not None: pdf_content=input_pdf_setup(uploaded_file) response=get_gemini_response(input_prompt1,pdf_content,input_text) st.subheader(\"The Repsonse is\") st.write(response) else: st.write(\"Please uplaod the resume\") elif submit2: if uploaded_file is not None: pdf_content=input_pdf_setup(uploaded_file) response=get_gemini_response(input_prompt2,pdf_content,input_text) st.subheader(\"The Repsonse is\") st.write(response) else: st.write(\"Please uplaod the resume\")", "source": "Repo:ATS:ATS.ipynb", "section": "ATS", "hash": "2ad23fe5"}
{"text": "from dotenv import load_dotenv load_dotenv() import base64 import streamlit as st import os import io from PIL import Image import pdf2image import google.generativeai as genai genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\")) def get_gemini_response(input,pdf_cotent,prompt): model=genai.GenerativeModel('gemini-pro-vision') response=model.generate_content([input,pdf_content[0],prompt]) return response.text def input_pdf_setup(uploaded_file): if uploaded_file is not None: ## Convert the PDF to image images=pdf2image.convert_from_bytes(uploaded_file.read()) first_page=images[0] # Convert to bytes img_byte_arr = io.BytesIO() first_page.save(img_byte_arr, format='JPEG') img_byte_arr = img_byte_arr.getvalue() pdf_parts = [ { \"mime_type\": \"image/jpeg\", \"data\": base64.b64encode(img_byte_arr).decode() # encode to base64 } ] return pdf_parts else: raise FileNotFoundError(\"No file uploaded\") ## Streamlit App st.set_page_config(page_title=\"ATS Resume EXpert\") st.header(\"ATS Tracking System\") input_text=st.text_area(\"Job Description: \",key=\"input\") uploaded_file=st.file_uploader(\"Upload your resume(PDF)...\",type=[\"pdf\"]) if uploaded_file is not None: st.write(\"PDF Uploaded Successfully\") submit1 = st.button(\"Tell Me About the Resume\") #submit2 = st.button(\"How Can I Improvise my Skills\") submit3 = st.button(\"Percentage match\") input_prompt1 = \"\"\" You are an experienced Technical Human Resource Manager,your task is to review the provided resume against the job description. Please share your professional evaluation on whether the candidate's profile aligns with the role. Highlight the strengths and weaknesses of the applicant in relation to the specified job requirements. \"\"\" input_prompt3 = \"\"\" You are an skilled ATS (Applicant Tracking System) scanner with a deep understanding of data science and ATS functionality, your task is to evaluate the resume against the provided job description. give me the percentage of match if the resume matches the job description. First the output should come as percentage and then keywords missing and last final thoughts. \"\"\" if submit1: if uploaded_file is not None: pdf_content=input_pdf_setup(uploaded_file) response=get_gemini_response(input_prompt1,pdf_content,input_text) st.subheader(\"The Repsonse is\") st.write(response) else: st.write(\"Please uplaod the resume\") elif submit3: if uploaded_file is not None: pdf_content=input_pdf_setup(uploaded_file) response=get_gemini_response(input_prompt3,pdf_content,input_text) st.subheader(\"The Repsonse is\") st.write(response) else: st.write(\"Please uplaod the resume\")", "source": "Repo:ATS:main.py", "section": "ATS", "hash": "e9db3493"}
{"text": "# CODE CELL import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns # CODE CELL df = pd.read_csv(\"drug.csv\") # Taking dataset df.head() # MARKDOWN CELL #### Data Preprocessing # CODE CELL # Finding null values df.isnull().sum() # CODE CELL df['Drug'].unique() # CODE CELL df.shape # Checking number of rows and columns # CODE CELL df.isna().sum() #Checking for NaN values # CODE CELL df.info() # CODE CELL # Checking for unique values df.nunique() # MARKDOWN CELL So there are 4 categorical variables that is Sex, BP, Cholestrol & Drug And; 2 numerical variable # MARKDOWN CELL ### Univariate Analysis # CODE CELL # Creating a variable that holds the numeric variable and storing it as a list numerical_cols = df.select_dtypes(include = np.number).columns.to_list() numerical_cols # CODE CELL for col in numerical_cols: plt.figure(figsize=(14,5)) sns.histplot(data=df,x=col,bins=10,kde=True) plt.title(f\"Distribution of {col}\") # MARKDOWN CELL We see that Na_to_K is right skewed, so performing log tranformation to look like normal distribution # CODE CELL df['Na_to_K'] = np.log(df['Na_to_K']) # CODE CELL plt.figure(figsize=(14,5)) sns.histplot(data=df,x=df['Na_to_K'],bins=10,kde=True) plt.title(f\"Distribution of Na_to_K\") # CODE CELL sns.histplot(data=df,x=df['Drug']) # MARKDOWN CELL #### There is an imbalance in the target labels i.e.; Drug Y is abundantly present. Hence to compensate this and ignore the baising of the result, I will be using class weights to balance this situation by putting weights= \"uniform\" during knn classification <a href= '#s0'>here</a>. # CODE CELL # Categorical variables categorical_cols=df.select_dtypes(include=object).columns.to_list() categorical_cols # CODE CELL # Encoding Categorical Variable from sklearn.preprocessing import LabelEncoder le = LabelEncoder() for n in categorical_cols: df[n] = le.fit_transform(df[n]) df.head() # CODE CELL # Distribution of dependent and independent variable x = df.iloc[:,:5] y = df.iloc[:,-1] print(x.head(),\"\\n\") print(y.head()) # CODE CELL # Splitting into train and test set from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x,y, random_state= 0) #Splitting test size to 25% as default value x_train.shape,y_train.shape,x_test.shape,y_test.shape # Checking the size of each test and train set # CODE CELL # Feature Scaling from sklearn.preprocessing import StandardScaler sc = StandardScaler() x_train = sc.fit_transform(x_train) x_test = sc.transform(x_test) # CODE CELL from sklearn.neighbors import KNeighborsClassifier # Using Euclidean distance # Using weights to ignore biasing of target variable k = KNeighborsClassifier(n_neighbors = 5, metric= \"minkowski\", p = 2, weights= \"uniform\") # MARKDOWN CELL #### Implementing cross validation to train model on multiple train-test splits<a id ='s0'></a> # CODE CELL from sklearn.model_selection import cross_val_score result=cross_val_score(k,x_train,y_train,cv=10) print(result) print(\"Average Result\", result.mean()) # MARKDOWN CELL As seen from the above result after applying K fold cross validation the lower accuracy of the model is 66.67% and the highest accuracy is 100, which can also be seen from the graph <a href= '#s1'>here</a> # CODE CELL k.fit(x_train,y_train) # Training models # CODE CELL # Making prediction y_pred = k.predict(x_test) # It will do the prediction of new data and give us the O/P # CODE CELL # Creating Confusion metrics from sklearn import metrics", "source": "Repo:Drug-Classification:KNN in drug classification.ipynb", "section": "Drug-Classification", "hash": "41987a6d"}
{"text": "also be seen from the graph <a href= '#s1'>here</a> # CODE CELL k.fit(x_train,y_train) # Training models # CODE CELL # Making prediction y_pred = k.predict(x_test) # It will do the prediction of new data and give us the O/P # CODE CELL # Creating Confusion metrics from sklearn import metrics cm = metrics.confusion_matrix(y_test, y_pred) #To compare predicted result with the actual result sns.heatmap(cm, annot=True) accuracy = metrics.accuracy_score(y_test, y_pred) # Checking accuracy of a model print(\"Accuracy score:\", accuracy) print(\"Precision Score : \", metrics.precision_score(y_test, y_pred, average='macro')) print(\"Recall Score : \",metrics.recall_score(y_test, y_pred, average='macro')) # MARKDOWN CELL We get an accuracy of 80%. # MARKDOWN CELL #### Now checking the performance of testing and training dataset for different n_neighbors values. I will choose 1-40. # CODE CELL training_accuracy = [] testing_accuracy = [] for i in range(1,40): knn = KNeighborsClassifier(n_neighbors= i) knn.fit(x_train, y_train) training_accuracy.append(knn.score(x_train,y_train)) testing_accuracy.append(knn.score(x_test,y_test)) # accuracy(y_test, y_pred) # MARKDOWN CELL #### Graph fro 1-40 knn values <a id ='s1'></a> # CODE CELL # Plotting test, train accuracy with n_neighbors plt.figure(figsize=(10,5)) plt.plot(range(1,40), testing_accuracy, label = \"Testing Accuracy\") plt.plot(range(1,40), training_accuracy, label = \"Training Accuracy\") plt.title('Training vs Testing Accuracy') plt.xlabel('n_neighbors') plt.ylabel('Accuracy') plt.legend(loc = 'best') plt.show()", "source": "Repo:Drug-Classification:KNN in drug classification.ipynb", "section": "Drug-Classification", "hash": "0cf90b3e"}
{"text": "# Drug Classification Analysis ## Overview This project focuses on analyzing a drug classification dataset. The analysis includes data preprocessing, model implementation, and performance evaluation. ## Dataset - **Source:** `drug.csv` - **Description:** The dataset contains information about drug classifications based on attributes like Age, Sex, Blood Pressure (BP), Cholesterol, Sodium to Potassium Ratio (Na_to_K), and the corresponding drug label. ## Data Preprocessing - Checked for missing values (None found). - Encoded categorical variables using Label Encoding. - Explored unique values for each categorical attribute. ## Univariate Analysis - Conducted univariate analysis for numerical attributes: 'Age' and 'Na_to_K'. - Applied log transformation to 'Na_to_K' to achieve a normal distribution. ## Model Implementation - Utilized K-Nearest Neighbors (KNN) classifier for drug classification. - Split the dataset into training and testing sets. - Applied Standard Scaling for feature scaling. - Explored different 'n_neighbors' values in KNN. - Evaluated the model using confusion matrices and performance metrics. ## Results - Achieved an accuracy of 80%. - Investigated the impact of 'n_neighbors' on model performance.", "source": "Repo:Drug-Classification:README.md", "section": "Drug-Classification", "hash": "54d0e54a"}
{"text": "KNN in drug classification In [1]: import pandas as pd import numpy as np import matplotlib pyplot as plt import seaborn as sns In [2]: df = pd read_csv ( \"drug csv\" ) # Taking dataset df head () Out[2]: Age Sex BP Cholesterol Na_to_K Drug 0 23 F HIGH HIGH 25 355 DrugY 1 47 M LOW HIGH 13 093 drugC 2 47 M LOW HIGH 10 114 drugC 3 28 F NORMAL HIGH 7 798 drugX 4 61 F LOW HIGH 18 043 DrugY Data Preprocessing ¶ In [3]: # Finding null values df isnull () sum () Out[3]: Age 0 Sex 0 BP 0 Cholesterol 0 Na_to_K 0 Drug 0 dtype: int64 In [4]: df [ 'Drug' ] unique () Out[4]: array(['DrugY', 'drugC', 'drugX', 'drugA', 'drugB'], dtype=object) In [5]: df shape # Checking number of rows and columns Out[5]: (200, 6) In [6]: df isna () sum () #Checking for NaN values Out[6]: Age 0 Sex 0 BP 0 Cholesterol 0 Na_to_K 0 Drug 0 dtype: int64 In [7]: df info () <class 'pandas core frame DataFrame'> RangeIndex: 200 entries, 0 to 199 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Age 200 non-null int64 1 Sex 200 non-null object 2 BP 200 non-null object 3 Cholesterol 200 non-null object 4 Na_to_K 200 non-null float64 5 Drug 200 non-null object dtypes: float64(1), int64(1), object(4) memory usage: 9 5+ KB In [8]: # Checking for unique values df nunique () Out[8]: Age 57 Sex 2 BP 3 Cholesterol 2 Na_to_K 198 Drug 5 dtype: int64 So there are 4 categorical variables that is Sex, BP, Cholestrol & Drug And; 2 numerical variable Univariate Analysis ¶ In [9]: # Creating a variable that holds the numeric variable and storing it as a list numerical_cols = df select_dtypes ( include = np number ) columns to_list () numerical_cols Out[9]: ['Age', 'Na_to_K'] In [10]: for col in numerical_cols : plt figure ( figsize = ( 14 , 5 )) sns histplot ( data = df , x = col , bins = 10 , kde = True ) plt title ( f \"Distribution of { col } \" ) We see that Na_to_K is right skewed, so performing log tranformation to look like normal distribution In [11]: df [ 'Na_to_K' ] = np log ( df [ 'Na_to_K' ]) In [12]: plt figure ( figsize = ( 14 , 5 )) sns histplot ( data = df , x = df [ 'Na_to_K' ], bins = 10 , kde = True ) plt title ( f \"Distribution of Na_to_K\" ) Out[12]: Text(0 5, 1 0, 'Distribution of Na_to_K') In [13]: sns histplot ( data = df , x = df [ 'Drug' ]) Out[13]: <AxesSubplot:xlabel='Drug', ylabel='Count'> There is an imbalance in the target labels i e ; Drug Y is abundantly present", "source": "Repo:Drug-Classification:KNN in drug classification.html", "section": "Drug-Classification", "hash": "459038cc"}
{"text": "plt title ( f \"Distribution of Na_to_K\" ) Out[12]: Text(0 5, 1 0, 'Distribution of Na_to_K') In [13]: sns histplot ( data = df , x = df [ 'Drug' ]) Out[13]: <AxesSubplot:xlabel='Drug', ylabel='Count'> There is an imbalance in the target labels i e ; Drug Y is abundantly present Hence to compensate this and ignore the baising of the result, I will be using class weights to balance this situation by putting weights= \"uniform\" during knn classification here ¶ In [14]: # Categorical variables categorical_cols = df select_dtypes ( include = object ) columns to_list () categorical_cols Out[14]: ['Sex', 'BP', 'Cholesterol', 'Drug'] In [15]: # Encoding Categorical Variable from sklearn preprocessing import LabelEncoder le = LabelEncoder () for n in categorical_cols : df [ n ] = le fit_transform ( df [ n ]) df head () Out[15]: Age Sex BP Cholesterol Na_to_K Drug 0 23 0 0 0 3 232976 0 1 47 1 1 0 2 572078 3 2 47 1 1 0 2 313921 3 3 28 0 2 0 2 053867 4 4 61 0 1 0 2 892758 0 In [16]: # Distribution of dependent and independent variable x = df iloc [:,: 5 ] y = df iloc [:, - 1 ] print ( x head (), \" \\n \" ) print ( y head ()) Age Sex BP Cholesterol Na_to_K 0 23 0 0 0 3 232976 1 47 1 1 0 2 572078 2 47 1 1 0 2 313921 3 28 0 2 0 2 053867 4 61 0 1 0 2 892758 0 0 1 3 2 3 3 4 4 0 Name: Drug, dtype: int64 In [17]: # Splitting into train and test set from sklearn model_selection import train_test_split x_train , x_test , y_train , y_test = train_test_split ( x , y , random_state = 0 ) #Splitting test size to 25% as default value x_train shape , y_train shape , x_test shape , y_test shape # Checking the size of each test and train set Out[17]: ((150, 5), (150,), (50, 5), (50,)) In [18]: # Feature Scaling from sklearn preprocessing import StandardScaler sc = StandardScaler () x_train = sc fit_transform ( x_train ) x_test = sc transform ( x_test ) In [19]: from sklearn neighbors import KNeighborsClassifier # Using Euclidean distance # Using weights to ignore biasing of target variable k = KNeighborsClassifier ( n_neighbors = 5 , metric = \"minkowski\" , p = 2 , weights = \"uniform\" ) Implementing cross validation to train model on multiple train-test splits ¶ In [20]: from sklearn model_selection import cross_val_score result = cross_val_score ( k , x_train , y_train , cv = 10 ) print ( result ) print ( \"Average Result\" , result mean ()) [0 86666667 0 93333333 0 73333333 0 93333333 0 73333333 0 66666667 0 66666667 1 0 86666667 0 86666667] Average Result 0 8266666666666668 As seen from the above result after applying K fold cross validation the lower accuracy of the model is 66", "source": "Repo:Drug-Classification:KNN in drug classification.html", "section": "Drug-Classification", "hash": "a1fafc40"}
{"text": "( \"Average Result\" , result mean ()) [0 86666667 0 93333333 0 73333333 0 93333333 0 73333333 0 66666667 0 66666667 1 0 86666667 0 86666667] Average Result 0 8266666666666668 As seen from the above result after applying K fold cross validation the lower accuracy of the model is 66 67% and the highest accuracy is 100, which can also be seen from the graph here In [21]: k fit ( x_train , y_train ) # Training models Out[21]: KNeighborsClassifier() In [22]: # Making prediction y_pred = k predict ( x_test ) # It will do the prediction of new data and give us the O/P In [23]: # Creating Confusion metrics from sklearn import metrics cm = metrics confusion_matrix ( y_test , y_pred ) #To compare predicted result with the actual result sns heatmap ( cm , annot = True ) accuracy = metrics accuracy_score ( y_test , y_pred ) # Checking accuracy of a model print ( \"Accuracy score:\" , accuracy ) print ( \"Precision Score : \" , metrics precision_score ( y_test , y_pred , average = 'macro' )) print ( \"Recall Score : \" , metrics recall_score ( y_test , y_pred , average = 'macro' )) Accuracy score: 0 8 Precision Score : 0 7779411764705882 Recall Score : 0 7838333333333333 We get an accuracy of 80% Now checking the performance of testing and training dataset for different n_neighbors values I will choose 1-40 ¶ In [24]: training_accuracy = [] testing_accuracy = [] for i in range ( 1 , 40 ): knn = KNeighborsClassifier ( n_neighbors = i ) knn fit ( x_train , y_train ) training_accuracy append ( knn score ( x_train , y_train )) testing_accuracy append ( knn score ( x_test , y_test )) # accuracy(y_test, y_pred) Graph fro 1-40 knn values ¶ In [25]: # Plotting test, train accuracy with n_neighbors plt figure ( figsize = ( 10 , 5 )) plt plot ( range ( 1 , 40 ), testing_accuracy , label = \"Testing Accuracy\" ) plt plot ( range ( 1 , 40 ), training_accuracy , label = \"Training Accuracy\" ) plt title ( 'Training vs Testing Accuracy' ) plt xlabel ( 'n_neighbors' ) plt ylabel ( 'Accuracy' ) plt legend ( loc = 'best' ) plt show ()", "source": "Repo:Drug-Classification:KNN in drug classification.html", "section": "Drug-Classification", "hash": "0904106a"}
{"text": "# CODE CELL import re import os import nltk import string import pickle import gensim import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from nltk.corpus import stopwords from gensim.models import Word2Vec from nltk.stem import WordNetLemmatizer from sklearn.metrics.pairwise import cosine_similarity from sklearn.feature_extraction.text import TfidfVectorizer # MARKDOWN CELL ## General function used in project # CODE CELL def create_dataframe(path): \"\"\" Creating a dataframe of the text file. Args: path: Path in which the text file exists. Return: df: Text file extracted in the dataframe for further computations. \"\"\" content = [] file_names = [] for filename in os.listdir(path): with open(os.path.join(path, filename), 'r') as file: content.append(file.read()) file_names.append(filename) df = pd.DataFrame({'Filename': file_names, 'Content': content}) return df ######################################################################################### \"\"\" Preprocessing the text dataset which is acquired. Args: df: Dataframe of whose preprocessing is to be done for a particular column. column: Column name for which preprocessing is to be done. Return: df: Preprocessed dataframe \"\"\" def preprocess_text(df, column): # Remove special characters and numbers df[column] = df[column].apply(lambda text: re.sub(r'[^a-zA-Z\\s]', '', str(text))) # Convert to lowercase df[column] = df[column].str.lower() # Remove multiple occurrences of 'i' characters df[column] = df[column].apply(lambda text: re.sub(r'(i{2,})', 'i', str(text))) # Remove single alphabets df[column] = df[column].apply(lambda text: re.sub(r'\\b[a-zA-Z]\\b', '', str(text))) # Tokenize the text df[column] = df[column].apply(lambda text: nltk.word_tokenize(text)) # Remove stopwords stop_words = set(stopwords.words('english')) df[column] = df[column].apply(lambda tokens: [token for token in tokens if token not in stop_words]) # Lemmatize the tokens lemmatizer = WordNetLemmatizer() df[column] = df[column].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens]) # Remove 'mmddyyy' df[column] = df[column].apply(lambda text: re.sub(r'(\\b[mmddyyy]+\\b)', '', str(text))) # Remove extra whitespace df[column] = df[column].apply(lambda text: re.sub(r'\\s+', '', str(text))) # Join the tokens back into a string df[column] = df[column].apply(lambda tokens: ''.join(tokens)) return df ######################################################################################### \"\"\" Calculate TF-IDF vectorization between train and test dataset. Args: train: Train dataframe. test: Test dataframe- column_train: Column of intrest from the training dataframe. column_test: Column of intrest from the testing dataframe. Returns: train: Array-like or sparse matrix representing the training vectors. test: Array-like or sparse matrix representing the test vectors. \"\"\" def tfidf(train, column_train, test, column_test): # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(train[column_train]) # Apply the same vectorizer on the test documents test = vectorizer.transform(test[column_test]) print(train.shape) print(test.shape) return train, test ######################################################################################### def calculate_cosine_similarity(train, test): \"\"\" Calculate cosine similarity between two sets of vectors. Args: train: Array-like or sparse matrix representing the training vectors. test: Array-like or sparse matrix representing the test vectors. Returns: cos_sim: Array representing the cosine similarity between the training and test vectors. \"\"\" cos_sim = cosine_similarity(train, test) return cos_sim ######################################################################################### #------------Data visualization--------------------# \"\"\" Plotting Heatmap and Histogram form better data visualization. Args: argument: Array cosine similarity between training and testing vectors. \"\"\" def heatmapvis(argument): # Create a heatmap plot plt.figure(figsize=(6, 4)) sns.heatmap(argument, cmap='cool') plt.title('Cosine Similarity') plt.xlabel('Test Documents') plt.ylabel('Train Documents') plt.show() def histogramvis(argument): plt.hist(argument.flatten(), bins=20) plt.xlabel('Cosine Similarity') plt.ylabel('Frequency') plt.title('Distribution of Cosine Similarity') plt.show() #########################################################################################", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.ipynb", "section": "Master-Thesis-Data-Drift", "hash": "ef338104"}
{"text": "Plotting Heatmap and Histogram form better data visualization. Args: argument: Array cosine similarity between training and testing vectors. \"\"\" def heatmapvis(argument): # Create a heatmap plot plt.figure(figsize=(6, 4)) sns.heatmap(argument, cmap='cool') plt.title('Cosine Similarity') plt.xlabel('Test Documents') plt.ylabel('Train Documents') plt.show() def histogramvis(argument): plt.hist(argument.flatten(), bins=20) plt.xlabel('Cosine Similarity') plt.ylabel('Frequency') plt.title('Distribution of Cosine Similarity') plt.show() ######################################################################################### \"\"\" Calculate top 10 document which is most similar according to Cosine similarity. Args: cosinesimilarity: Array-like cosine similarity between training and testing vectors. Return: top_similarities: Document number which is most similar in both the data. \"\"\" def top10(cosinesimilarity): doc_index = 0 # Index of the document to find similarities for N = 10 # Number of top similarities to retrieve top_similarities = sorted(range(len(cosinesimilarity[doc_index])), key=lambda i: cosinesimilarity[doc_index][i], reverse=True)[:N] return top_similarities ######################################################################################### \"\"\" Calculate pecentile 25th, 50th and 75th percentile to find distribution of the dataset w.r.t cosine similarity of TF-IDF vectorization. Args: cosinesimilarity: Array-like cosine similarity between training and testing vectors. \"\"\" def percentile(cosinesimilarity): percentiles = np.percentile(cosinesimilarity, [10, 50, 75]) print(\"10th percentile:\", percentiles[0]*100) print(\"50th percentile:\", percentiles[1]*100) print(\"75th percentile:\", percentiles[2]*100) ######################################################################################### \"\"\" Calculate the Word2Vec of dataset Args: df: Name of the dataframe who comparision is tobe done with train dataset. column: Column name for which vectorization is to be done with training dataset. Return: similarity_matrix_name: Array like similarity between trian and chossen dataset. \"\"\" def word2vec(df,column): # Train Word2Vec model model = gensim.models.Word2Vec(df[column], vector_size=100, window=5, min_count=1, workers=4) return model # MARKDOWN CELL ## Train Dataset # CODE CELL df_train = create_dataframe(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/train_set\") # CODE CELL #df_train # CODE CELL df_train = preprocess_text(df_train, \"Content\") # CODE CELL #df_train[\"Content\"][0] # MARKDOWN CELL ## In_sample Dataset # CODE CELL df_in_test = create_dataframe(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/In_sample_test\") # CODE CELL df_in_test = preprocess_text(df_in_test, \"Content\") # MARKDOWN CELL ## Out_of_sample Dataset # CODE CELL df_out_test = create_dataframe(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/Out_of_sample_test\") # CODE CELL df_out_test = preprocess_text(df_out_test, \"Content\") # MARKDOWN CELL ## Foreign Dataset # CODE CELL df_foreign = pd.read_csv(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake.csv\") # CODE CELL #df_foreign.head() # CODE CELL selected_text = df_foreign['text'].sample(n=500, random_state=42) # Create a new DataFrame with the selected text selected_df = pd.DataFrame(selected_text, columns=['text']) # CODE CELL selected_df = preprocess_text(selected_df,\"text\") # CODE CELL # selected_df # CODE CELL selected_df.shape # MARKDOWN CELL ## Calculating TF-IDF on Train and In_Sample, Out_of_Sample & Foregin Dataset # CODE CELL # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_in_test[\"Content\"]) cos1 = calculate_cosine_similarity(train, test) mean_similarity_In_Sample = np.mean(cos1, axis=1) # CODE CELL heatmapvis(cos1) # CODE CELL top10(cos1) # CODE CELL # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_out_test[\"Content\"]) cos2= calculate_cosine_similarity(train, test) mean_similarity_Out_Of_Sample = np.mean(cos2, axis=1) # CODE CELL heatmapvis(cos2) # CODE CELL top10(cos2) # CODE CELL # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(selected_df[\"text\"]) cos3 = calculate_cosine_similarity(train, test) mean_similarity_foreign = np.mean(cos3, axis=1)", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.ipynb", "section": "Master-Thesis-Data-Drift", "hash": "f7f59868"}
{"text": "# CODE CELL heatmapvis(cos2) # CODE CELL top10(cos2) # CODE CELL # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(selected_df[\"text\"]) cos3 = calculate_cosine_similarity(train, test) mean_similarity_foreign = np.mean(cos3, axis=1) # CODE CELL heatmapvis(cos3) # CODE CELL top10(cos3) # CODE CELL # Load the saved threshold with open(\"threshold(tfidf).pkl\", \"rb\") as f: threshold = pickle.load(f)# Create a grouped box plot data = [mean_similarity_In_Sample, mean_similarity_Out_Of_Sample, mean_similarity_foreign] labels = ['In Sample Test', 'Out Sample Test', 'Foreign Test'] # Add a dashed line for the threshold plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold') plt.boxplot(data, labels=labels) plt.title(\"Box Plot of Mean Cosine Similarities(TF-IDF)\") plt.ylabel(\"Mean Cosine Similarity\") plt.legend() plt.show() # CODE CELL # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5, min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_in_test[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) # Calculate mean similarity for each row mean_similarity_In_Sample = np.mean(similarity_matrix, axis=1) # CODE CELL heatmapvis(similarity_matrix) # CODE CELL top10(similarity_matrix ) # CODE CELL # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5, min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_out_test[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) # Calculate mean similarity for each row mean_similarity_Out_Of_Sample = np.mean(similarity_matrix, axis=1) # CODE CELL heatmapvis(similarity_matrix) # CODE CELL top10(similarity_matrix ) # CODE CELL # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5, min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in selected_df[\"text\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) # Calculate mean similarity for each row mean_similarity_foreign = np.mean(similarity_matrix, axis=1) # CODE CELL heatmapvis(similarity_matrix) # CODE CELL top10(similarity_matrix ) # CODE CELL # Load the saved threshold with open(\"threshold(w2v).pkl\", \"rb\") as f: threshold = pickle.load(f) data = [mean_similarity_In_Sample, mean_similarity_Out_Of_Sample, mean_similarity_foreign] labels = ['In Sample Test', 'Out Sample Test', 'Foreign Test'] # Add a dashed line for the threshold plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold') plt.boxplot(data, labels=labels) plt.title(\"Box Plot of Mean Cosine Similarities(Word2Vec)\") plt.ylabel(\"Mean Cosine Similarity\") plt.legend() plt.show()", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.ipynb", "section": "Master-Thesis-Data-Drift", "hash": "57ebbd79"}
{"text": "# Project Overview This repository comprises a suite of functions dedicated to text data preprocessing, TF-IDF vectorization, cosine similarity computation, data visualization, and analysis. The primary focus is on text similarity calculations and comparisons. ## Table of Contents 1. [General Functions](#general-functions) 2. [Text Preprocessing](#text-preprocessing) 3. [TF-IDF Calculation](#tf-idf-calculation) 4. [Cosine Similarity Calculation](#cosine-similarity-calculation) 5. [Data Visualization](#data-visualization) 6. [Word2Vec Analysis](#word2vec-analysis) 7. [Thresholds Analysis](#thresholds-analysis) --- ## General Functions <a name=\"general-functions\"></a> The repository offers a set of general functions specifically designed for managing text data: - `create_dataframe(path)`: Creates a Pandas DataFrame from text files located at a specified path. - `preprocess_text(df, column)`: Handles text data preprocessing within a DataFrame for a specified column. - `tfidf(train, column_train, test, column_test)`: Computes TF-IDF vectorization for both training and test data. - `calculate_cosine_similarity(train, test)`: Determines the cosine similarity between two sets of vectors. - `heatmapvis(argument)`: Generates a heatmap plot for improved data visualization. - `histogramvis(argument)`: Produces a histogram plot for visualizing data distributions. - `top10(cosinesimilarity)`: Identifies the top 10 most similar documents based on cosine similarity. - `percentile(cosinesimilarity)`: Calculates the 10th, 50th, and 75th percentiles for the distribution of cosine similarity within the dataset. - `word2vec(df, column)`: Computes Word2Vec representation for a given dataset. --- ## Text Preprocessing <a name=\"text-preprocessing\"></a> The `preprocess_text()` function operates on a DataFrame and performs various preprocessing tasks, including special character and number removal, text conversion to lowercase, elimination of single alphabets, tokenization, stopword removal, lemmatization, and more. --- ## TF-IDF Calculation <a name=\"tf-idf-calculation\"></a> The `tfidf()` function calculates TF-IDF vectorization for the provided training and test datasets using the TfidfVectorizer from scikit-learn. --- ## Cosine Similarity Calculation <a name=\"cosine-similarity-calculation\"></a> The `calculate_cosine_similarity()` function computes the cosine similarity between two sets of vectors, a crucial tool for text similarity analysis. --- ## Data Visualization <a name=\"data-visualization\"></a> The repository includes functions like `heatmapvis()` and `histogramvis()` for visualizing cosine similarity through heatmap plots and histogram distributions, enhancing data interpretation. --- ## Word2Vec Analysis <a name=\"word2vec-analysis\"></a> The repository also offers functions dedicated to Word2Vec analysis. `word2vec()` computes Word2Vec representations for a given dataset. --- ## Thresholds Analysis <a name=\"thresholds-analysis\"></a> The analysis involves loading previously saved thresholds and creating box plots for mean cosine similarities obtained from both TF-IDF and Word2Vec analysis for different test datasets. For comprehensive details on usage and functions, please refer to the provided code.", "source": "Repo:Master-Thesis-Data-Drift:README.md", "section": "Master-Thesis-Data-Drift", "hash": "b2d01a26"}
{"text": "Master Thesis February 21, 2024 [1]: import re import os import nltk import string import pickle import gensim import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from nltk.corpus import stopwords from gensim.models import Word2Vec from nltk.stem import WordNetLemmatizer from sklearn.metrics.pairwise import cosine_similarity from sklearn.feature_extraction.text import TfidfVectorizer 0.1 General function used in project [2]: def create_dataframe(path): \"\"\" Creating a dataframe of the text file. Args: path: Path in which the text file exists. Return: df: Text file extracted in the dataframe for further computations. \"\"\" content = [] file_names = [] for filename in os.listdir(path): with open(os.path.join(path, filename), 'r') as file: content.append(file.read()) file_names.append(filename) 1 df = pd.DataFrame({'Filename': file_names, 'Content': content}) return df ######################################################################################### \"\"\" Preprocessing the text dataset which is acquired. Args: df: Dataframe of whose preprocessing is to be done for a particular␣ ↪column. column: Column name for which preprocessing is to be done. Return: df: Preprocessed dataframe \"\"\" def preprocess_text(df, column): # Remove special characters and numbers df[column] = df[column].apply(lambda text: re.sub(r'[^a-zA-Z\\s]', '',␣ ↪str(text))) # Convert to lowercase df[column] = df[column].str.lower() # Remove multiple occurrences of 'i' characters df[column] = df[column].apply(lambda text: re.sub(r'(i{2,})', 'i',␣ ↪str(text))) # Remove single alphabets df[column] = df[column].apply(lambda text: re.sub(r'\\b[a-zA-Z]\\b', '',␣ ↪str(text))) # Tokenize the text df[column] = df[column].apply(lambda text: nltk.word_tokenize(text)) # Remove stopwords stop_words = set(stopwords.words('english')) df[column] = df[column].apply(lambda tokens: [token for token in tokens if␣ ↪token not in stop_words]) # Lemmatize the tokens lemmatizer = WordNetLemmatizer() df[column] = df[column].apply(lambda tokens: [lemmatizer.lemmatize(token)␣ ↪for token in tokens]) 2 # Remove 'mmddyyy' df[column] = df[column].apply(lambda text: re.sub(r'(\\b[mmddyyy]+\\b)', '',␣ ↪str(text))) # Remove extra whitespace df[column] = df[column].apply(lambda text: re.sub(r'\\s+', '', str(text))) # Join the tokens back into a string df[column] = df[column].apply(lambda tokens: ''.join(tokens)) return df ######################################################################################### \"\"\" Calculate TF-IDF vectorization between train and test dataset. Args: train: Train dataframe. test: Test dataframe- column_train: Column of intrest from the training dataframe. column_test: Column of intrest from the testing dataframe. Returns: train: Array-like or sparse matrix representing the training vectors. test: Array-like or sparse matrix representing the test vectors. \"\"\" def tfidf(train, column_train, test, column_test): # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(train[column_train]) # Apply the same vectorizer on the test documents test = vectorizer.transform(test[column_test]) print(train.shape) print(test.shape) return train, test ######################################################################################### 3 def calculate_cosine_similarity(train, test): \"\"\" Calculate cosine similarity between two sets of vectors. Args: train: Array-like or sparse matrix representing the training vectors. test: Array-like or sparse matrix representing the test vectors. Returns: cos_sim: Array representing the cosine similarity between the training␣ ↪and test vectors. \"\"\" cos_sim = cosine_similarity(train, test) return cos_sim ######################################################################################### #------------Data visualization--------------------# \"\"\" Plotting Heatmap and Histogram form better data visualization. Args: argument: Array cosine similarity between training and testing vectors. \"\"\" def heatmapvis(argument): # Create a heatmap plot plt.figure(figsize=(6, 4)) sns.heatmap(argument, cmap='cool') plt.title('Cosine Similarity') plt.xlabel('Test Documents') plt.ylabel('Train Documents') plt.show() def histogramvis(argument): plt.hist(argument.flatten(), bins=20) plt.xlabel('Cosine Similarity') plt.ylabel('Frequency') plt.title('Distribution of Cosine Similarity') plt.show() ######################################################################################### \"\"\" 4", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.pdf", "section": "Master-Thesis-Data-Drift", "hash": "3c21d2e6"}
{"text": "and Histogram form better data visualization. Args: argument: Array cosine similarity between training and testing vectors. \"\"\" def heatmapvis(argument): # Create a heatmap plot plt.figure(figsize=(6, 4)) sns.heatmap(argument, cmap='cool') plt.title('Cosine Similarity') plt.xlabel('Test Documents') plt.ylabel('Train Documents') plt.show() def histogramvis(argument): plt.hist(argument.flatten(), bins=20) plt.xlabel('Cosine Similarity') plt.ylabel('Frequency') plt.title('Distribution of Cosine Similarity') plt.show() ######################################################################################### \"\"\" 4 Calculate top 10 document which is most similar according to Cosine␣ ↪similarity. Args: cosinesimilarity: Array-like cosine similarity between training and␣ ↪testing vectors. Return: top_similarities: Document number which is most similar in both the␣ ↪data. \"\"\" def top10(cosinesimilarity): doc_index = 0 # Index of the document to find similarities for N = 10 # Number of top similarities to retrieve top_similarities = sorted(range(len(cosinesimilarity[doc_index])),␣ ↪key=lambda i: cosinesimilarity[doc_index][i], reverse=True)[:N] return top_similarities ######################################################################################### \"\"\" Calculate pecentile 25th, 50th and 75th percentile to find distribution of␣ ↪the dataset w.r.t cosine similarity of TF-IDF vectorization. Args: cosinesimilarity: Array-like cosine similarity between training and␣ ↪testing vectors. \"\"\" def percentile(cosinesimilarity): percentiles = np.percentile(cosinesimilarity, [10, 50, 75]) print(\"10th percentile:\", percentiles[0]*100) print(\"50th percentile:\", percentiles[1]*100) print(\"75th percentile:\", percentiles[2]*100) #########################################################################################␣ ↪ \"\"\" Calculate the Word2Vec of dataset 5 Args: df: Name of the dataframe who comparision is tobe done with train␣ ↪dataset. column: Column name for which vectorization is to be done with training␣ ↪dataset. Return: similarity_matrix_name: Array like similarity between trian and chossen␣ ↪dataset. \"\"\" def word2vec(df,column): # Train Word2Vec model model = gensim.models.Word2Vec(df[column], vector_size=100, window=5,␣ ↪min_count=1, workers=4) return model 0.2 Train Dataset [3]: df_train = create_dataframe(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/ ↪train_set\") [4]: #df_train [5]: df_train = preprocess_text(df_train, \"Content\") [6]: #df_train[\"Content\"][0] 0.3 In_sample Dataset [7]: df_in_test = create_dataframe(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/ ↪In_sample_test\") [8]: df_in_test = preprocess_text(df_in_test, \"Content\") 0.4 Out_of_sample Dataset [9]: df_out_test = create_dataframe(\"/mnt/c/Users/alamm9/Desktop/ ↪text_file_train_test/Out_of_sample_test\") [10]: df_out_test = preprocess_text(df_out_test, \"Content\") 6 0.5 Foreign Dataset [11]: df_foreign = pd.read_csv(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake. ↪csv\") [12]: #df_foreign.head() [13]: selected_text = df_foreign['text'].sample(n=500, random_state=42) # Create a new DataFrame with the selected text selected_df = pd.DataFrame(selected_text, columns=['text']) [14]: selected_df = preprocess_text(selected_df,\"text\") [15]: # selected_df [16]: selected_df.shape [16]: (500, 1) [ ]: 0.6 Calculating TF-IDF on Train and In_Sample, Out_of_Sample & Foregin Dataset [17]: # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_in_test[\"Content\"]) cos1 = calculate_cosine_similarity(train, test) mean_similarity_In_Sample = np.mean(cos1, axis=1) [18]: heatmapvis(cos1) 7 [19]: top10(cos1) [19]: [304, 300, 302, 301, 303, 282, 138, 19, 241, 79] [20]: # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_out_test[\"Content\"]) cos2= calculate_cosine_similarity(train, test) mean_similarity_Out_Of_Sample = np.mean(cos2, axis=1) [21]: heatmapvis(cos2) 8 [22]: top10(cos2) [22]: [22, 23, 35, 32, 37, 24, 25, 26, 40, 28] [23]: # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(selected_df[\"text\"]) cos3 = calculate_cosine_similarity(train, test) mean_similarity_foreign = np.mean(cos3, axis=1) [24]: heatmapvis(cos3) 9", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.pdf", "section": "Master-Thesis-Data-Drift", "hash": "f7aab861"}
{"text": "32, 37, 24, 25, 26, 40, 28] [23]: # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fit the vectorizer on the training documents train = vectorizer.fit_transform(df_train[\"Content\"]) # Apply the same vectorizer on the test documents test = vectorizer.transform(selected_df[\"text\"]) cos3 = calculate_cosine_similarity(train, test) mean_similarity_foreign = np.mean(cos3, axis=1) [24]: heatmapvis(cos3) 9 [25]: top10(cos3) [25]: [95, 7, 85, 54, 368, 234, 240, 22, 433, 494] [26]: # Load the saved threshold with open(\"threshold(tfidf).pkl\", \"rb\") as f: threshold = pickle.load(f)# Create a grouped box plot data = [mean_similarity_In_Sample, mean_similarity_Out_Of_Sample,␣ ↪mean_similarity_foreign] labels = ['In Sample Test', 'Out Sample Test', 'Foreign Test'] # Add a dashed line for the threshold plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold') plt.boxplot(data, labels=labels) plt.title(\"Box Plot of Mean Cosine Similarities(TF-IDF)\") plt.ylabel(\"Mean Cosine Similarity\") plt.legend() plt.show() 10 [27]: # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5,␣ ↪min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token␣ ↪in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the␣ ↪training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in␣ ↪model.wv], axis=0) for doc_tokens in df_in_test[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) # Calculate mean similarity for each row mean_similarity_In_Sample = np.mean(similarity_matrix, axis=1) similarity_matrix (1626, 307) 11 [28]: heatmapvis(similarity_matrix) [29]: top10(similarity_matrix ) [29]: [110, 18, 295, 153, 296, 8, 95, 145, 254, 282] [30]: # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5,␣ ↪min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token␣ ↪in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the␣ ↪training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in␣ ↪model.wv], axis=0) for doc_tokens in df_out_test[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) 12 # Calculate mean similarity for each row mean_similarity_Out_Of_Sample = np.mean(similarity_matrix, axis=1) similarity_matrix (1626, 41) [31]: heatmapvis(similarity_matrix) [32]: top10(similarity_matrix ) [32]: [11, 3, 7, 10, 20, 19, 18, 23, 17, 16] [33]: # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5,␣ ↪min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token␣ ↪in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the␣ ↪training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in␣ ↪model.wv], axis=0) for doc_tokens in selected_df[\"text\"]] 13 # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) print(\"similarity_matrix\",similarity_matrix.shape) # Calculate mean similarity for each row mean_similarity_foreign = np.mean(similarity_matrix, axis=1) similarity_matrix (1626, 500) [34]: heatmapvis(similarity_matrix) [35]: top10(similarity_matrix ) [35]: [128, 126, 311, 287, 393, 309, 155, 433, 242, 139] [37]: # Load the saved threshold with open(\"threshold(w2v).pkl\", \"rb\") as f: threshold = pickle.load(f) data = [mean_similarity_In_Sample, mean_similarity_Out_Of_Sample,␣ ↪mean_similarity_foreign] labels = ['In Sample Test', 'Out Sample Test', 'Foreign Test'] 14", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.pdf", "section": "Master-Thesis-Data-Drift", "hash": "e7c37e75"}
{"text": "axis=1) similarity_matrix (1626, 500) [34]: heatmapvis(similarity_matrix) [35]: top10(similarity_matrix ) [35]: [128, 126, 311, 287, 393, 309, 155, 433, 242, 139] [37]: # Load the saved threshold with open(\"threshold(w2v).pkl\", \"rb\") as f: threshold = pickle.load(f) data = [mean_similarity_In_Sample, mean_similarity_Out_Of_Sample,␣ ↪mean_similarity_foreign] labels = ['In Sample Test', 'Out Sample Test', 'Foreign Test'] 14 # Add a dashed line for the threshold plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold') plt.boxplot(data, labels=labels) plt.title(\"Box Plot of Mean Cosine Similarities(Word2Vec)\") plt.ylabel(\"Mean Cosine Similarity\") plt.legend() plt.show() [ ]: 15", "source": "Repo:Master-Thesis-Data-Drift:Master Thesis.pdf", "section": "Master-Thesis-Data-Drift", "hash": "4b162ca3"}
{"text": "# KNN-ML-project # iPhone Purchase Prediction with K-Nearest Neighbors (KNN) This project utilizes the K-Nearest Neighbors algorithm to predict whether a person will purchase an iPhone based on their gender, age, and salary. The dataset contains information about individuals, and the goal is to build a model that accurately predicts iPhone purchases. ## Dataset Overview The dataset 'iphone.csv' includes columns for Gender, Age, Salary, and the target variable 'Purchase Iphone'. Initial exploration shows no missing values. ## Data Preprocessing - **Handling Categorical Data**: Gender information is converted into numerical values using LabelEncoder. - **Feature Scaling**: StandardScaler is used to standardize the independent variables for the KNN algorithm. ## Model Building - **K-NN Classifier**: A K-Neighbors Classifier is trained with n_neighbors set to 5. - **Training Accuracy**: Achieves an accuracy of 91.67%. ## Model Evaluation ### Confusion Matrix Metrics - **True Negatives (TN)**: 64 - **False Positives (FP)**: 4 - **False Negatives (FN)**: 3 - **True Positives (TP)**: 29 ### Performance Metrics - **Accuracy Score**: 93% - **Precision Score**: 87.88% - **Recall Score**: 90.62% ## Prediction The model predicts whether a 'Female' aged 47 with a salary of 30000 will purchase an iPhone as '1' (indicating purchase). ## Determining Optimal n_neighbors - Plotting training and testing accuracy against varying n_neighbors (1-20). - Observed overfitting for lower n_neighbors (1,2) and achieved convergence at n_neighbors = 3. - Optimal n_neighbors found at 3, as both training and testing accuracy stabilize around 93%. ## Conclusion The model best operates with n_neighbors = 3 for this dataset, providing a stable prediction accuracy of 93% without overfitting.", "source": "Repo:KNN-ML-project:README.md", "section": "KNN-ML-project", "hash": "1cca21a7"}
{"text": "# CODE CELL import pandas as pd import matplotlib.pyplot as plt # CODE CELL df = pd.read_csv(\"iphone.csv\") #taking the dataset print(df.head()) # CODE CELL df.isnull().sum() #checking for missing values # CODE CELL # Distributing dependent and independent variable x = df.iloc[:,:-1].values # x contains independent variable y = df.iloc[:,3].values # y contains dependent variable \"Purchase iPhone\" # CODE CELL # Converting categorical variable into numbers from sklearn.preprocessing import LabelEncoder le_gender = LabelEncoder() x[:,0] = le_gender.fit_transform(x[:,0]) # CODE CELL # Splitting data into train and test for KNN from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=.25, random_state = 0) # here splitting testing set to 25% # CODE CELL # Feature Scaling from sklearn.preprocessing import StandardScaler # StandardScaler is use to do Feature Transformation with mean = 0 and # std deviation = 1 sc = StandardScaler() # In \"fit\", we are computing the mean and std-deviation in case of StandardScalar; # In \"transform\" we are applying entire formula in each and every data points. x_train_scalar = sc.fit_transform(x_train) x_test_scalar = sc.transform(x_test) # Data which will use in Making Predictions # CODE CELL # Implementing k-NN Classifier [Model Building] from sklearn.neighbors import KNeighborsClassifier # for Euclidean distance we use p = 2 and specify metric as minkowski c = KNeighborsClassifier(n_neighbors= 5, metric= \"minkowski\", p =2) # Here, the model will only learn parameters and weight c.fit(x_train_scalar,y_train) # Learn from I/P and O/P data and sets its parameter and weight # CODE CELL # Accuracy in Training dataset is 91.67% c.score(x_train_scalar,y_train) # CODE CELL # Making Predictions y_pred = c.predict(x_test_scalar) # It will do the prediction of new data and give us the O/P # CODE CELL from sklearn import metrics cm = metrics.confusion_matrix(y_test, y_pred) #Creating Confusion Matrix; to compare predicted result with the actual result print(cm) accuracy = metrics.accuracy_score(y_test, y_pred) # Checking accuracy of a model print(\"Accuracy score:\", accuracy) precision = metrics.precision_score(y_test,y_pred) # Precision score is % of predicted postive events that are actually positive print(\"Precision score:\", precision) recall = metrics.recall_score(y_test,y_pred) # Recall score is the % of positive events that we predicted correctly print(\"Recall score:\", recall) # MARKDOWN CELL <b>Accuracy Score</b> = (TP+TN)/(TP+TN+FP+FN)<br><br> <b>Recall Score</b> = TP/(TP+FN)<br><br> <b>Precision Score</b> = TP/(TP+FP) # MARKDOWN CELL We get an accuracy of 93% and only 7 that is 3+4 incorrect predictions were made.<br><br> 64 --> True Negative i.e,; Person has not bought an iPhone and predicted value also says the same<br> 4 --> Flase Positive i.e,; Person has not bought an iPhone but the predicted value says they did buy<br> 3 --> Flase Negative i.e,; Person has bought an iPhone but the predicted value says they did not buy<br> 29 --> True Positive i.e,; Person has bought an iPhone and the predicted value also says they bought<br> # CODE CELL # Predicting whether Gender \"Female\" with age 47 and Salary 40000 will purchase iPhone. c.predict([[1,47,30000]]) # MARKDOWN CELL The output is 1; which means according to our model the person will purchase an iPhone.", "source": "Repo:KNN-ML-project:kNN Classifier iPhone proj.ipynb", "section": "KNN-ML-project", "hash": "12fc5703"}
{"text": "Person has bought an iPhone and the predicted value also says they bought<br> # CODE CELL # Predicting whether Gender \"Female\" with age 47 and Salary 40000 will purchase iPhone. c.predict([[1,47,30000]]) # MARKDOWN CELL The output is 1; which means according to our model the person will purchase an iPhone. # MARKDOWN CELL #### Now checking the performance of testing and training dataset for different n_neighbors values. I will choose 1 - 20. # CODE CELL training_accuracy = [] testing_accuracy = [] for i in range(1,21): knn = KNeighborsClassifier(n_neighbors= i) knn.fit(x_train_scalar, y_train) training_accuracy.append(knn.score(x_train_scalar,y_train)) testing_accuracy.append(knn.score(x_test_scalar,y_test)) # MARKDOWN CELL After running this code I got the training and testing accuracy for different n_neighbors # CODE CELL # Plotting test, train accuracy with n_neighbors plt.figure(figsize=(10,5)) plt.plot(range(1,21), testing_accuracy, label = \"Testing Accuracy\") plt.plot(range(1,21), training_accuracy, label = \"Training Accuracy\") plt.title('Training vs Testing Accuracy') plt.xlabel('n_neighbors') plt.ylabel('Accuracy') plt.legend(loc = 'best') plt.show() # MARKDOWN CELL #### Analyzing the graph above In the beginning when n_neighbor were 1,2 training accuracy was lot higher than the testing accuracy. So, the model was suffering from overfitting.<br><br> After that the training and testing accuracy become closer and thats the spot that we wanted. <br><br> Moving forward, when the n_neighbor got higher, the testing accuracy acquire a constant value of about 93% and training accuracy gradually went down from n_neighbour = 5; which we do not need.<br><br> Therefore, from the above graph the n_neighbour for this particular dataset and model should be 3.", "source": "Repo:KNN-ML-project:kNN Classifier iPhone proj.ipynb", "section": "KNN-ML-project", "hash": "8d027da9"}
{"text": "#!/usr/bin/env python # coding: utf-8 # ### Problem 1 [3 points] # Load the data provided in universities.csv from the moodle course page. The data is about economic and # demographic characteristics of various US universities. # Load the data from tuition_income.csv from the same page. # Select from the universities dataset data for all universities located in the following states: New Mexico, # Utah, Montana, Kentucky, South Carolina, Alabama, Idaho, West Virginia, Arkansas, Mississipi. Use this # restricted data set as basis for the entire exam (and modify accordingly in the following questions). # In[1]: import pandas as pd import plotly.express as px from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import scale from sklearn.linear_model import LinearRegression from sklearn.svm import SVR, SVC from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix pd.set_option('max_columns', None) import warnings warnings.filterwarnings('ignore') from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_curve,auc,plot_confusion_matrix # In[2]: # Taking University dataset uni= pd.read_csv(\"universities.csv\") uni # In[3]: # Taking Tuition Income datatset dftution= pd.read_csv(\"tuition_income.csv\") # ### 1. How many universities are in the resulting dataset? [1] # In[4]: df = uni df = df.loc[df['state'].isin([\"New Mexico\", \"Utah\", \"Montana\", \"Kentucky\", \"South Carolina\", \"Alabama\", \"Idaho\", \"West Virginia\", \"Arkansas\", \"Mississipi\"])] df = df[df[\"name\"].str.contains('University')] df # Solution: There in total 99 Universities in the resulting Dataset. # ### 2. Graduates of how many universities have an estimated early career pay greater than 50000 dollars per year? [1] # In[5]: # ECP is estimated early career pay ECP = df[df['early_career_pay']>50000] ECP # Solution: 15 Universities Graduates are having an estimated early career pay greater than 50000 dollars # per year? # ### 3. How many universities are not public? [1] # In[6]: # Not public = Private + For Profit pvt = df[df['type']!= 'Public'] print(pvt.type.value_counts()) pvt # Solution: 38 Universities are not public . # ### Problem 2: Making sense of your data [15 points] # In[7]: df.dtypes # ### 1. Take a quick look at the data structure. Describe the dataset in a few sentences. [2] # # Most of the columns in our University data are of type \"int64\". This means that they are 64 bit integers. But type \"float64\" column is a floating point value which means it contains decimals. Some datatypes are of \"object\", which means it contain numeric or strings or both vaues.", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "d7440e7f"}
{"text": "# Most of the columns in our University data are of type \"int64\". This means that they are 64 bit integers. But type \"float64\" column is a floating point value which means it contains decimals. Some datatypes are of \"object\", which means it contain numeric or strings or both vaues. # ### 2. Which facts or principles do you need to know about US university system to make a good sense of this dataset? [2] # # According to my evaluation, name of the university, state, total enrollment is important as it will give us an idea of the total number of students getting admission each year. Henceforth, the state code, type of university is also informative to know whether the university is public, private or for profit. Walking down the road, degree length gives the duration of the course plus, the private in_state_tuition, out_of_state_tuition and private in_state_total, out_of_state_total accounts for same data, so maybe we can merge it into one column to reduce number of columns. Early career pay, mid career pay gives a solid idea of how much they are going to earn during and after getting a degree. Make_world_better_percent also importantly contribute to the dataset as it gives an idea as how much we are contributing towars world with our knowledge. # ### 3. Which things about the data you have at hand you do not know for sure and have to assume? [2] # # Some of the data's I have less idea about thier existance are: # # n_asian # n_black # n_hispanic # n_pacific # n_total_minority # n_multiracial # n_unknown # n_white # room_and_board # stem_percent # ### 4. In this project, you will have to predict out-of-state tuition fee. Do students choose university solely based on the cost? Which other factors might be important? [2] # # No, student have not selected university only on the basis of cost. If we will compare total_enrollment of row no. 70 and 84, the out_of_state tuition fee of row number 70 is 15848 and total_enrollment = 12002, also out_of_state tuition fee of row number 84 is 12870 and total_enrollment = 3128. As we can see the university in row 70 costs higher and also have higher enrollment of student than university in row number 84, which is cheaper, it can deduce that students are not choosing university on the basis of cost. # # Other factors which might be important other than the University are, type of university, early_career_pay and mid_career_pay. # ### 5. To whom is this cost variable more important than the other three? Explain. [2] # # # Other than those three the cost variable are important room_and_board. As we can see that the total cost whether it be in_state_tuition or out_of_state_tuition, the room_and_board are adding up in the total cost of both of these. # ### 6. Formulate a reasonable business goal, execution of which would require analysis of out-of-state tuition fee in this dataset. [1] #", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "ed0368ed"}
{"text": "As we can see that the total cost whether it be in_state_tuition or out_of_state_tuition, the room_and_board are adding up in the total cost of both of these. # ### 6. Formulate a reasonable business goal, execution of which would require analysis of out-of-state tuition fee in this dataset. [1] # # ### 7. Which variable would you have to optimize for this goal? What variables would you have to constrain for it to be reasonable? [1] # The variable which I will optimize will be total_enrollment. I will put constrain on mid_career_pay. # ### 8. Which data manipulations would you have to perform for an analysis or an ML system for this goal? What would you have to predict? Would classification or regression be more suitable, and why? [3] # # ### Problem 3: EDA and Data preprocessing [10 points] # In[8]: # Number of time different states occurs in dataset. print(df.state.value_counts()) # In[9]: # Number of time different states_codes occurs in dataset. print(df.state_code.value_counts()) # In[10]: # Different types of University print(df.type.value_counts()) # In[11]: # Different length period print(df.degree_length.value_counts()) # In[12]: # Counting all categorical variables in dataset columns = ['state', 'state_code', 'type', 'degree_length'] for cols in columns: samples = df[cols].nunique() print(\"Total number of categories in\", cols , \" is \", samples) # ### 1. Check how many categorical features exist and for each categorical feature see how many samples belong to each category. [2] # # There are four categorical features which exist, those are: state, state_code, type and degree.length. # # No. of samples belong to each category are: # # state = 9; # state_code = 9; # type = 3; # degree.length = 2; # ### 2. Visualize the distributions of all features in the data set and summarize your findings. [6] # In[13]: get_ipython().run_line_magic('matplotlib', 'inline') import seaborn as sns import matplotlib.pyplot as plt import numpy as np df.plot.hist(subplots= True,bins=100,figsize=(15,30),edgecolor='black');", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "4b7c72c0"}
{"text": "= 9; # state_code = 9; # type = 3; # degree.length = 2; # ### 2. Visualize the distributions of all features in the data set and summarize your findings. [6] # In[13]: get_ipython().run_line_magic('matplotlib', 'inline') import seaborn as sns import matplotlib.pyplot as plt import numpy as np df.plot.hist(subplots= True,bins=100,figsize=(15,30),edgecolor='black'); # ### Summary # # Unnamed: We don't see much distriution as it is equal to the Serial number <br><br> # total_enrollment: There we experience a right skewed plot. Which means number of students getting enrolled are as high as 5000, hence declintion is observed until 35000.<br><br> # n_native: Few thousand native students are only enrolled in university <br><br> # n_asian: This also contrubute as same as n_native, concluding that few students from asian countries went to USA for studying. <br><br> # n_black: This shows a minor right skewed graph which means that we will find these category student on a average, <br><br> # n_hispanic: It shows distribution almost equal to what we have seen in n_asian. Contributing only few thousands of students <br><br> # n_pacific: It shows distribution almost equal to what wehave seen in n_asian. Contributing only few thousands of students<br><br> # n_nonresident: It shows distribution almost equal to what wehave seen in n_asian. Contributing only few thousands of students<br><br> # n_total_minority: Here we see a right skewed, indicating high frequescy of students coming from minorities. <br><br> # n_multiracial: Multiracial shows excalty same distribution as n_pacific. Contributing few thousands of students. <br><br> # n_unknown: This growth is similar to n_hispanic distribution. <br><br> # n_white: This shows a right skewed showing high number of student in each intervals upto 45000 approx. <br><br> # n_women: This is also a right skewed which show high number during 5000 thousand and then there is a certain drop gofing forward. <br><br> # room_and_board: This histogram shows a rise and then fall of frequency before and after 10000's<br><br> # in_state_tuition: Leading at approx 8000, in state tution shows quite a distribution of mean value in each frequency level.<br><br> # in_state_total: The distribution shows a gradual incease and gradual decreas in the frequency, shooting up again at approx 32000, extending the mean upto 63000 approximate.<br><br> # out_of_state_tuition: Show a little co-relation with in_state_total where the average frequency is below 5.<br><br> # out_of_state_total: This represent a left skewed graph. Extending upto 62000 with highest frequency at 35000.<br><br> # early_career_pay: Shows an average frequency distribution of below 10.<br><br> # mid_career_pay: Shows a right skewed, with frequency of 1.0 approaching to 80000. Starting its mean value at 65000 mean approx and extending above 100000. <br><br> # make_world_better_percent: This also contrubute as same as n_native. <br><br> # stem_percent: This also contrubute as same as n_native # ### 3. Split the data set into a training (70%) and a test (30%) set with the help of stratified sampling based on the degree length. Report mean and standard deviation for out_of_state_tuition in train and test data set. [2] # In[14]: # Splitting data into test and train dataset", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "b6993def"}
{"text": "### 3. Split the data set into a training (70%) and a test (30%) set with the help of stratified sampling based on the degree length. Report mean and standard deviation for out_of_state_tuition in train and test data set. [2] # In[14]: # Splitting data into test and train dataset from sklearn.model_selection import train_test_split X = df.loc[:, df.columns != 'out_of_state_tuition'] y = df['out_of_state_tuition'] X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=df['degree_length'], test_size=0.3) # In[15]: y_train.describe() # In[16]: y_test.describe() # ### Problem 4: Data Visualization [16 points] # In[17]: from scipy import stats, optimize, interpolate columns = ['n_native', 'n_asian', 'n_black', 'n_hispanic', 'n_pacific', 'n_nonresident', 'n_total_minority', 'n_multiracial', 'n_unknown', 'n_white', 'n_women'] for col in columns: ax = sns.scatterplot(data =df[(np.abs(stats.zscore(df[col])) < 3)] , x=col, y='total_enrollment', hue='out_of_state_tuition', s=100) plt.style.use('fivethirtyeight') plt.title(col +' vs total_enrollment') plt.xlabel(col) plt.ylabel('total_enrollment') plt.legend(title = 'out_of_state_tuition' ) plt.gcf().set_size_inches(7, 7) plt.show() # ### 1. Describe what can be seen and interpret it. [5] # # The scatterplot between n_native and enrollment shows an association that is positive, linear, and appears to be somewhat strong with a few outliers We can also find good number of native students with the total enrollment of 32000 and few around 350 native students with enrollment approx 60000 <br><br> # # The scatterplot between n_asian and enrollment shows an association that is positive, linear, and appears to be somewhat strong with a medium outliers We can also find high density of asian students upto total_enrollment of 5000 <br><br> # # The scatterplot between n_black and enrollment shows an association that is positive, non-linear, and appears to be somewhat strongly scatters We can also find high density of black students during first few total enrollment of 5000 But this graph also depicts there are quite a number of students scattered around each interval of total_enrollment <br><br> # # The scatterplot between n_hispanic and enrollment shows an association that is positive, linear, and appears to be somewhat strong with a few outliers The number of hispanic students are approx 2500 in the total enrollment of 30000 and below, and only few around 4000 with total enrollment of 60000 <br><br> # # # The scatterplot between n_pacific and enrollment shows an association that is positive, linear, and appears to be somewhat strong with few outliers Maximum of approx 38 pacific students can be observed for the enrollment upto 30000 and few students with medium out of state tuition are represented in the outliers <br><br> # # The scatterplot between n_resident and enrollment shows an association that is positive, curve, and appears to be somewhat strong with few outliers A gap can br observed in nonresident students between around 800 to 1600 However, we can also find few hundered students in the total enrollment of around 60000 <br><br> # # The scatterplot between n_minority and enrollment shows an association that is positive, non-linear, and appears to be somewhat strong with a medium outliers Maximum number of students are getting chance in university with high enrollment of above 2000", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "e96491e3"}
{"text": "hundered students in the total enrollment of around 60000 <br><br> # # The scatterplot between n_minority and enrollment shows an association that is positive, non-linear, and appears to be somewhat strong with a medium outliers Maximum number of students are getting chance in university with high enrollment of above 2000 <br><br> # # The scatterplot between n_multiracial and enrollment shows an association that is positive, linear, and appears to be somewhat strong with a few outliers The linear growth represent with the increase in total enrollment the number of student in multiracial are also getting increased <br><br> # # The scatterplot between n_unknown and enrollment shows an association that is positive, non-linear, and appears to be somewhat strong with a medium outliers We can find good number of unknown students with the total enrollment of 30000 and few around 1700 unknown students with enrollment approx 60000 <br><br> # # The scatterplot between n_white and enrollment shows an association that is positive, linear, and appears to be somewhat light with a few outliers We can find a good linear growth of white students with high number of enrolled students <br><br> # # The scatterplot between n_women and enrollment shows an association that is positive, linear with no outliers We can find a directly proportionality of women students with the number of enrolled students # # # ### 2. Which demographic characteristics are more pronounced for more expensive universities? [3] # There is a high competition in almost all demographics for more expensive university. The most pronounced one I will consider all of them as we can see from the graph that on an average out_of_state_tuition=30000 dollar for all demogrpahics. Moving forward we can also observe high out_of_state_tuition= 40000 for few demographics such as n_asian ad n_white. # ### 3. Write down your assumptions about why universities with lower tuition fees tend to attract certain groups more than more expensive universities. [3] # It is very natural that the University with low tuition fees tends to attract certian group of people because moving to anothter countries for education not only add up tuition fee on a student, but also adds up staying and fooding price. Hence it is also see that the native students are also a part of low tuition fees and reason could be that the native student belongs to that country and hence has to pay low, whereas, which is not in case with non native or students. # ### 4. Which important considerations might these visualization conceal, rather than reveal? Produce visualizations that illustrate the findings that might be unobvious from scatterplots above. [5] # Problem 4 part 4 # ### Problem 5: Correlation [15 points] # Create and look at the correlation plots between the variables. # ### 1. Which feature has the strongest correlation to the target feature? Plot the correlation between them exclusively. [2] # In[18]: def corr(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); corr(df,'out_of_state_tuition')", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "85507d99"}
{"text": "Correlation [15 points] # Create and look at the correlation plots between the variables. # ### 1. Which feature has the strongest correlation to the target feature? Plot the correlation between them exclusively. [2] # In[18]: def corr(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); corr(df,'out_of_state_tuition') # The strongest correlation with respect to out_of_state_tuition is out_of_state_total. # ### 2. Describe and interpret what you can see in this correlation plot. Any unusual occurrences? [3] # In the pearson correlation heatmap, we can observe the most correlated or variables which are highly proportional to out_of_state_tuition are out_of_state_total, in_state_total, in_state_tuition, room_and_board which are relted upto 60%. The reason could be, as these corresponds to the dollars which are costing to the students, and it do also calls for university profit of tuiton fees and room and board.<br> Further going there a drop of atleast 25% in comaparision to mid_career_pay. We can also observe n_nonresident and early_career_pay corresponds equal that is 30% correlation with out_of_state_tuition. <br>Henceforth, there is a sharp drop of values from 27% to 9.5% when comparision hits different type of student in the dataframe.<br> We can also see negative correlation of values to -1.3% to -21%. Which means that out_of_state_tuition are not much related with values lesser than 0. # ### 3. Which three features correlate the most with make_world_better_percent, a percentage of alumni who make the world a better place? [1] # In[19]: def corr1(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); corr1(df,'make_world_better_percent') # Three features correlate the most with make_world_better_percent are out_of_state_total, room_and_board and n_white. These three features are inversely correlated with make_world_better_place. # ### 4. Choose the strongest of these three correlations and propose four hypotheses about the nature of the link between these variables. [4] # 4 Hypothesis are:<br><br> # Hypothesis 1 - Lower is the cost of room_and_board, will have a positive impact on make_world_better_percent.<br><br> # Hypothesis 2 - More students belonging from different ethenicity, less will be the discrimination and it will also give a postive impact on make_world_better_percent. As there will young talent from all around the world. <br><br> # Hypothesis 3 - Lower the overall total cost of studying in University, more student will take addmission and probability of educated youth will increase which will make a positive impact on make_world_better_percent.<br><br> # Hypothesis 4 - More students means more research will be done by University which ultimately impact on make_world_better_percent.<br><br> # ### 5. Which hypothesis do you find the most plausible? Which sources are there supporting it? [2] # The most plausible hypothesis I found is Hypothesis 4. The sources which support it are the number of students from different Ethenicity contributes high percentage of make_world_better_percent. # ### 6. Which features do you lack in this dataset, which would have helped you determine whether your hypothesis is likely true? [1]", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "af590bfd"}
{"text": "most plausible hypothesis I found is Hypothesis 4. The sources which support it are the number of students from different Ethenicity contributes high percentage of make_world_better_percent. # ### 6. Which features do you lack in this dataset, which would have helped you determine whether your hypothesis is likely true? [1] # The feature which is lacking in dataset which would have determine my hypothesis to be true is \"Research dataset\". The number of research done by each University would have greatly contribute to prove my hypothesis true. # ### 7. Explain the difference between Pearson and Spearman correlation coefficients. Which of them have you just used in this problem? Which of them would be more feasible for the analysis you are doing?[2] # In Pearson correlation the variables are directly proportional to each other, or we can say there is a linear relationship between two values. As the one change in one variable occurs, proportional change in other variable also occurs.<br><br> # # In Spearman, there is monotonic relationship between two variables. Variable change togeather but not necessatily at constant rate.<br><br> # # I have used Pearson correlation.<br><br> # # Spearman correlation may have a better analysis. # ### Problem 6: Correlation 2 [16 points] # # Create new attributes by combining different features with each other. # In[20]: dfethnicity = df.n_native+df.n_asian+df.n_black+df.n_hispanic+df.n_pacific+df.n_nonresident+df.n_total_minority+df.n_multiracial+df.n_unknown+df.n_white+df.n_women dfethnicity # In[21]: dfmid = pd.merge(df, dfethnicity.to_frame(),left_index=True, right_index=True) dfmid = dfmid.drop([\"n_native\",\"n_asian\",\"n_black\",\"Unnamed: 0\",\"n_pacific\",\"n_nonresident\",\"n_total_minority\",\"n_multiracial\",\"stem_percent\",\"n_unknown\",\"n_white\",\"n_women\",\"state_code\",\"room_and_board\",\"in_state_tuition\",\"n_hispanic\"], axis=1) dfmid.columns.values[11] = \"ethnicity\" dfmid # ### 1. Explain why you think a combination will be useful before you create them. [2] # The newly created combination \"dfmid\" is useful as I have combined all the different ethnicity into one which reduces the size and complexity of dataset drastically. Moving forward I removed more features as there were already combined into one. This combination now give a proper visualization of datas which may postively impact the dataset. # ### 2. Check their correlation with the out_of_state_tuition in comparison with the other features from before. Show the correlations with the target feature in a descending order. [2] # In[22]: # Heat map of newly created dataset with respect to out_of_state_tuition def make_corr(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); make_corr(dfmid,'out_of_state_tuition') # ### 3. Do your newly created features have higher correlation with the target feature? [1] # Yes, as represented from the above heatmap, I find these features to have higher correlation with out_of_state_tuition. # ### 4. Take a look at the data from tuition_income dataset and decide which features or combinations of features you think will also be beneficial for prediction. Repeat steps 1-3 for them. Note that you may need to make some extra transformations to add them. [4] # In[23]: dftution.head() dfmerge = pd.merge(dfmid,dftution,how=\"inner\",on = \"name\") dftuiton_filter = dftution[dftution[\"name\"].isin(dfmerge.name.unique().tolist())] dftuiton_filter = pd.concat([dftuiton_filter,pd.get_dummies(dftuiton_filter.income_lvl)],axis = 1).drop(\"income_lvl\",axis=1) dftuiton_filter_merge = pd.DataFrame() low = [] mid = [] high = [] veryhigh = [] highest = [] total_price = [] year = [] campus = [] net_cost=[] income_lvl = []", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "2e1b17a7"}
{"text": "add them. [4] # In[23]: dftution.head() dfmerge = pd.merge(dfmid,dftution,how=\"inner\",on = \"name\") dftuiton_filter = dftution[dftution[\"name\"].isin(dfmerge.name.unique().tolist())] dftuiton_filter = pd.concat([dftuiton_filter,pd.get_dummies(dftuiton_filter.income_lvl)],axis = 1).drop(\"income_lvl\",axis=1) dftuiton_filter_merge = pd.DataFrame() low = [] mid = [] high = [] veryhigh = [] highest = [] total_price = [] year = [] campus = [] net_cost=[] income_lvl = [] for i in dftuiton_filter.name.unique(): temp = dftuiton_filter[dftuiton_filter[\"name\"]==i] total_price.append(temp[\"total_price\"].median()) year.append(temp['year'].mode(dropna = True).iloc[0]) campus.append(temp['campus'].mode(dropna = True).iloc[0]) net_cost.append(temp['net_cost'].median()) low.append(temp[\"0 to 30,000\"].sum()) mid.append(temp[\"30,001 to 48,000\"].sum()) high.append(temp[\"48_001 to 75,000\"].sum()) veryhigh.append(temp[\"75,001 to 110,000\"].sum()) highest.append(temp['Over 110,000'].sum()) dftuiton_filter_merge[\"name\"] = dftuiton_filter.name.unique().tolist() dftuiton_filter_merge[\"year\"] = year dftuiton_filter_merge[\"total_price\"] = total_price dftuiton_filter_merge[\"net_cost\"] = net_cost dftuiton_filter_merge[\"0 to 30,000\"] = low dftuiton_filter_merge[\"30,001 to 48,000\"] = mid dftuiton_filter_merge[\"48,001 to 75,000\"] = high dftuiton_filter_merge[\"75,001 to 110,000\"] = veryhigh dftuiton_filter_merge[\"Over 110,000\"] = highest dftuiton_filter_merge[\"campus\"] = campus # In[24]: dfuni_tuition_combined = pd.merge(dfmid,dftuiton_filter_merge,how=\"inner\",on = \"name\") # In[25]: # Displaying merge dataset of university and tuition income. dfuni_tuition_combined # The combination is useful because it narrow down data to more specific dataset which contain all the useful information needed. # In[26]: # Correlation heatmap of combine datatset with out_of_state_tuition def corr(dataframe,i): plt.figure(figsize=(6, 6)) heatmap = sns.heatmap(pd.DataFrame(dataframe[dataframe.columns[1:]].corr()[i][:]).sort_values(ascending=False,by=i), annot=True) heatmap.set_title('Pearson Correlation Heatmap', fontdict={'fontsize':10}, pad=5); corr1(dfuni_tuition_combined,'out_of_state_tuition') # Yes, my newly created features are having higher correlation with the target feature that is out_of_state_tuition. # ### 8. Do any of your new features shine the new light on possible determinants of what makes students feel they are making the world the better place? Explain your insights. [2] # # If we compare both the heat map that is <b>Correlation heatmap of combine datatset with out_of_state_tuition</b> and <b> Heat map of newly created dataset with respect to out_of_state_tuition</b>, we will not see much difference on make_world_better_percent. In Correlation heatmap of combine datatset with out_of_state_tuition the value is -0.26 and in Heat map of newly created dataset with respect to out_of_state_tuition is -0.21 i.e.; there is only a growth of -0.05 units value. # ### Problem 7: Data cleaning [10 points] # ### 1. Find out which variables contain missing values. If there are any, how many values are missing? [2] # In[27]: dfuni_tuition_combined.isnull().sum() * 100 / len(dfuni_tuition_combined) # ### 2. Which approaches exist for handling missing data? Describe two approaches of how to handle them. Write one advantage and one disadvantage for each of those methods. [4] # Approaches exist for handling missing data are: Multiple Imputatuion and K Nearest Neighbors(KNN)<br><br> # Multiple Imputation: It is mainly for large dataset. In this uses subsituting data in place of missing data.<br> # Advantage: Results are readily interpreted.<br> # Disadvantage:It assumes a random data in place of missing data.<br><br> # KNN: This method uses Eucliden distance between neighbouring data cordinates to know how similar data is.<br> # Advantage: It do not assumes data.<br> # Disadvamtage: Since it stores all training data, hence it requires large memory capacity. # # ### 3. Handle the missing data by methods you find the most suitable. Explain how you chose the method for each column. [3] # In[28]: # Handling missing datas. dfuni_tuition_combined_imputation = dfuni_tuition_combined.fillna(dfuni_tuition_combined.mean()) # In[29]: dfuni_tuition_combined_imputation.head()", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "cbfe0316"}
{"text": "Disadvamtage: Since it stores all training data, hence it requires large memory capacity. # # ### 3. Handle the missing data by methods you find the most suitable. Explain how you chose the method for each column. [3] # In[28]: # Handling missing datas. dfuni_tuition_combined_imputation = dfuni_tuition_combined.fillna(dfuni_tuition_combined.mean()) # In[29]: dfuni_tuition_combined_imputation.head() # While cleaning the data we observed 3 features that is are early_career_pay, mid_career_pay, make_world_better_percent denotes missing values. So I implemented Multiple Imputation technique to fill the missing values as thats is the best technique for large dataset. # In[30]: # Splitting values into train and test set with 70% versus 30% ratio, respectively. dfuni_tuition_combined_imputation.drop([\"name\"],inplace=True,axis=1) dfuni_tuition_combined_imputation[\"year\"] = dfuni_tuition_combined_imputation.year.apply(str) X = dfuni_tuition_combined_imputation.loc[:, dfuni_tuition_combined_imputation.columns != 'out_of_state_tuition'] y = dfuni_tuition_combined_imputation['out_of_state_tuition'] X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=dfuni_tuition_combined_imputation['degree_length'], test_size=0.3,random_state = 131) # ### 4. Handle the categorical data with one-hot-encoding. [1] # In[31]: X_train = pd.get_dummies(X_train) X_test = pd.get_dummies(X_test) # ### Problem 8: Feature scaling [9 points] # # ### 1. What feature scaling methods exist? Name five methods and write a short description for three of them. Pick one to use for this data set. [6] # Feature Scaling is the process by which we transform data into a better version. It is use to normalize the features in the dataset into a finite range.<br><br> # Five Feature Scaling methods are:<br><br> # <b>Absolute Maximum Scaling:</b> In this scaling, datas are scaled into maximum value and the results varies approximately within the range of -1 to 1<br><br> # <b>Min-Max Scaling:</b> It is feature pre-processing technique in which data are scaled in fixed range of 0 to 1.<br><br> # <b>Normalization:</b> It is the feature by which numeric columns in dataset changes to common scale.<br><br> # <b>Standardization:</b> In this method the datas are converted into uniform format which is further used to different operations.<br><br> # <b>Robust Scaling:</b> In this the algorithm scale features that are robust to outliers. # In[32]: dfuni_tuition_combined_imputation.head() # In[33]: # Performing Standardization method for my dataset. from sklearn.preprocessing import StandardScaler def scal_independent(DataFrame): scale = StandardScaler() cols = ['total_enrollment','in_state_total', 'out_of_state_total','early_career_pay', 'mid_career_pay', 'make_world_better_percent','total_price', 'net_cost','ethnicity'] DataFrame[cols] = scale.fit_transform(DataFrame[cols]) return DataFrame def scal_dependent(DataFrame): dataframe = scale(DataFrame) return dataframe X_train = scal_independent(X_train) X_test = scal_independent(X_test) y_train = scal_dependent(y_train) y_test = scal_dependent(y_test) # In[34]: # Dropping non-numeric features from imputed dataset. X_train.drop(list(set(X_train.columns.tolist())- set(X_test.columns.tolist())),axis=1,inplace=True) # In[35]: X_test.drop('state_Montana',inplace = True, axis = 1) # ### 2. Find the feature importance with a quick Random Forest and show them in a plot. What insights do you get out of it? [3] # In[36]: from sklearn.datasets import make_regression X, y = make_regression(n_samples=165, n_features=52, n_informative=5, random_state=1) mdl = RandomForestRegressor() mdl.fit(X_train, y_train) feature_scores = pd.Series(mdl.feature_importances_, index=X_train.columns).sort_values(ascending=False) f, ax = plt.subplots(figsize=(30, 24)) ax = sns.barplot(x=feature_scores, y=feature_scores.index, data=df) ax.set_title(\"Visualize feature scores of the features\") ax.set_yticklabels(feature_scores.index) ax.set_xlabel(\"Feature importance score\") ax.set_ylabel(\"Features\") plt.show() # I can observe that not much feature shows up with the Random Forest feature scaling method.", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "cd70de17"}
{"text": "n_informative=5, random_state=1) mdl = RandomForestRegressor() mdl.fit(X_train, y_train) feature_scores = pd.Series(mdl.feature_importances_, index=X_train.columns).sort_values(ascending=False) f, ax = plt.subplots(figsize=(30, 24)) ax = sns.barplot(x=feature_scores, y=feature_scores.index, data=df) ax.set_title(\"Visualize feature scores of the features\") ax.set_yticklabels(feature_scores.index) ax.set_xlabel(\"Feature importance score\") ax.set_ylabel(\"Features\") plt.show() # I can observe that not much feature shows up with the Random Forest feature scaling method. # ### Problem 9: Test data [6 points] # # Perform the same data cleaning steps from questions 7 and 8 for the test data set. (Replace missing # values, handle the categorical data, add the new features, and scale the features) [6] # # #### Already perform above in question 7 & 8. # ### Problem 10: Regression [19 points] # # Select and train the following regression models on the training set. Linear model, support vector # regression, and random forest. # ### 1. Evaluate the three regression models on the test set. Which model performs best? [9] # In[37]: # Performing Linear Regression. r =LinearRegression() model = r.fit(X_train, y_train) y_pred = r.predict(X_test) residuals = y_test - y_pred mae = round(metrics.mean_absolute_error(y_test, y_pred),2) mse = round(metrics.mean_squared_error(y_test, y_pred),2) rmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2) mape = round(np.mean(abs(residuals/y_test)),2) accuracy = str(100-(round(np.mean(abs(residuals/y_test)),4)*100)) +' %' nRMSE = round((rmse/70.127909),2) aError = round(np.mean(abs(residuals)),2) error_df = pd.DataFrame(data = [[mae, mse, rmse,mape, accuracy, nRMSE,aError]], columns = ['MAE', 'MSE', 'RMSE','MAPE','Model Accuracy','Normalized RMSE','Average Error'], index = ['Linear Regression']) print(error_df) # In[38]: # Performing Random Forest Regression r = RandomForestRegressor() model = r.fit(X_train, y_train) y_pred = r.predict(X_test) residuals = y_test - y_pred mae = round(metrics.mean_absolute_error(y_test, y_pred),2) mse = round(metrics.mean_squared_error(y_test, y_pred),2) rmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2) mape = round(np.mean(abs(residuals/y_test)),2) accuracy = str(100-(round(np.mean(abs(residuals/y_test)),4)*100)) +' %' nRMSE = round((rmse/70.127909),2) aError = round(np.mean(abs(residuals)),2) error_df = pd.DataFrame(data = [[mae, mse, rmse,mape, accuracy, nRMSE,aError]], columns = ['MAE', 'MSE', 'RMSE','MAPE','Model Accuracy','Normalized RMSE','Average Error'], index = ['Random Forest Regression']) print(error_df) # In[39]: # Performing Support Vector Regression r =SVR () model = r.fit(X_train, y_train) y_pred = r.predict(X_test) residuals = y_test - y_pred mae = round(metrics.mean_absolute_error(y_test, y_pred),2) mse = round(metrics.mean_squared_error(y_test, y_pred),2) rmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2) mape = round(np.mean(abs(residuals/y_test)),2) accuracy = str(100-(round(np.mean(abs(residuals/y_test)),4)*100)) +' %' nRMSE = round((rmse/70.127909),2) aError = round(np.mean(abs(residuals)),2) error_df = pd.DataFrame(data = [[mae, mse, rmse,mape, accuracy, nRMSE,aError]], columns = ['MAE', 'MSE', 'RMSE','MAPE','Model Accuracy','Normalized RMSE','Average Error'], index = ['SVR']) print(error_df) # ##### Solution: # Random forest model performs the best. # ### 2. Explain one approach about how the models can be further optimized. [3] # One approach by which a model can be further be optimize is through isotonic regression. Isotonic Regression is a technique by which a free-form line is fitted to a sequence of observation in such a way that fitted line is neither non-decreasing or non-increasing everywhere rather should be close to the observation as much as possible. # ### 3. Based on the data you have at hand and your background knowledge, which problems might you encounter when trying use your best model to predict out-of-state tuition fee for the next year? [2] #", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "dc1cdbee"}
{"text": "non-increasing everywhere rather should be close to the observation as much as possible. # ### 3. Based on the data you have at hand and your background knowledge, which problems might you encounter when trying use your best model to predict out-of-state tuition fee for the next year? [2] # # ### 4. Name and explain two accuracy metrics for regression other than MSE or RMSE. Are any of them better suited for evaluating your models, given the goal you have formulated earlier? [2] # Two accuracy model except MSE and MAPE are: <br><br> # MAE (Mean Absolute Error): It is the average measure of errors in a set of models without taking vector into account.<br><br> # MAPE (Mean Absolute Percentage Error): It is the percentage of the average difference between forecasted and true values.<br><br> # Yes they both sucessfully suited for the evaluation of my model. # # ### 5. What has to change in real life to completely invalidate the results you get and accuracy of your model? I.e., what laws, natural events, societal changes have to happen to make conclusions based on this dataset inadmissible for further decision-making? [3] # If out_of_state_tuition and in_state_total feature gets invalidate, the results and accuracy of the whole model will become inaccurate. If such a law is passed that no fees will be taken by the students the dataset will become inadmissible for further decision making. # ### Problem 11: Classification [21 points] # # Categorize the target variable (out_of_state_tuition) into five categories and build a classification model # for the above pre-processed data. # ### 1. Train the following classification models on the training set for classification and evaluate the models on the test set: SVM, k-NN, and Random Forest. [9] # In[40]: # Function for classification. def classification(Xtrain, Xtest, ytrain, ytest, classifier): cls = classifier cls.fit(Xtrain, ytrain) ypred = cls.predict(Xtest) print(f\"Accuracy of the classifier is: {accuracy_score(ytest, ypred)}\") print(f\"Precision Score of the classifier is: {precision_score(ytest, ypred,average='macro')}\") print(f\"Recall Score of the classifier is: {recall_score(ytest, ypred,average='macro')}\") print(f\"F1 Score of the classifier is: {f1_score(ytest, ypred,average='macro')}\") plot_confusion_matrix(classifier, Xtest, ytest) # In[41]: # Creating bins for y_train and y_test y_train_class = pd.cut(y_train, bins = [-3,-1,0,1,3,4], include_lowest = True, labels=[1,2,3,4,5]) y_test_class = pd.cut(y_test, bins = [-3,-1,0,1,3,4], include_lowest = True, labels=[1,2,3,4,5]) # In[42]: # Plotting Confusion matrix for model k-NN classification(X_train, X_test, y_train_class, y_test_class, KNeighborsClassifier()) # In[43]: # Plotting Confusion matrix for model SVM classification(X_train, X_test, y_train_class, y_test_class, SVC()) # In[44]: # Plotting Confusion matrix for model Random Forest classification(X_train, X_test, y_train_class, y_test_class, RandomForestClassifier()) # ### 3. Which model performs best based on which evaluation method? [2] # Random Forest perform best on our model. The can be seen by the accuracy of the classifier which is 88.4615% approx # ### 4. Explain the evaluation method you used. [2]", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "3c7d52bb"}
{"text": "classification(X_train, X_test, y_train_class, y_test_class, RandomForestClassifier()) # ### 3. Which model performs best based on which evaluation method? [2] # Random Forest perform best on our model. The can be seen by the accuracy of the classifier which is 88.4615% approx # ### 4. Explain the evaluation method you used. [2] # The evaluation method I have used here are: <br><br> # <b>SVM:</b> SVM is a supervised learning method. These are used in high dimenation spaces.<br><br><b>k-NN:</b> This method does not make assumption on underlying datas. It perform action at the time of classifcation, hence it is also known as lazy lerner algorithm.<br><br>and,<br><br> <b>Random Forest:</b> It uses ensemble learning that is it combines classifiers to perform solution to complex problem. # ### 5. For which applications would classification into these artificial categories be more useful than regression? Name at least two. [2] # Application where classification will be more useful than regression is when there will discrete value. In these artificial category we can opt for \"type\" and \"degree_length\". # ### 6. For which applications would regression on original values be more useful? Name at least two. [2] # Regression is used in contunues values. Here we can use in \"out_of_state_total\" and \"total_enrollment\" # ### Part 2 # # ### Problem 12: Text Mining [40 points] # # The dataset GoodReads.csv contains book descriptions. Load this dataset and draw random sample of size 5000 from it, setting the seed to 45. # In[45]: Goodreads = pd.read_csv('GoodReads.csv') Goodreads.head() # In[46]: # dfgr is the name of the data frame of Good Reads book dfgr = Goodreads.sample(n=5000, random_state=45) dfgr.head() # ### 1. Plot and describe the distribution of average ratings of the books.[2] # In[47]: fig = px.histogram(x = dfgr['rating'], width=800, marginal='box', labels={'col':'col'}) fig.update_layout(title = 'Average Ratings of books') fig.show() # ### 2. Clean the data in your dataframe. Explain your reasoninng for selecting the cleaning methods you decided to use.[3] # In[48]: dfgr = dfgr.drop(['img', 'isbn', 'isbn13', 'link'], axis = 1) dfgr.head() # I decided to drop image, link and isbn, because, img and link are the web-links for the picture and the PDF file of the book, I also ommitted the isbn, which will not be using. # ### 3. Categorize the rating variable into several categories. Explain how many categories you decided to have, where you drew the line, and why you decided to do it this way[5] # In[49]: category = pd.cut(dfgr['rating'], bins = [0, 1, 2, 3, 4, 5.1], include_lowest = True, labels=['Very Poor', 'Poor', 'Neutral', 'Good', 'Very Good']) dfgr['Rating Category'] = category dfgr.head() # ### 4. Get a feel about the distribution of text lengths of the book descriptions by adding a new feature for the length of each message. Check the statistical values. [2] # In[50]: msgLen = dfgr['desc'].str.len() dfgr['Description Length'] = msgLen dfgr.head() # ### 5. Visualize the distribution of text lengths with a histogram, where the colouring is according to the rating category. [2] # In[51]:", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "293037eb"}
{"text": "by adding a new feature for the length of each message. Check the statistical values. [2] # In[50]: msgLen = dfgr['desc'].str.len() dfgr['Description Length'] = msgLen dfgr.head() # ### 5. Visualize the distribution of text lengths with a histogram, where the colouring is according to the rating category. [2] # In[51]: x1 = dfgr.loc[dfgr['Rating Category'] == 'Very Poor', 'Description Length'] x2 = dfgr.loc[dfgr['Rating Category'] == 'Poor', 'Description Length'] x3 = dfgr.loc[dfgr['Rating Category'] == 'Neutral', 'Description Length'] x4 = dfgr.loc[dfgr['Rating Category'] == 'Good', 'Description Length'] x5 = dfgr.loc[dfgr['Rating Category'] == 'Very Good', 'Description Length'] kwargs = dict(alpha = 0.5, bins = 80) plt.figure(figsize=(20,9)) plt.hist(x1, **kwargs, color='#465362', label='Very Poor') plt.hist(x2, **kwargs, color='#011936', label='Poor') plt.hist(x3, **kwargs, color='#C2EABD', label='Neutral') plt.hist(x4, **kwargs, color='#F9DC5C', label='Good') plt.hist(x5, **kwargs, color='#ED254E', label='Very Good') plt.gca().set(title = 'Frequency Histogram of Text Length', ylabel = 'Frequency', xlabel = 'Text Length') plt.legend(loc = 'upper right', prop = {'size': 20}); # ### 6. Create a random stratified training and test split (70/30 split). Verify the correct proportions of the splitted data sets by creating proportion table. [1] # In[52]: X = dfgr.loc[:, dfgr.columns != 'Description Length'] y = dfgr['Description Length'] X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=dfgr['Rating Category'],test_size=0.3) grStr = {'X': [len(X_train)/(len(X_train)+len(X_test)),len(X_test)/(len(X_train)+len(X_test))], 'Y': [len(y_train)/(len(y_train)+len(y_test)),len(y_test)/(len(y_train)+len(y_test))]} grProportion = pd.DataFrame(grStr) grProportion.head() # In[53]: # Train dataset Xtrain_tab = pd.crosstab(index = X_train['Rating Category'], columns = 'Count') Xtrain_tab # In[54]: # Test dataset Xtest_tab = pd.crosstab(index = X_test['Rating Category'], columns = 'Count') Xtest_tab # ### 7. Tokenize the descriptions with help of the quanteda package and illustrate the effect of each action by showing the content of some of the reviews in the training data set: # # Remove the numbers, punctuations, symbols, and hyphens. Turn the texts in the reviews into lower case. Remove the stopwords in the modified training set (the tokens) with the predefined stopword list of quanteda. Perform stemming on the tokens. [5] # In[55]: import nltk from nltk.tokenize import sent_tokenize nltk.download('punkt') nltk.download('stopwords') import string from nltk.corpus import stopwords from nltk.stem.snowball import SnowballStemmer # In[56]: # Getting rid of NaN in the description column, to prepare it for tokenization dfgr['desc']=dfgr['desc'].fillna('') dfgr.head(40) # In[57]: # Creating the new string with lowercase desctiption dfgr['Lower Case'] = dfgr['desc'].str.lower() dfgr.head(25) # In[58]: # Getting rid of the punctuations in the Lower Case variable dfgr['Lower Case'] = dfgr['Lower Case'].str.replace('[^\\w\\s]', '') dfgr.head() # In[59]: # Deleting stopwords stop = stopwords.words('english') dfgr['Without Stopwords'] = dfgr['Lower Case'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) dfgr.head() # In[60]: # Create a new column with tokenized lowercase column dfgr['Tokenized Description'] = dfgr.apply(lambda row: nltk.word_tokenize(row['Without Stopwords']), axis = 1) dfgr.head() # In[61]: # Stemming stemmer = SnowballStemmer('english') dfgr['Stemmed Token'] = dfgr['Tokenized Description'].apply(lambda x: [stemmer.stem(y) for y in x]) dfgr.head() # ### 8. Create a bag-of-words model and add bi-grams to the normal feature matrix. [2] # In[62]: vocab = [] for i in dfgr['Stemmed Token']: for x in i: vocab.append(x) vocab # In[63]: # Getting rid of duplicated values vocabDupl = list(dict.fromkeys(vocab)) vocabDupl # In[64]:", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "a80f728c"}
{"text": "y in x]) dfgr.head() # ### 8. Create a bag-of-words model and add bi-grams to the normal feature matrix. [2] # In[62]: vocab = [] for i in dfgr['Stemmed Token']: for x in i: vocab.append(x) vocab # In[63]: # Getting rid of duplicated values vocabDupl = list(dict.fromkeys(vocab)) vocabDupl # In[64]: def convert(org_list, separator = ' '): return separator.join(org_list) sentences = [] for j in dfgr['Stemmed Token']: sentence = '' sentence = convert(j) sentences.append(sentence) sentences # In[65]: sentence100 = sentences[:100] print(sentence100) # In[66]: # Transforming given text into vectors from sklearn.feature_extraction.text import CountVectorizer import itertools count_vec = CountVectorizer() cdf = count_vec.fit_transform(sentence100) bag = pd.DataFrame(cdf.toarray(), columns = count_vec.get_feature_names()) bag # In[67]: # Bi-Gram for 100 sentences bi_gram = [(x, i.split()[j + 1]) for i in sentence100 for j, x in enumerate(i.split()) if j < len(i.split()) - 1] print (\"The BiGram are :\\n\\n \" + str(bi_gram)) # ### 9. Build a function for relative term frequency (TF) and another one to calculate the inverse document frequency (IDF). # In[68]: # Calculating TF def tf(wordDict, bow): tfDict = {} bowCount = len(bow) for word, count in wordDict.items(): tfDict[word] = count/float(bowCount) return tfDict # In[69]: # Calculating IDF def idf(docList): import math idfDict = dict() n = len(docList) idfDict = dict.fromkeys(docList[0].keys(), 0) for doc in docList: for word, val in doc.items(): if val == 0: idfDict[word] += 1.0 for word, val in idfDict.items(): idfDict[word] = math.log10(n / float(val)) return idfDict # In[70]: import __future__ from __future__ import division # Calculating TFIDF def computeTFIDF(tfBow,idfs): tfidf = {} for word, val in tfBow.items(): tfidf[word] = val * idfs[word] return tfidf # In[71]: # Create a dictionary, to count the words in a certain second wordDict1 = dict.fromkeys(vocabDupl, 0) wordDict2 = dict.fromkeys(vocabDupl, 0) # Counting words for word in dfgr['Stemmed Token'].values[0]: wordDict1[word] += 1 for word in dfgr['Stemmed Token'].values[1]: wordDict1[word] += 1 wordDict1 # In[72]: # Convert into Data Frame pd.DataFrame([wordDict1]) # In[74]: tfBow1 = tf(wordDict1, dfgr['Stemmed Token'].values[0]) tfBow2 = tf(wordDict1, dfgr['Stemmed Token'].values[1]) tfBow1 # In[75]: idfs = idf([wordDict1, wordDict2]) # In[76]: tfidfBow1 = computeTFIDF(tfBow1, idfs) tfidfBow2 = computeTFIDF(tfBow2, idfs) pd.DataFrame([tfidfBow1, tfidfBow2])", "source": "Repo:Data-Analysis-Project:DA Project.py", "section": "Data-Analysis-Project", "hash": "27dc72c6"}
{"text": "# CODE CELL import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler # CODE CELL df = pd.read_excel(\"CS2_37_1_18_11.xlsx\") df # MARKDOWN CELL Test_Time(s): The time in seconds since the start of the battery test. It represents how long the battery has been undergoing the testing process. Date_Time: The date and time stamp when the specific data point was recorded. It provides the chronological timing of the battery measurements. Step_Time(s): The time in seconds that has passed since the start of the current step within the battery test. It helps track the duration of individual steps during the testing process. Step_Index: A numerical index that represents the specific step within the battery test. Each step could involve different conditions or operations applied to the battery. Cycle_Index: A numerical index that represents the specific cycle of the battery test. A cycle typically refers to one complete charge and discharge cycle of the battery. Current(A): The electric current in Amperes (A) that is either flowing into the battery during charging or flowing out of the battery during discharging. It's a measure of the rate of electron flow. Voltage(V): The electric potential difference in Volts (V) across the battery's terminals. It represents the battery's electrical energy per unit charge. Charge_Capacity(Ah): The capacity of the battery during the charging process, measured in Ampere-hours (Ah). It indicates the amount of charge the battery can store when charging. Discharge_Capacity(Ah): The capacity of the battery during the discharging process, also measured in Ampere-hours (Ah). It shows the amount of charge the battery can deliver when discharging. Charge_Energy(Wh): The energy consumed or produced during the charging process, measured in Watt-hours (Wh). It represents the amount of electrical energy stored in the battery. Discharge_Energy(Wh): The energy consumed or produced during the discharging process, also measured in Watt-hours (Wh). It represents the amount of electrical energy released from the battery. dV/dt(V/s): The rate of change of voltage with respect to time, often expressed in Volts per second (V/s). It gives insights into the battery's voltage behavior over time. Internal_Resistance(Ohm): The internal resistance of the battery, measured in Ohms (Ω). It represents the opposition to the flow of electric current within the battery itself. # CODE CELL df.shape # CODE CELL df.info() # CODE CELL for column in df.columns: unique_values = df[column].nunique() print(f\"{column} : {unique_values}\") # CODE CELL df.isnull().sum() # CODE CELL df.describe() # CODE CELL # Dropping unnecessary columns df_modify = df.drop(['Is_FC_Data', 'AC_Impedance(Ohm)', 'ACI_Phase_Angle(Deg)'], axis=1) # CODE CELL df_modify.info() # MARKDOWN CELL ### Health status # CODE CELL features = ['Charge_Capacity(Ah)', 'Discharge_Capacity(Ah)', 'Internal_Resistance(Ohm)', 'Voltage(V)'] scaler = MinMaxScaler() normalized_features = scaler.fit_transform(df_modify[features]) # Define weights for each feature weights = [0.3, 0.3, 0.2, 0.2] # Calculate the battery health metric battery_health_metric = (normalized_features * weights).sum(axis=1) plt.figure(figsize=(8, 6)) plt.hist(battery_health_metric, bins=20, color='blue') plt.xlabel('Battery Health Metric') plt.ylabel('Frequency') plt.title('Distribution of Battery Health Metric') plt.grid(True) plt.show() # Calculate threshold based on histogram threshold_low = 0.2 threshold_high = 0.8", "source": "Repo:NASA-Battery-project:Li ion.ipynb", "section": "NASA-Battery-project", "hash": "b6ed5e81"}
{"text": "Define weights for each feature weights = [0.3, 0.3, 0.2, 0.2] # Calculate the battery health metric battery_health_metric = (normalized_features * weights).sum(axis=1) plt.figure(figsize=(8, 6)) plt.hist(battery_health_metric, bins=20, color='blue') plt.xlabel('Battery Health Metric') plt.ylabel('Frequency') plt.title('Distribution of Battery Health Metric') plt.grid(True) plt.show() # Calculate threshold based on histogram threshold_low = 0.2 threshold_high = 0.8 # Calculate battery health status based on thresholds battery_health_status = [] for value in battery_health_metric: if value < threshold_low: battery_health_status.append(\"Unhealthy\") elif value < threshold_high: battery_health_status.append(\"Intermediate\") else: battery_health_status.append(\"Healthy\") df_modify['Battery_Health_Status'] = battery_health_status # CODE CELL #df_modify # CODE CELL health_status_counts = df_modify['Battery_Health_Status'].value_counts() # Define colors for each category colors = ['red' if status == 'Unhealthy' else 'green' if status == 'Healthy' else 'blue' for status in health_status_counts.index] # Create a pie chart plt.figure(figsize=(12, 8)) plt.pie(health_status_counts.values, labels=health_status_counts.index, colors=colors, autopct=lambda p: '{:.0f} ({:.1f}%)'.format(p * sum(health_status_counts.values) / 100, p), startangle=140) plt.title('Battery Health Status Distribution') plt.axis('equal') plt.show() # CODE CELL df_modify.info() # MARKDOWN CELL ### Aggregation # CODE CELL # Step-Based Aggregations step_statistics = df_modify.groupby('Step_Index')[['Voltage(V)', 'Current(A)']].agg(['mean', 'median', 'std']) # Cycle-Based Aggregations cycle_statistics = df_modify.groupby('Cycle_Index')[['Voltage(V)', 'Current(A)']].agg(['mean', 'median', 'std']) # Visualize Step-Based Aggregations plt.figure(figsize=(16, 8)) plt.plot(step_statistics.index, step_statistics['Voltage(V)']['mean'], marker='^', label='Mean Voltage (Step)', color='blue') plt.plot(step_statistics.index, step_statistics['Current(A)']['mean'], marker='^', label='Mean Current (Step)', color='orange') plt.plot(step_statistics.index, step_statistics['Voltage(V)']['median'], marker='d', label='Median Voltage (Step)', color='blue', linestyle='dashed') plt.plot(step_statistics.index, step_statistics['Current(A)']['median'], marker='d', label='Median Current (Step)', color='orange', linestyle='dashed') plt.plot(step_statistics.index, step_statistics['Voltage(V)']['std'], marker='*', label='Std Dev Voltage (Step)', color='blue', linestyle='dotted') plt.plot(step_statistics.index, step_statistics['Current(A)']['std'], marker='*', label='Std Dev Current (Step)', color='orange', linestyle='dotted') plt.title('Step-Based Aggregations') plt.xlabel('Step Index') plt.ylabel('Values') plt.legend() plt.grid() plt.show() # Visualize Cycle-Based Aggregations plt.figure(figsize=(16, 8)) plt.plot(cycle_statistics.index, cycle_statistics['Voltage(V)']['mean'], marker='^', label='Mean Voltage (Cycle)', color='green') plt.plot(cycle_statistics.index, cycle_statistics['Current(A)']['mean'], marker='^', label='Mean Current (Cycle)', color='red') plt.plot(cycle_statistics.index, cycle_statistics['Voltage(V)']['median'], marker='d', label='Median Voltage (Cycle)', color='green', linestyle='dashed') plt.plot(cycle_statistics.index, cycle_statistics['Current(A)']['median'], marker='d', label='Median Current (Cycle)', color='red', linestyle='dashed') plt.plot(cycle_statistics.index, cycle_statistics['Voltage(V)']['std'], marker='*', label='Std Dev Voltage (Cycle)', color='green', linestyle='dotted') plt.plot(cycle_statistics.index, cycle_statistics['Current(A)']['std'], marker='*', label='Std Dev Current (Cycle)', color='red', linestyle='dotted') plt.title('Cycle-Based Aggregations') plt.xlabel('Cycle Index') plt.ylabel('Values') plt.legend() plt.grid() plt.show() # CODE CELL # Discharge Efficiency charge_energy = df['Charge_Energy(Wh)'] discharge_energy = df['Discharge_Energy(Wh)'] discharge_efficiency = (discharge_energy / charge_energy) * 100 df_modify['Discharge_Efficiency'] = discharge_efficiency plt.figure(figsize=(15, 8)) plt.hist(discharge_efficiency, bins=20, color='orange', edgecolor='black') plt.title('Distribution of Discharge Efficiency (Wh)') plt.xlabel('Efficiency (%)') plt.ylabel('Frequency') plt.tight_layout() plt.show() # CODE CELL time_series_columns = ['Date_Time', 'Discharge_Efficiency', 'Battery_Health_Status'] time_series_df = df_modify[time_series_columns] # Set Date_Time as the index time_series_df.set_index('Date_Time', inplace=True) # Plot time-series trends of Discharge Efficiency by Battery Health Status plt.figure(figsize=(16, 8)) for health_status in time_series_df['Battery_Health_Status'].unique(): subset = time_series_df[time_series_df['Battery_Health_Status'] == health_status] plt.plot(subset.index, subset['Discharge_Efficiency'], label=health_status) plt.title('Time-Series Trends of Discharge Efficiency by Battery Health Status') plt.xlabel('Date Time') plt.ylabel('Discharge Efficiency') plt.legend() plt.grid() plt.show()", "source": "Repo:NASA-Battery-project:Li ion.ipynb", "section": "NASA-Battery-project", "hash": "39466b6b"}
{"text": "# NASA-Battery-project ## Overview The dataset 'CS2_37_1_18_11.xlsx' contains information about a battery undergoing various testing procedures. It provides multiple parameters and measurements that can be utilized for understanding the battery's health, efficiency, and behavior during different steps and cycles of testing. ### General Dataset Information - The dataset comprises 9871 entries and 17 columns representing different battery parameters. - Various features such as Test_Time, Date_Time, Step_Time, Current, Voltage, Capacity, Energy, and Resistance, among others, are included. - Initial inspection indicates no missing values and diverse data types, primarily float and integer types. ## Exploratory Data Analysis ### Battery Health Metric Calculation - Data normalization and feature weighting were applied to derive a 'Battery Health Metric.' - A histogram is plotted to depict the distribution of the calculated metric. - Categorized battery health status into 'Unhealthy,' 'Intermediate,' and 'Healthy.' ### Battery Health Status Visualization - A pie chart visualizes the distribution of different battery health statuses. ### Step and Cycle-Based Aggregations - Calculated statistical measures (mean, median, and standard deviation) of Voltage and Current for both steps and cycles. - Graphically represented step-based and cycle-based aggregations, illustrating the trends across different steps and cycles. ### Discharge Efficiency Analysis - Discharge efficiency was computed from charge and discharge energy values. - A histogram was plotted to demonstrate the distribution of discharge efficiency. ### Time-Series Analysis of Discharge Efficiency - Utilized time-series analysis to observe the discharge efficiency trends concerning battery health status over time. - Plotted these trends to visualize how discharge efficiency fluctuates concerning the battery's health status. ## Insights and Observations - The battery health metric calculation allowed for better understanding of battery health status. - Aggregations provided insights into step and cycle-based trends of voltage and current. - Discharge efficiency analysis revealed a distribution, showing the variance in efficiency measurements. ## Conclusion The analysis provides a comprehensive understanding of the battery's health, efficiency, and behavior during testing procedures. The visualizations and calculations offer insights into the battery's performance, guiding further analysis or actions.", "source": "Repo:NASA-Battery-project:README.md", "section": "NASA-Battery-project", "hash": "5e9ba1d9"}
{"text": "Sleep Aid Tablets | Sleep Products | Boots Skip to navigation Skip to content Skip to search Message Dialog Maximum basket size reached Out of stock placeholder Display Update Message x Select your shipping destination Never be without your favourite Boots products with our international delivery options", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "4facece0"}
{"text": "Sleep Aid Tablets | Sleep Products | Boots Skip to navigation Skip to content Skip to search Message Dialog Maximum basket size reached Out of stock placeholder Display Update Message x Select your shipping destination Never be without your favourite Boots products with our international delivery options Austria Balearic Islands Belgium Bulgaria Canada Czech Republic Denmark Estonia Finland France Germany Greece Hungary Ireland Italy Latvia Lithuania Luxembourg Monaco Netherlands Norway Poland Portugal Romania Slovakia Slovenia Spain (Mainland) Sweden UK (including BFPO) USA Find out more about international delivery Country specific sites Boots has products available in other retail outlets in a number of countries, select from the country-specific sites below to find out more: Bahrain Hong Kong Kuwait Qatar Saudi Arabia Singapore Thailand United Arab Emirates No7 Better than 1/2 price gift sets Shop now CHANEL WONDERLAND Shop now Boots Health Hub Online access to health and wellness services Boots Free Online NHS Repeat Prescription Service Boots Health Hub Online access to health and wellness services Price Advantage Unlock exclusive offers with your Advantage Card Price Advantage Unlock exclusive offers with your Advantage Card No7 Better than 1/2 price gift sets Shop now Ship to Find a store Help Accessibility toolbar Mobile Burger Menu Icon Find a store Mobile search Menu My account Order history Favourites Log out Log in/register Sign up for Boots Advantage Card Boots Advantage Card worth of Advantage Card points Shop by department Shop by department christmas christmas visit christmas shop all christmas star gifts 3 for 2 mix & match advent calendars gifts for her gifts for her visit gifts for her gifts for him gifts for him visit gifts for him gifts for kids gifts for kids visit gifts for kids gift ideas gift ideas visit gift ideas stocking fillers stocking fillers visit stocking fillers secret santa 100 best christmas gifts 2023 vegan gifts Disney gifts health & pharmacy health & pharmacy visit health & pharmacy health offers health value packs & bundles health value packs & bundles visit health value packs & bundles medicines & treatments medicines & treatments visit medicines & treatments pharmacy medicines vitamins & supplements vitamins & supplements visit vitamins & supplements baby & child health baby & child health visit baby & child health women's health women's health visit women's health men's health men's health visit men's health lifestyle & wellbeing lifestyle & wellbeing visit lifestyle & wellbeing sexual pleasure & wellbeing sexual pleasure & wellbeing visit sexual pleasure & wellbeing incontinence incontinence visit incontinence electrical health & diagnostics electrical health & diagnostics visit electrical health & diagnostics mobility & daily living aids mobility & daily living aids visit mobility & daily living aids travel health travel health visit travel health new in health COVID-19 Information Products & Testing COVID-19 Information Products & Testing visit covid-19 information products & testing reusable & disposable face masks beauty & skincare beauty & skincare visit beauty & skincare No7 No7 visit no7 brands new in beauty & skincare skincare skincare visit skincare makeup makeup visit makeup premium beauty & skincare premium beauty & skincare visit premium beauty & skincare hair hair visit hair accessories accessories visit accessories black-founded & inclusive brands beauty minis virtual beauty beauty value packs & bundles vegan beauty vegan beauty visit vegan beauty beauty awards beauty awards visit beauty awards top 10 top 10 visit top 10 beauty boxes fragrance fragrance visit fragrance fragrance offers fragrance offers visit fragrance offers perfume perfume visit perfume aftershave aftershave visit aftershave fragrance gift sets fragrance gift sets visit fragrance gift sets luxury fragrance fragrance finder vegan fragrance new in fragrance recommended recommended visit recommended celebrity fragrance 5* rated perfumes & aftershaves brands home fragrance home fragrance visit home fragrance fragrance exclusives baby & child baby & child visit baby & child baby event Boots Parenting Club baby & child offers baby value packs & bundles travel travel visit travel nursery & bedding nursery & bedding visit nursery & bedding clothing clothing visit clothing feeding feeding visit feeding bathing & changing bathing & changing visit bathing & changing pregnancy & maternity pregnancy & maternity visit pregnancy & maternity baby & child health baby & child health visit baby & child health toys toys visit toys sustainable baby new in baby & child baby backpacks Nursery Advice Service electrical electrical visit electrical electrical offers all electrical hair styling tools hair styling tools visit hair styling tools electrical dental electrical dental visit electrical dental female hair removal tools female hair removal tools visit female hair removal tools male grooming tools male grooming tools visit male grooming tools beauty tools beauty tools visit beauty tools electrical wellbeing electrical wellbeing visit electrical wellbeing electrical health & diagnostics electrical health & diagnostics visit electrical health & diagnostics Boots Kitchen Appliances Boots Kitchen Appliances visit boots kitchen appliances audio & visual tech audio & visual tech visit audio & visual tech smart watches fans heaters & humidifiers home appliances & accessories home appliances & accessories visit home appliances & accessories new in electrical recycle your electricals sun & holiday sun & holiday visit sun & holiday holiday value packs & bundles suncare suncare visit suncare fake & gradual tan fake & gradual tan visit fake & gradual tan travel toiletries travel health travel health visit travel health travel accessories travel accessories visit travel accessories kids travel festival festival visit festival sunglasses sunglasses visit sunglasses travel insurance staycation essentials outdoor toys outdoor toys visit outdoor toys outdoor living love island love island visit love island wellness wellness visit wellness wellness offers new in wellness vegan vegan visit vegan immunity & protection vitamins & supplements vitamins & supplements visit vitamins & supplements sexual pleasure & wellbeing sexual pleasure & wellbeing visit sexual pleasure & wellbeing sleep sleep visit sleep everyday stress diet & weight management diet & weight management visit diet & weight management sports nutrition sports nutrition visit sports nutrition alternative therapies alternative therapies visit alternative therapies digestive health energy support food & drink food & drink visit food & drink sustainability at Boots sustainability at Boots visit sustainability at boots beauty supplements activity trackers recipe books & accessories all vegan products trending in wellness wellness inspiration toiletries toiletries visit toiletries new in toiletries toiletries offers travel toiletries toiletries value packs & bundles skincare skincare visit skincare hair hair visit hair suncare suncare visit suncare fake & gradual tan fake & gradual tan visit fake & gradual tan female hair removal female hair removal visit female hair removal dental dental visit dental luxury bath & body luxury bath & body visit luxury bath & body bathroom essentials bathroom essentials visit bathroom essentials men's toiletries men's toiletries visit men's toiletries period products period products visit period products deodorants & antiperspirants deodorants & antiperspirants visit deodorants & antiperspirants men's men's visit men's new in men's men's toiletries men's toiletries visit men's toiletries men's value packs & bundles shaving & grooming shaving & grooming visit shaving & grooming men's skincare & body male grooming tools male grooming tools visit male grooming tools male incontinence male incontinence visit male incontinence men's health men's health visit men's health aftershave aftershave visit aftershave men's gift sets opticians opticians visit opticians book an eye test opticians offers opticians offers visit opticians offers glasses frames glasses frames visit glasses frames glasses lenses glasses lenses visit glasses lenses contact lenses contact lenses visit contact lenses Boots optician sunglasses Boots optician sunglasses visit boots optician sunglasses hearingcare brands A-Z photo photo visit photo photo offers photo printing photo printing visit photo printing albums & frames albums & frames visit albums & frames audio & visual tech audio & visual tech visit audio & visual tech novelty photo gifts new in photo vegan vegan visit vegan all vegan products vegan food vegan beauty vegan beauty visit vegan beauty vegan vitamins vegan protein & supplements vegan fragrance vegan gifts gift gift visit gift christmas christmas visit christmas advent calendars all gifts gifts for her gifts for her visit gifts for her gifts for him gifts for him visit gifts for him Eid gifts home fragrance home fragrance visit home fragrance experience days experience days visit experience days luxury gifts personalised photo gifts birthday gifts gift cards gift type gift type visit gift type gift by recipient gift by recipient visit gift by recipient gift by occasion gift by occasion visit gift by occasion vegan gifts new in new in visit new in new in beauty & skincare new in fragrance new in premium beauty & skincare new in baby & child new in baby & kids clothes new in No7 new in hair new in wellness new in electrical new in health new in footcare new in luxury bath & body new in toiletries new in diet & weight management new in photo sale clearance brand A-Z Prescriptions Prescriptions NHS repeat prescriptions prescriptions prescriptions visit prescriptions prescription delivery service nominated pharmacy text message service late night pharmacy NHS electronic prescription registration support from your local pharmacist support with your medication prescription FAQs NHS prescriptions FRPS FAQ page NHS services NHS services visit nhs services NHS Blood Pressure Checking Service NHS New Medicine Service NHS Substance Dependency Service NHS Discharge Medicines Review (Wales) NHS Discharge Medicines Service (England) NHS Minor Ailment Scheme NHS Stop Smoking Service Practice Plus Prescription Stock Checker Health hub Health hub health hub health hub visit health hub A - Z health conditions A - Z health conditions visit a - z health conditions A - Z health services A - Z health services visit a - z health services men's health men's health visit men's health pain management pain management visit pain management sexual health sexual health visit sexual health digestive health advice digestive health advice visit digestive health advice skin conditions skin conditions visit skin conditions women's health women's health visit women's health living well living well visit living well travel health advice travel health advice visit travel health advice childrens health childrens health visit childrens health seasonal illnesses seasonal illnesses visit seasonal illnesses services Boots online doctor Boots online doctor visit boots online doctor mens health mens health visit mens health womens health womens health visit womens health general health general health visit general health acne & skin conditions acne & skin conditions visit acne & skin conditions sexual health sexual health visit sexual health testing services testing services visit testing services appointment booking Winter Flu Jab Service opticians opticians visit opticians book an eye test Complete Satisfaction Guarantee Opticians Glasses Cover opticians FAQs opticians store locator opticians terms & conditions hearingcare Boots for business Boots for business visit boots for business Corporate Flu Vaccination Service Corporate COVID-19 Lateral Flow Testing Service Corporate Hepatitis B Vaccination Service Corporate Pneumonia Vaccination Service Boots Care Services Corporate DTP Vaccination Service corporate travel health Hospital Outpatient Pharmacies Corporate Giftcards vaccinations & travel vaccinations & travel visit vaccinations & travel Winter Flu Jab Service Travel Vaccinations & Health Advice Service Chickenpox Vaccination Service HPV Vaccination Service Pneumonia Vaccination Service Shingles Vaccination Service MenB Vaccination Service MenB Vaccination Service visit menb vaccination service Malaria Prevention Service Travel Vaccination Quick Check Tool England COVID-19 spring booster vaccination service Northern Ireland Covid Vaccination Service macmillan & cancer support macmillan & cancer support visit macmillan & cancer support Boots Macmillan Beauty Advisors Boots Macmillan Information Pharmacist Virtual Boots Macmillan Information Pharmacist Support & Charity Fundraising Inspire Me Inspire Me Health & Beauty Christmas inspiration Christmas inspiration visit christmas inspiration gifts by recipient gifts by recipient visit gifts by recipient gifts by interest gifts by interest visit gifts by interest great value gifts great value gifts visit great value gifts hot right now hot right now visit hot right now health health visit health allergy & hayfever allergy & hayfever visit allergy & hayfever baby & child health baby & child health visit baby & child health cancer cancer visit cancer cold & flu cold & flu visit cold & flu coronavirus (COVID-19) coronavirus (COVID-19) visit coronavirus (covid-19) diabetes diabetes visit diabetes digestion & gut health digestion & gut health visit digestion & gut health eyes & vision eyes & vision visit eyes & vision fertility & conception fertility & conception visit fertility & conception first aid advice first aid advice visit first aid advice footcare footcare visit footcare general health general health visit general health hair loss hair loss visit hair loss men's health men's health visit men's health mental health mental health visit mental health oral health oral health visit oral health pain management pain management visit pain management sexual health sexual health visit sexual health smoking & cutting down smoking & cutting down visit smoking & cutting down travel advice travel advice visit travel advice vitamins & supplements vitamins & supplements visit vitamins & supplements women's health women's health visit women's health wellness wellness visit wellness healthy lifestyle healthy lifestyle visit healthy lifestyle life balance life balance visit life balance nutrition nutrition visit nutrition sleep sleep visit sleep smoking & cutting down smoking & cutting down visit smoking & cutting down women women visit women beauty & skincare beauty & skincare visit beauty & skincare beauty edits beauty edits visit beauty edits fake tan fake tan visit fake tan hair hair visit hair hair removal hair removal visit hair removal makeup makeup visit makeup nails nails visit nails skincare skincare visit skincare virtual beauty skincare diagnostic tool fragrance fragrance visit fragrance 8 of the best spring fragrances that will attract all the compliments felicity hayward: my life in fragrance fragrance hints & tips how to apply perfume: tips to make it last longer how to buy fragrance How to do perfume layering – and 10 of the best body lotions to give this Christmas scent profile: how to choose the right perfume for you and others the Boots guide to 10 perfect perfumes the Boots guide to the top 8 men’s aftershaves fragrance finder baby & child baby & child visit baby & child Boots Parenting Club baby baby visit baby buyer's guides buyer's guides visit buyer's guides new parent new parent visit new parent newborn newborn visit newborn pregnancy & maternity pregnancy & maternity visit pregnancy & maternity premature baby advice premature baby advice visit premature baby advice toddler toddler visit toddler electrical electrical visit electrical the Boots guide to the best electric beauty tools choosing your IPL system the Boots guide to the best hair dryers the Boots guide to the best hair stylers the Boots guide to the best electric toothbrushes the Boots guide to the best electric shavers opticians advice opticians advice visit opticians advice opticians coronavirus advice caring for your glasses children's eyes children's eyes visit children's eyes contact lenses contact lenses visit contact lenses eye health & conditions eye health & conditions visit eye health & conditions eye test information eye test information visit eye test information glasses lenses guide Varifocal Lenses explained Varilux Varifocal Lenses glasses style glasses style visit glasses style how to be more eco-friendly with your eyewear Klarna pay in 3 sunglasses sunglasses visit sunglasses sun & holiday sun & holiday visit sun & holiday summer beauty summer beauty visit summer beauty sun care sun care visit sun care gifting gifting visit gifting 5 perfume discovery sets that will transform the way you shop for fragrances 8 fragrances to transport you to sunnier climates 10 of the best gifts for lunar new year 10 of the best winter fragrances that will turn heads 10 of our best gifts for Diwali 10 great graduation gifts for her that she’s certain to love 12 brilliant bridesmaid gift ideas for all budgets 12 of the best baby shower gifts 13 thoughtful wedding day gifts for all budgets 14 of the best anniversary gifts you can find at Boots the Boots guide to the best birthday gifts Boots guide to the best gifts for dads who have everything Boots guide to the best graduation gifts Boots guide to the best perfumes for mums fall in love with these five Valentine’s Day picks Gift ideas for a chemo care hamper gift inspiration for mums guide to the best housewarming gifts the best Eid gifts you can get at Boots the best gift experiences for couples the best gifts for new parents: 2022 edition the best letterbox Valentine’s Day gifts for your BFF the Boots guide to the best engagement gifts The Boots guide to the best first Mother’s Day gifts the Boots guide to the best home fragrance gift ideas best photo gift ideas The Boots guide to the best Valentine’s Day gifts The Boots guide to thoughtful gifts for mums who have everything The Boots going to uni gift guide Boots Beauty Specialists recycle at Boots Offers Offers offers sale clearance savings savings visit savings haircare savings toiletries savings beauty savings electrical beauty savings fragrance savings No7 savings baby and child savings healthcare savings skincare savings value packs & bundles value packs & bundles visit value packs & bundles all value packs & bundles health value packs & bundles health value packs & bundles visit health value packs & bundles toiletries value packs & bundles baby value packs & bundles hair value packs and bundles hair value packs and bundles visit hair value packs and bundles beauty value packs & bundles premium value packs & bundles electrical value packs & bundles men's value packs & bundles holiday value packs & bundles bigger packs and sizes £10 tuesday great new price fragrance offers fragrance offers visit fragrance offers save £15 save up to half price free gifts everyday low prices clearance electrical offers skincare offers makeup offers seasonal events seasonal events visit seasonal events back to school back to school visit back to school christmas christmas visit christmas black friday mother's day mother's day visit mother's day father's day father's day visit father's day valentine's day valentine's day visit valentine's day halloween halloween visit halloween cyber monday Children in Need Eid gifts health offers baby & child offers toiletries offers photo offers opticians offers opticians offers visit opticians offers Klarna pay in 3 free contact lens trial great value glasses NHS eye care offers for students offers for over 60s No7 No7 No7 No7 visit no7 gifts gifts visit gifts age-defying day cream age defying serums future renew shop all no7 bestsellers new in No7 skincare skincare visit skincare makeup makeup visit makeup better than half price collections mens mens visit mens suncare & fake tan bath & body tools value sets advice advice visit advice no7 clearance shop all gifts gifts visit gifts No7 christmas gifts No7 Beauty Calendars no7 bestsellers new in No7 skincare skincare visit skincare No7 Beauty Calendars derm solutions derm solutions visit derm solutions no7 personalised skin analysis no7 personalised skin analysis visit no7 personalised skin analysis future renew protect & perfect lift & luminate restore & renew laboratories retinol range hydraluminous early defence anti-ageing serums anti-ageing skincare Radiance+ advanced ingredients moisturisers cleansers & toners face masks makeup makeup visit makeup shop all makeup Stay Perfect foundation foundation analysis face eyes lips nails mascaras brows mens mens visit mens No7 Men's Energising bath & body suncare & fake tan tools value sets advice advice visit advice shop the stellar dreams look shop the ethereal radiance look shop the star glazing lip look our mission find your perfect no7 skincare regime find your perfect no7 skincare regime visit find your perfect no7 skincare regime no7 foundation analysis find your perfect No7 serum discover No7 Advanced Retinol discover No7 Advanced Retinol visit discover no7 advanced retinol no7 waiting list discover no7 laboratories discover no7 line correcting booster serum discover no7 advanced ingredients capsules no7 beauty how to videos no7 beauty how to videos visit no7 beauty how to videos no7 x Macmillan no7 x Macmillan visit no7 x macmillan no7 beauty appointments no7 beauty appointments visit no7 beauty appointments no7 personalised skin analysis no7 personalised skin analysis visit no7 personalised skin analysis better than half price collections no7 clearance Search Search Search Search Suggested keywords Matching products View all products ( 0 ) Core UI/2", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "a744040f"}
{"text": "visit no7 x macmillan no7 beauty appointments no7 beauty appointments visit no7 beauty appointments no7 personalised skin analysis no7 personalised skin analysis visit no7 personalised skin analysis better than half price collections no7 clearance Search Search Search Search Suggested keywords Matching products View all products ( 0 ) Core UI/2 Atoms/Icons/24px/Navigation & Action/OK Item added to your basket Total (0 items) Excludes delivery VIEW BASKET CHECKOUT NOW FREE click & collect for orders over £15 FREE standard delivery orders over £25 Next day delivery check for availability Airport click & collect from selected airports Delivered on demand by Deliveroo Home health & pharmacy medicines & treatments sleep Categories bedding bedtime routine pillow spray sleep supplements snoring solutions Filter by: Hide out of stock items Price range £2 £220 Min to Max APPLY Brand Silentnight (76) Dreamland (42) Kally Sleep (20) Boots (16) This Works (12) Feather & Down (10) Bach Rescue Remedy (8) Tisserand (8) Snoreeze (7) Alpine (5) View all Rating Only (29) & Up (96) & Up (127) & Up (140) & Up (143) Promotions 3 for 2 on selected Vitamins and Supplements - cheapest free (48) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last (15) Save 10 percent on selected Boots Brand with your Advantage Card (14) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last (12) Receive £10 worth of points for every £60 spent across selected Electrical Beauty (12) Save 1/3 on selected Bach Rescue Remedy - online only (6) Save up to £20 on selected Dreamland - online only (6) 3 for 2 on selected Boots earplugs - cheapest free (5) 3 for 2 on selected Feather and Down - cheapest free (5) 3 for 2 on selected Soap and Glory - cheapest free (4) View all Product type Sleep aid (53) Electric blanket (33) Pillows (29) Blanket (27) Heated throw (22) Mattress topper (18) Duvet (17) Mattress protector (14) Earplugs (13) Pillow spray (11) View all Gender Unisex (7) Womens (5) Size 10 (2) 200 (1) Double (20) King (15) Single (15) 11-20 (13) Large (12) 0-10 (11) 21-30 (9) 71-100 (9) View all Gift type Electrical beauty gifts (4) Sleep gifts (3) Candles & diffusers (2) Home & lifestyle gifts (2) Bath & body gifts (1) Fitness gifts (1) Stocking fillers (1) Recipient Gifts for women (5) Gifts for men (4) Gifts for sport enthusiasts (4) Gifts for gadget lovers (2) Secret santa (1) Skin type All skin types (13) Key features Relaxing (7) Cruelty-free (5) Dermatologically tested (5) Multi sport tracking (4) Sleep tracking (4) Guided breathing sessions (3) Heart rate monitor (3) Waterproof (3) Anti-bacterial (2) Cardio fitness tracking (2) View all Body area Body (12) Eye (1) Wrist (1) Format Spray (23) Tablet (12) Oil (10) Liquid (8) Strip (5) Capsule (4) Foam (3) Mist (3) Rollerball (3) Bag (2) View all Suitable from 3 months (3) 6 years (1) Colour White (29) Grey (4) Free from Silicone-free (6) Alcohol-free (2) Aluminium-free (2) Paraben-free (2) Sulphate-free (2) Talc-free (2) Artificial colours (1) Artificial flavours (1) Artificial preservatives (1) Gelatine-free (1) View all Fragrance scent Aromatic (1) Suitable for Adults (31) Vegan (29) Vegetarian (12) Baby (2) Kids (2) Pregnancy (1) Teens (1) Condition Sleeping problems (14) Snoring (13) Mild anxiety (2) Stress (1) Ingredients Lavender (22) Sandalwood (11) Jasmine (7) Chamomile (6) Vetiver (6) Magnesium (3) CBD (2) Aloe vera (1) Retinol (1) Shea butter (1) View all Active ingredient Valerian root (8) Diphenhydramine (4) Hops strobile (2) Passion flower (2) Rhodiola rosea root extract (1) Flavour Mint (2) Apple (1) Raspberry (1) Strawberry (1) Vanilla (1) Pharmacy medicine Yes (4) Pack size 0–10 (12) 11–20 (4) 200+ (1) 21–30 (1) 41–50 (1) 51–70 (1) 71–100 (1) sleep 3 for 2* on Healthspan CBD Oil", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "11c3c24e"}
{"text": "Hops strobile (2) Passion flower (2) Rhodiola rosea root extract (1) Flavour Mint (2) Apple (1) Raspberry (1) Strawberry (1) Vanilla (1) Pharmacy medicine Yes (4) Pack size 0–10 (12) 11–20 (4) 200+ (1) 21–30 (1) 41–50 (1) 51–70 (1) 71–100 (1) sleep 3 for 2* on Healthspan CBD Oil Our most popular CBD oil droppers**, ideal for day or night SHOP NOW *3 for 2 on selected vitamins, supplements health foods and complementary medicines Cheapest free Subject to availability **To verify, please contact customercare@healthspan co uk Showing 270 of 270 Sort: Most relevant none Boots Sleepeaze Tablets 50 mg - 20s Save 10 percent on selected Boots Pharmacy Medicines - Advantage Card Holders only £5 79 20UNI | £0 29 per 1UNI View product Offer none Boots Sleepeaze Herbal Plus Tablets - 30 Tablets (12) 3 for 2 on selected Vitamins and Supplements - cheapest free £5 50 30UNI | £0 18 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 870232, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Boots Sleepeaze Tablets 25 mg - 20s Save 10 percent on selected Boots Pharmacy Medicines - Advantage Card Holders only £3 90 20UNI | £0 20 per 1UNI View product Offer none Feather and Down Perfect Partners (4) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £7 50 Save £2 50 Was £10 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2132548, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Botanics Peaceful Night Sleep Duo Gift Set (3) £8 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2617121, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Kalms Night One-a-Night - 28 Tablets (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £9 99 28UNI | £0 36 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2885611, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Tisserand The Little Box of Sleep 3 x 10ml (9) 3 for 2 on selected Vitamins and Supplements - cheapest free £13 50 30ML | £45 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2135937, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "3e7cf278"}
{"text": "and Supplements - cheapest free £13 50 30ML | £45 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2135937, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Bach Rescue Remedy Night Liquid Melts 28 Capsules (12) Save 1/3 on selected Bach Rescue Remedy - online only £7 00 Save £3 50 Was £10 50 28UNI | £0 25 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1045962, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Nytol One-A-Night 50mg Tablets - 20 Tablets £8 60 20UNI | £0 43 per 1UNI View product Offer none Nytol Herbal Simply Sleep One-A-Night Tablets (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £7 35 21UNI | £0 35 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1983950, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Kalms Night - 56 Tablets (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £5 99 56UNI | £0 11 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2885609, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Feather & Down Sweet Dreams Sleeping Bag Gift Set (5) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £14 25 Save £4 75 Was £19 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1969543, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Kira Restful Sleep Valerian Root Extract 300 mg Tablets - 25 Tablets (2) 3 for 2 on selected Vitamins and Supplements - cheapest free £6 90 25UNI | £0 28 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1240452, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Boots Sleepeaze Lavender Pillow Mist 100ml (8) 3 for 2 on selected Vitamins and Supplements - cheapest free £6 00 100ML | £6 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "889d435a"}
{"text": "jpg\" } ] Offer none Boots Sleepeaze Lavender Pillow Mist 100ml (8) 3 for 2 on selected Vitamins and Supplements - cheapest free £6 00 100ML | £6 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2796123, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Rescue Remedy Gummies Night 60s (7) £14 00 60UNI | £0 23 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2756094, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Botanics Pillow Mist Lavender & Sweet Marjoram 100ml (128) £7 50 100ML | £7 50 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2268433, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Boots Sleepeaze Snoring Relief Oral Strips - 21 Strips 3 for 2 on selected Vitamins and Supplements - cheapest free £7 50 21UNI | £0 36 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 38895, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Bach Rescue Night Dropper 20ml (9) Save 1/3 on selected Bach Rescue Remedy - online only £8 33 Save £4 17 Was £12 50 20ML | £41 65 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2461419, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Champneys Sleep Pillow Mist 50ml (11) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £12 00 50ML | £24 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2565919, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Intelliheat Warming Throw - Grey (214) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £60 00 Save £9 99 Was £69 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2811605, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "87773653"}
{"text": "Beauty £60 00 Save £9 99 Was £69 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2811605, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Intelliheat Faux Fur Warming Throw - Alaskan Husky (147) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £99 99 Save £30 00 Was £129 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2796103, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Boots Soft Silicone Earplugs - 3 Pairs (94) 3 for 2 on selected Boots earplugs - cheapest free £3 96 Save £0 99 Was £4 95 3UNI | £1 32 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1121442, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Boots Mouldable Wax Earplugs - 5 Pairs (121) 3 for 2 on selected Boots earplugs - cheapest free £2 16 Save £0 54 Was £2 70 5UNI | £0 43 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1129527, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Soap & Glory Perfect Zen Calming Bath Milk 500ml (56) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £6 99 500ML | £1 40 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2613783, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none This Works Deep Sleep™ Pillow Spray 75ml (156) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £21 00 75G | £28 00 per 100G Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1242286, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Intelliheat Throw - Teal (95) Save up to £20 on selected Dreamland - online only £59 99 Save £10 00 Was £69 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "e93f16bf"}
{"text": ": \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Intelliheat Throw - Teal (95) Save up to £20 on selected Dreamland - online only £59 99 Save £10 00 Was £69 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2833117, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Lumie Sunrise Alarm (35) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £49 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2187945, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Soap & Glory Perfect Zen Foaming Shower Oil 200ml (62) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £8 99 200ML | £4 50 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2604753, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Baylis & Harding Goodness Sleep Lavender & Bergamot Sleep Bath Soak 500ml (5) £5 00 500ML | £1 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2636603, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Boots Sleepeaze Snoring Relief Throat Spray - 14ml 3 for 2 on selected Vitamins and Supplements - cheapest free £5 50 14ML | £39 29 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1520654, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Herringbone Throw - Emerald Green (32) Save up to £20 on selected Dreamland - online only £74 99 Save £20 00 Was £94 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2833123, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none This Works Deep Sleep™ Pillow Spray 250ml (14) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £37 00 250ML | £14 80 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2005923, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "d6c2f22a"}
{"text": "selected This Works - online only, whilst stocks last £37 00 250ML | £14 80 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2005923, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Soap & Glory Perfect Zen Body Souffle 300ml (51) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £9 99 300ML | £3 33 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2604755, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Hurry Home Warming Throw - Grey 160x120 (35) Save up to £20 on selected Dreamland - online only £74 99 Save £20 00 Was £94 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2833119, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Soap & Glory Perfect Zen Warming Body Scrub 250ml (48) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £8 99 250ML | £3 60 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2604757, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Snoreeze Snoring Relief Oral Strips - 14 Applications 3 for 2 on selected Vitamins and Supplements - cheapest free £8 00 14UNI | £0 57 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 21582, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Snugsie Giant Blanket Neutral (0) £32 99 View product Offer none Fitbit Luxe Platinum/ Orchid Receive £10 worth of points for every £60 spent across selected Electrical Beauty £99 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2600882, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Baylis & Harding Goodness Sleep Lavender & Bergamot Sleep Pillow Mist 100ml (4) £7 00 100ML | £7 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2636607, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "741f4b48"}
{"text": "Sleep Pillow Mist 100ml (4) £7 00 100ML | £7 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2636607, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Feather & Down Sweet Dreams Sleep Essentials Set (4) 3 for 2 on selected Vitamins and Supplements - cheapest free £14 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1969545, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Boots Sleepeaze Snoring Relief Throat Spray - 42ml 3 for 2 on selected Vitamins and Supplements - cheapest free £14 50 42ML | £34 52 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1364135, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Soap & Glory The Rest Assured Sleep Mask (7) Buy 1 get 2nd 1/2 price on selected Soap And Glory £7 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2102967, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Botanics Pure Essential Oil Lavender 10ml (78) £6 50 10ML | £65 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2268421, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Naptime Warming Sherpa Blanket - Tartan Check Red Grey 180X135 (31) Save up to £20 on selected Dreamland - online only £89 99 Save £20 00 Was £109 99 View product Offer none Feather & Down Sweet Dreams Melting Shower Cream 250ml (41) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £7 00 250ML | £2 80 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1970456, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Boots Sleepeaze Snoring Relief Congestion Nasal Strips - 20 Strips (40) 3 for 2 on selected Vitamins and Supplements - cheapest free £9 50 20UNI | £0 48 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "1ed6287e"}
{"text": "none Boots Sleepeaze Snoring Relief Congestion Nasal Strips - 20 Strips (40) 3 for 2 on selected Vitamins and Supplements - cheapest free £9 50 20UNI | £0 48 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1219239, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Revive Me Multipurpose Heat Pad Standard Size (42) £39 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2753839, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Hygge Days Faux Fur Warming Throw - Fallow Deer (37) Only £99 99 on selected Dreamland £99 99 Save £30 00 Was £129 99 View product Offer none Snoreeze Snoring Relief Oral Device (42) 3 for 2 on selected Vitamins and Supplements - cheapest free £37 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1894648, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Feather & Down Pillow Spray 200ml (6) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £14 00 200ML | £7 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2609162, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Nytol Herbal Tablets 30 tablets (11) 3 for 2 on selected Vitamins and Supplements - cheapest free £6 20 30UNI | £0 21 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 922798, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Boots Sleepeaze Snoring Relief Congestion Nasal Spray - 10ml 3 for 2 on selected Vitamins and Supplements - cheapest free £11 00 10ML | £110 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2359564, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Snoreeze Lozenges - 16 lozenges 3 for 2 on selected Vitamins and Supplements - cheapest free £9 00 16UNI | £0 56 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "731e8461"}
{"text": ": \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Snoreeze Lozenges - 16 lozenges 3 for 2 on selected Vitamins and Supplements - cheapest free £9 00 16UNI | £0 56 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1272401, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Herringbone Throw - Navy Blue (36) Save up to £20 on selected Dreamland - online only £74 99 Save £20 00 Was £94 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2833121, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none BioEars Soft Silicone Earplugs with Activ Aloe - 3 pairs (53) £5 55 3UNI | £1 85 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 39425, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Feather & Down Soothing Sleep Butter 300ml (2) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £10 00 300ML | £3 33 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2416583, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Westlab Sleep Epsom Bath Salts with Lavender 1kg (7) £7 49 1000G | £0 75 per 100G Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2294028, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Botanics Pure Essential Oil Lavender 20ml (32) £11 00 20ML | £55 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2268423, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Alpine Sleepdeep Sleeping Earplugs 1 Pair (24) £12 95 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2767727, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none This Works Deep Sleep™ Bath Soak 200g (4) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £24 00 200G | £12", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "58caeb37"}
{"text": "}, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none This Works Deep Sleep™ Bath Soak 200g (4) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £24 00 200G | £12 00 per 100G Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1242288, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Bach Rescue Remedy Night Spray 20ml – Flower Essences for Natural Night's Sleep (7) Save 1/3 on selected Bach Rescue Remedy - online only £8 40 Save £4 20 Was £12 60 20ML | £42 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1983958, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Mute Medium - 3 pack (30 Night Supply) (16) 3 for 2 on selected Vitamins and Supplements - cheapest free £19 00 3UNI | £6 33 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1758171, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Rescue Balance & Positivity Capsules 30s (49) Save 1/3 on selected Bach Rescue Remedy - online only £13 97 Save £6 98 Was £20 95 30UNI | £0 47 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2585146, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Bach Rescue Peaceful Night Capsules 30s (68) Save 1/3 on selected Bach Rescue Remedy - online only £10 63 Save £5 32 Was £15 95 30UNI | £0 35 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2585148, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Snoreeze Snoring Relief Nasal Strips Small/Medium - 20 Applications 3 for 2 on selected Vitamins and Supplements - cheapest free £10 00 20UNI | £0 50 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 870191, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "42d5b4e0"}
{"text": "free £10 00 20UNI | £0 50 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 870191, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Snoreeze Snoring Relief Nasal Spray 10ml 3 for 2 on selected Vitamins and Supplements - cheapest free £13 00 10ML | £130 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 33372, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Snugsie Wearable Blanket Silver (0) £18 99 View product Offer none This Works Sleep Plus Pillow Spray 30ml (0) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £19 00 30ML | £63 33 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2597091, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Geltex Pillow (1) £30 00 View colours none Dreamland Snuggle Up Warming Throw Pink 120X160Cm (27) £69 99 View product Offer none Tisserand Aromatherapy Sleep Better Roller Ball - 10ml (14) 3 for 2 on selected Vitamins and Supplements - cheapest free £8 00 10ML | £80 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 870243, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Baylis & Harding Goodness Sleep Lavender & Bergamot Sleep Body Wash 500ml (7) £5 00 500ML | £1 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2636605, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none BetterYou Magnesium Sleep Body Lotion - 180ml (9) 3 for 2 on selected Vitamins and Supplements - cheapest free £9 99 180ML | £5 55 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2248929, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none This Works Deep Sleep Pillow Spray 35ml (4) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £14 00 35ML | £40", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "447755bf"}
{"text": "}, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none This Works Deep Sleep Pillow Spray 35ml (4) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £14 00 35ML | £40 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2561040, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Nytol Original Tablets 25mg - 20 Tablets £6 30 20UNI | £0 32 per 1UNI View product none Alpine Sleepdeep Mini Sleeping Earplugs 1 Pair (13) £12 95 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2767689, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Snugsie Giant Blanket Green (0) £32 99 View product Offer none Dreamland Silent Power Pure Air Fan Heater (3) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £119 99 View product none Silentnight Snugsie Wearable Blanket Blush (0) £18 99 View product Offer none BetterYou Magnesium Sleep Flakes - 750g (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £6 99 750G | £0 93 per 100G Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2894607, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Aroma Active Laboratories Sleep Salt Soak 500g (11) Save 1/2 price across the Aroma Active range - online only £7 50 Save £7 50 Was £15 00 500G | £1 50 per 100G Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2465989, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Aroma Active Laboratories Sleep Over Night Recovery Face Oil 30ml (4) Save 1/2 price across the Aroma Active range - online only £7 50 Save £7 50 Was £15 00 30ML | £25 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2465991, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Bach Rescue Remedy Night Dropper 10ml - Flower Essences for Natural Night's Sleep (4) Save 1/3 on selected Bach Rescue Remedy - online only £6 33 Save £3 17 Was £9 50 10ML | £63", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "35074349"}
{"text": ": \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Bach Rescue Remedy Night Dropper 10ml - Flower Essences for Natural Night's Sleep (4) Save 1/3 on selected Bach Rescue Remedy - online only £6 33 Save £3 17 Was £9 50 10ML | £63 30 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 122323, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Feather & Down Sweet Dreams Bath Essence 500ml (9) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £8 00 500ML | £1 60 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1970454, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Nytol Herbal Simply Sleep & Calm Elixir - 100ml (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £7 90 100ML | £7 90 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1655165, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Revive Me Back Heat Pad 5T 61X38 Cm (1) £59 99 View product Offer none Dreamland Hygge Days Faux Fur Warming Throw - Zebra (19) Only £99 99 on selected Dreamland £99 99 Save £30 00 Was £129 99 View product Offer none This Works Deep Sleep™ Heavenly Candle (3) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £26 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2368973, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Tisserand Aromatherapy Sleep Better Pillow Mist (7) 3 for 2 on selected Vitamins and Supplements - cheapest free £13 00 100ML | £13 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1927862, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Snoreeze Snoring Relief Nasal Strips Large - 20 Applications (16) 3 for 2 on selected Vitamins and Supplements - cheapest free £10 00 20UNI | £0 50 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "67c06ad7"}
{"text": "Offer none Snoreeze Snoring Relief Nasal Strips Large - 20 Applications (16) 3 for 2 on selected Vitamins and Supplements - cheapest free £10 00 20UNI | £0 50 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 923897, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Snugsie Wearable Blanket Charcoal (0) £18 99 View product none Silentnight Waterproof Mattress Protector King (0) £19 99 View product Offer none This Works Sleep Plus Pillow Spray 100ml (3) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £35 00 100ML | £35 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2645840, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Snowed In Organic Cotton Warming Mattress Protector King 2 Controls 200X150Cm (19) £149 99 View product none Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer King 2 Controls 200X150Cm (54) £124 99 View product none Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Double 1 Control 190X137Cm (45) £99 99 View product Offer none Aroma Active Laboratories Sleep Pulsepoint Rollerball 6ml (2) Save 1/2 price across the Aroma Active range - online only £4 00 Save £4 00 Was £8 00 6ML | £66 67 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2465987, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight V Shaped Support Pillow (0) £17 00 View colours none Silentnight Serenity Weighted Eye Mask (0) £17 99 View colours Offer none Kalms Rhodiola Tablets- 20 Tablets (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £10 50 20UNI | £0 53 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2545126, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Tisserand Aromatherapy Sleep Better Bath Oil 200ml (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £16 00 200ML | £8 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2224959, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Snuggle Up Warming Throw - Mustard 120X160Cm (39) £69", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "74f731b8"}
{"text": "basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2224959, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Snuggle Up Warming Throw - Mustard 120X160Cm (39) £69 99 View product Offer none Dreamland Silent Power Eco Fan Heater (2) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £94 99 View product none Dreamland Peaceful Dreams Warming Over Blanket Double Dual 6T 180X180 (16) £114 99 View product none Alpine Sleepdeep Multi Size Sleeping Earplugs 2 Pairs (11) £17 95 2UNI | £8 98 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2767691, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Impress Memory Foam Pillow (0) £29 99 View product Group 10 none Healthspan Night Time CBD Oil 260mg 10ml £18 95 with Advantage Card £19 95 10ML | £199 50 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2564532, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Silent Power Comfort (2) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £69 99 View product none Dreamland Peaceful Dreams Warming Over Blanket Single 6T 180X135 Cm (14) £84 99 View product none Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Double 2 Controls 190X137Cm (39) £114 99 View product none Dreamland Peaceful Dreams Warming Over Blanket King Dual 6T 215X225 (19) £124 99 View product none Dreamland Indulgent Days Soft Velvet Warming Throw Geometric Peacock 120 X 160 Cm (10) £79 99 View product Offer none Fitbit Versa 3 Black (7) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £169 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2467425, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Snugsie Kids Glow In The Dark Hoody Denim (0) £22 99 View product none Silentnight Snugsie Oversized Hoody Charocal (0) £27 99 View product none Silentnight Snugsie Wearable Blanket Sage (0) £18 99 View product none Sealy Side Sleeper Pillow (1) £25 00 View colours none Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Superking 2 Controls 200X180Cm (34) £139 99 View product none Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Single 190X90Cm (31) £79", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "9a7058d3"}
{"text": "(0) £18 99 View product none Sealy Side Sleeper Pillow (1) £25 00 View colours none Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Superking 2 Controls 200X180Cm (34) £139 99 View product none Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Single 190X90Cm (31) £79 99 View product Offer none This Works Deep Sleep™ Night Oil 120ml (68) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £26 00 120ML | £21 67 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1279205, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Feather & Down Breathe Well Pillow Spray (3) 3 for 2 on selected Vitamins and Supplements - cheapest free £8 00 100ML | £8 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2133035, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Aromatherapy Associates Deep Relax Roller Ball 10ml (79) 3 for 2 on selected Vitamins and Supplements - cheapest free £25 00 10ML | £250 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2308469, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Snowed In Organic Cotton Warming Mattress Protector Double 1 Control 190X137Cm (9) £124 99 View product Offer none Aromatherapy Associates Deep Relax Sleep Mist 50ml (80) 3 for 2 on selected Vitamins and Supplements - cheapest free £32 00 50ML | £64 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2407515, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Klearvol Pillow Spray 100ml (2) £9 99 100ML | £9 99 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2710091, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Klearvol Vapour Rub 50ml (4) £4 99 50ML | £9 98 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2710093, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "0b5fa191"}
{"text": "(4) £4 99 50ML | £9 98 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2710093, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Cosy Dreamer Superior Cotton Mattress Warmer Single 150X80 Cm (2) £69 99 View product none Silentnight Supersoft Thermal Mattress Cover Double (0) £19 99 View product none Silentnight Heat Genie Self Heating Mattress Topper King (0) £39 99 View product none Silentnight Heat Genie Self Heating Mattress Topper Single (0) £28 99 View product none Silentnight Snugsie Oversized Hoody Charocal Glitter (0) £27 99 View product none Silentnight Deep Sleep Duvet 13 5 Tog Double (0) £34 99 View product none Klearvol Vapour Rub & Pillow Spray Bundle (0) £13 48 150ML | Save £1 50 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2912593, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none NytEase Stress + Tension Support Pillow Spray - 100ml (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £7 33 Save £3 66 Was £10 99 100ML | £7 33 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2887679, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Serenity Sweet Dreams Gift Set - Eye Mask, Pillow Mist and Ear Plugs Set (0) £14 99 View colours none Alpine Soft Silicone Earplugs 3 Pairs (0) £8 99 3UNI | £3 00 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2767693, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none This Works Sleep Plus Massage Relief 10ml (0) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £15 00 10ML | £150 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2605605, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Cowshed Sleepy Bath Salts 300g (0) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £24 00 300G | £8 00 per 100G Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "5d6c7e99"}
{"text": "] Offer none Cowshed Sleepy Bath Salts 300g (0) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £24 00 300G | £8 00 per 100G Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2408995, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Naptime Warming Sherpa Blanket Beige Tartan Check 180X135 Cm (3) £109 99 View product none Dreamland Snowed In Organic Cotton Warming Mattress Protector Single 190X90Cm (4) £99 99 View product none Silentnight Squishy Body Support Pillow (1) £28 00 View colours none Silentnight Wellbeing Collection Cool Touch Pillow (1) £30 00 View colours none Kally Sleep Body Pillow - Pure White (1) £59 99 View product none Kally Sleep Acid Reflux Wedge Pillow (2) £69 99 View product none Headspace Mind Giftcard - 6 months Pre-Paid Membership (1) £30 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2434462, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Cowshed Sleepy Calming Pillow Mist 100ml (4) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £18 00 100ML | £18 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2408993, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Tisserand Aromatherapy Dream Bath Sleep Better Bathtime Collection (1) 3 for 2 on selected Vitamins and Supplements - cheapest free £15 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2327939, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Snowed In Organic Cotton Warming Mattress Protector Superking 2 Controls 200X180Cm (13) £169 99 View product none Dreamland Snuggle Up Warming Throw - Navy 120X160Cm (32) £69 99 View product Offer none Aromatherapy Associates Relax Body Oil 100ml (49) 3 for 2 on selected Vitamins and Supplements - cheapest free £52 00 100ML | £52 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2308479, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Snuggle Up Warming Throw - Black 120X160Cm (6) £69", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "ff63308b"}
{"text": "basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2308479, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Snuggle Up Warming Throw - Black 120X160Cm (6) £69 99 View product Offer none This Works Sleep Plus Vegan Pillow Spray 10ml (18) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £12 00 10ML | £120 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2605603, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Heat Genie Self Heating Mattress Topper Super King (0) £44 99 View product none Silentnight Deep Sleep Duvet 13 5 Tog King (0) £39 99 View product none Silentnight Anti Allergy Duvet - 7 5 Tog - Double (0) £25 00 View colours none Silentnight Anti Allergy Duvet - 7 5 Tog - Single (0) £23 00 View colours none Silentnight Wellbeing Collection Re-balance Pillow Pair (0) £23 00 2UNI | £11 50 per 1UNI View colours none Kally Sleep Honeycomb Cooling Pillow (0) £54 99 View product none Twinings Superblends Unwind Tea Bags 20s (0) £2 99 20UNI | £0 15 per 1UNI Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2747664, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Headspace Sleep Giftcard - 6 months Pre-Paid Membership (0) £30 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2434464, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Dreamland Silent Power Protection Fan Heater (1) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £94 99 View product none Kally Sleep Ultimate Side Sleeper Pillow (1) £49 99 View product none Dreamland Snowed In Organic Cotton Warming Mattress Protector Double 2 Controls 190X137Cm (7) £139 99 View product none Dreamland Hurry Home Warming Throw - Mustard 160X120 (29) £94 99 View product Offer none Feather & Down Sweet Dreams Sleep Butter 300ml (16) 3 for 2 on selected Vitamins and Supplements - cheapest free £10 00 300ML | £3 33 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1969895, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Alpine Sleepsoft Earplugs 1 Pair (2) £11", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "149d2d8c"}
{"text": "reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 1969895, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Alpine Sleepsoft Earplugs 1 Pair (2) £11 95 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2446419, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Aromatherapy Associates Deep Relax Sleep Mist 10ml (2) 3 for 2 on selected Vitamins and Supplements - cheapest free £25 00 10ML | £250 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2431439, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Tisserand Aromatherapy Sleep Better Candle (5) 3 for 2 on selected Vitamins and Supplements - cheapest free £20 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2327953, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Dreamland Cosy Up Silky Soft Faux Fur Warming Throw Pink 160X120 Cm (2) £109 99 View product Offer none Fitbit Sense Lunar White (5) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £219 99 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2467421, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Luxury Anti-Snore Pillow (1) £22 00 View colours Offer none Aromatherapy Associates 3 Step Introduction to Sleep (1) 3 for 2 on selected Vitamins and Supplements - cheapest free £45 00 Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2564534, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] none Silentnight Heat Genie Self Heating Mattress Topper Double (0) £37 99 View product none Silentnight Snugsie Kids Glow In The Dark Hoody Blush (0) £22 99 View product none Silentnight Snugsie Oversized Hoody Blush (0) £27 99 View product none Silentnight Sealy Dual Comfort Pillow (0) £40 99 View product none Silentnight Sealy Airflow Pillow (0) £37 99 View product none Silentnight Airmax Mattress Topper 8cm Super King (0) £64 99 View product none Silentnight Airmax Mattress Topper 8cm King (0) £59 99 View product none Silentnight Airmax Mattress Topper 8cm Double (0) £49", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "3b618472"}
{"text": "Pillow (0) £40 99 View product none Silentnight Sealy Airflow Pillow (0) £37 99 View product none Silentnight Airmax Mattress Topper 8cm Super King (0) £64 99 View product none Silentnight Airmax Mattress Topper 8cm King (0) £59 99 View product none Silentnight Airmax Mattress Topper 8cm Double (0) £49 99 View product none Silentnight Airmax Mattress Topper 8cm Single (0) £42 99 View product none Silentnight Airmax Support Pillow Pair (0) £29 99 2UNI | £15 00 per 1UNI View product none Silentnightt Impress Memory Foam Mattress Topper Super King (0) £149 99 View product none Silentnight Impress Memory Foam Mattress Topper King (0) £139 99 View product none Silentnight Impress Memory Foam Mattress Topper Double (0) £129 99 View product none Silentnight Impress Memory Foam Mattress Topper Small Double (0) £124 99 View product none Silentnight Impress Memory Foam Mattress Topper Single (0) £109 99 View product none Silentnight Deep Sleep Duvet 13 5 Tog Super King (0) £44 99 View product none Silentnight Deep Sleep Duvet 13 5 Tog Single (0) £26 99 View product none Silentnight Warm & Cosy Pillow Pair (0) £17 99 View product none Silentnight Warm & Cosy 15 Tog Duvet Super King (0) £49 99 View product none Silentnight Healthy Growth Waterproof Mattress Protector Double (0) £17 99 View product none Silentnight Healthy Growth Waterproof Mattress Protector Single (0) £14 99 View product none Carmen Kingsize Heated Under Blanket (0) £55 99 View product none Dreamland Cosy Dreamer Superior Mattress Warmer King Dual 150x160 CM (0) £119 99 View product none Dreamland Cosy Dreamer Superior Cotton Mattress Warmer Double Dual 150X137 Cm (0) £99 99 View product none Dreamland Cosy Up Silky Soft Faux Fur Warming Throw Terracotta 160 X 120 Cm (0) £109 99 View product none Silentnight Summer Breeze Duvet - 2 5 Tog - King (0) £24 00 View colours none Silentnight Summer Breeze Duvet - 2 5 Tog - Double (0) £21 00 View colours none Silentnight Summer Breeze Duvet - 2 5 Tog - Single (0) £18 00 View colours none Silentnight Cooler Summer Pillow - 2 Pack (0) £18 00 2UNI | £9 00 per 1UNI View colours none Silentnight Anti Allergy Mattress Protector - King (0) £19 00 View colours none Silentnight Anti Allergy Mattress Protector - Double (0) £18 00 View colours none Silentnight Anti Allergy Mattress Topper - King (0) £28 00 View colours none Silentnight Anti Allergy Mattress Protector - Small Double (0) £17 00 View colours none Silentnight Anti Allergy Mattress Topper - Double (0) £25 00 View colours none Silentnight Anti Allergy Mattress Topper - Single (0) £20 00 View colours none Silentnight Anti Allergy Duvet - 7 5 Tog - King (0) £28 00 View colours none Silentnight Anti Allergy Pillow - 2 Pack (0) £18 00 2UNI | £9 00 per 1UNI View colours none Silentnight Wellbeing Collection Lavender Scented Pillow (0) £19 00 View colours none Silentnight Wellbeing Collection Copper Pillow (0) £19", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "75288948"}
{"text": "- 7 5 Tog - King (0) £28 00 View colours none Silentnight Anti Allergy Pillow - 2 Pack (0) £18 00 2UNI | £9 00 per 1UNI View colours none Silentnight Wellbeing Collection Lavender Scented Pillow (0) £19 00 View colours none Silentnight Wellbeing Collection Copper Pillow (0) £19 00 View colours none Kally Sleep Anti-Snore Pillow (0) £39 99 View product none Kally Sleep Cooling Mattress Topper - King (0) £89 99 View product none Kally Sleep Cooling Pillow (0) £49 99 View product none Kally Sleep Sherpa Fleece Body Pillow - Pink (0) £69 99 View product none Kally Sleep Cooling Mattress Topper - Double (0) £79 99 View product none Kally Sleep Copper Mattress Topper - Double (0) £64 99 View product none Kally Sleep Sherpa Fleece Body Pillow - Grey (0) £69 99 View product none Kally Sleep Sherpa Fleece Body Pillow - Cream (0) £69 99 View product Offer none This Works Sleep Plus Dream Body Lotion 50ml (0) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £28 00 50ML | £56 00 per 100ML Maximum quantity reached in your basket Add to basket x Are you sure you want to remove this product Yes No [ { \"catentry_id\" : 2645842, \"Attributes\" : { }, \"ItemImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemImage467\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon jpg\", \"ItemThumbnailImage\" : \"/wcsstore/eBootsStorefrontAssetStore/images/NoImageIcon_sm jpg\" } ] Offer none Boots Soft Disposable Ear Plugs (25) 3 for 2 on selected Boots earplugs - cheapest free £5 60 Save £1 40 Was £7 00 20UNI | £0 28 per 1UNI Stock coming soon none Soundasleep Speaker Pillow (1) £29 00 View colours Offer none This Works Deep Sleep™ Pillow Talk Set (4) Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last £27 00 Stock coming soon none Silentnight Snugsie Giant Blanket Charcoal (0) £32 99 View product Offer none Tisserand Aromatherapy Sleep Better Massage & Body Oil 100ml (4) 3 for 2 on selected Vitamins and Supplements - cheapest free £11 00 100ML | £11 00 per 100ML Stock coming soon none Dreamland Revive Me Comfy Foot Warmer 5T 50 X 48 (3) £59 99 View product none Dreamland Cosy Up Silky Soft Faux Fur Warming Throw - Cream 160X120 Cm (1) £109 99 View product none Silentnight Cooler Summer Duvet - 7 5 Tog - King (1) £26 00 View colours Offer none Kalms Night Valerian Root Extract 96mg - 50 Tablets 3 for 2 on selected Vitamins and Supplements - cheapest free £4 50 50UNI | £0 09 per 1UNI Stock coming soon Offer none Lumie Bodyclock Spark 100 wake-up light alarm clock (17) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £99 00 Stock coming soon Offer none Feather & Down Pillow Spray Duo (13) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £4", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "b2752b4f"}
{"text": "Bodyclock Spark 100 wake-up light alarm clock (17) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £99 00 Stock coming soon Offer none Feather & Down Pillow Spray Duo (13) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £4 87 Save £1 63 Was £6 50 Stock coming soon Offer none Feather & Down Sweet Dreams Pillow Spray 100ml (123) Free candle when you buy 3 selected indulgent bathing products - whilst stocks last £8 00 100ML | £8 00 per 100ML Stock coming soon none Yankee Candle Sleep Diffuser - Silver Peaceful Dream (136) £39 99 Stock coming soon none Twinings Superblends Sleep - 30g (6) £2 69 20UNI | £0 13 per 1UNI Stock coming soon Offer none Bach Rescue Peaceful Night Duo Capsule & Spray (67) Clearance - when its gone its gone £10 00 Stock coming soon none EarHub Soft Foam Earplugs 10 Pairs (12) £4 99 10UNI | £0 50 per 1UNI Stock coming soon Offer none Fitbit Charge 4 - Rosewood (15) Receive £10 worth of points for every £60 spent across selected Electrical Beauty £129 99 Stock coming soon Offer none Snoreeze Snoring Relief Throat Spray 23 5ml (26) 3 for 2 on selected Vitamins and Supplements - cheapest free £9 80 23 5ML | £41 70 per 100ML Stock coming soon Offer none Boots Foam Ear Plugs - 20s (66) 3 for 2 on selected Boots earplugs - cheapest free £5 76 Save £1 44 Was £7 20 20UNI | £0 29 per 1UNI Stock coming soon Offer none Boots Foam Earplugs - 3 Pairs with Carry Case (40) 3 for 2 on selected Boots earplugs - cheapest free £2 80 3UNI | £0 93 per 1UNI Stock coming soon none Silentnight Supersoft Thermal Mattress Cover King (0) £20 99 View product none Silentnight Supersoft Thermal Mattress Cover Single (0) £17 99 View product none Silentnight Mediflow The Water Pillow (0) £39 99 View product none Silentnight Warm & Cosy 15 Tog Duvet King (0) £42 99 View product none Silentnight Warm & Cosy 15 Tog Duvet Double (0) £37 99 View product none Silentnight Warm & Cosy 15 Tog Duvet Single (0) £32 99 View product none Silentnight Waterproof Mattress Protector Double & Pillow Pair (0) £32 99 View product none Silentnight Waterproof Mattress Protector Single & Pillow Pair (0) £26 99 View product none Silentnight Waterproof Pillow Protector Pair (0) £14 99 View product none Silentnight Waterproof Mattress Protector Double (0) £17 99 View product none Carmen Double Heated Under Blanket (0) £49 99 View product none Silentnight Cooler Summer Duvet - 7 5 Tog - Double (0) £23 00 View colours none Silentnight Cooler Summer Duvet - 7 5 Tog - Single (0) £21 00 View colours none Silentnight Serenity Cosy Up Gift Set - Mini Hot Water Bottle and Scented Candle Set (0) £17", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "abf506f3"}
{"text": "none Silentnight Cooler Summer Duvet - 7 5 Tog - Double (0) £23 00 View colours none Silentnight Cooler Summer Duvet - 7 5 Tog - Single (0) £21 00 View colours none Silentnight Serenity Cosy Up Gift Set - Mini Hot Water Bottle and Scented Candle Set (0) £17 99 View product none Silentnight Serenity Erase and Rewind Gift Set - Hot Water Bottle and Eye Mask Set (0) £22 99 View product none Silentnight Wellbeing Collection Weighted Eye Mask (0) £11 00 View colours none Kally Sleep Knee Pillow (0) £29 99 View colours none Kally Sleep Neck Pain Pillow (0) £49 99 View colours none Kally Sleep Pillow Heathered Grey (0) £59 99 View colours Offer none Boots Sleepeaze Snoring Relief Throat Spray - 42ml (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £14 50 42ML | £34 52 per 100ML Stock coming soon Offer none Boots Sleepeaze Snoring Relief Throat Spray - 14ml (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £5 50 14ML | £39 29 per 100ML Stock coming soon none Kally Sleep Body Pillow - Stone Blue (0) £59 99 View product none Kally Sleep Anti-Ageing Copper Pillow (0) £29 99 View product none Kally Sleep Copper Mattress Topper - King (0) £74 99 View product none Kally Sleep TENCEL™ Cooling Pillows - Twin Pack (0) £49 99 View product none Kally Sleep Sports Recovery Pillow - Blue (0) £69 99 View product none Withings Sleep Analyzer Under-Mattress (0) £129 95 View product none Skinnydip x Sophie Hannah Good Night Sleep Spray and Eyemask Set (0) £16 00 Stock coming soon Offer none Tisserand Sleep Better Diffuser Oil 9ml (0) 3 for 2 on selected Vitamins and Supplements - cheapest free £9 50 9ML | £105 56 per 100ML Stock coming soon none Treets Wellbeing Calming Pillow Mist 130ml £4 99 130ML | £3 84 per 100ML Stock coming soon shopping with us shopping with us A-Z Brands A-Z Store Boots Advantage Card Boots app sitemap customer services customer services help & FAQs delivery information returns & exchange product recall contact us about Boots about Boots company information environmental, social & governance modern slavery & human trafficking careers privacy & cookies terms & conditions Our partner sites Copyright © The Boots Company PLC All rights reserved Boots com is a trading name of Boots UK Limited Registered office: Nottingham NG2 3AA Registered in England: company number 928555 Registered VAT number 116300129 For details of Boots online pharmacy services see Using Our Pharmacy Services page This item has been successfully added to your list x This item has been successfully added to your list x x Inactivity Warning Dialog x x Your session is about to timeout due to inactivity Click OK to extend your time for an additional 20 minutes", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "d5b97241"}
{"text": "Our Pharmacy Services page This item has been successfully added to your list x This item has been successfully added to your list x x Inactivity Warning Dialog x x Your session is about to timeout due to inactivity Click OK to extend your time for an additional 20 minutes OK Privacy preference centre Your privacy Your privacy When you visit any web site, it may store or retrieve information on your browser, mostly in the form of cookies This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to The information does not usually directly identify you, but it can give you a more personalised web experience Because we respect your right to privacy, you can choose not to allow some types of cookies Click on the different category headings to find out more and change our default settings However, blocking some types of cookies may impact your experience of the site and the services we are able to offer More information Strictly Necessary Cookies Strictly Necessary Cookies Always Active Strictly Necessary Cookies These cookies are necessary for the website to function and cannot be switched off in our systems They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms You can set your browser to block or alert you about these cookies, but some parts of the site will not then work These cookies do not store any personally identifiable information Performance Cookies Performance Cookies Performance Cookies These cookies allow us to anonymously monitor our website usage Count visits and traffic sources so we can measure and improve the performance of our site They help us to know which pages are the most and least popular and see how visitors move around the site All information these cookies collect is aggregated and therefore anonymous If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance Functional Cookies Functional Cookies Functional Cookies These cookies enable the website to provide enhanced functionality and personalisation to improve your website experience They may be set by us or by third party providers whose services we have added to our pages If you do not allow these cookies then some or all of these services may not function properly Targeting Cookies Targeting Cookies Targeting Cookies These cookies may be set through our site by our advertising partners They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites and help provide communications relevant to your interests They do not store directly personal information, but are based on uniquely identifying your browser and internet device If you do not allow these cookies, you will experience less targeted advertising", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "0a8ea316"}
{"text": "of your interests and show you relevant adverts on other sites and help provide communications relevant to your interests They do not store directly personal information, but are based on uniquely identifying your browser and internet device If you do not allow these cookies, you will experience less targeted advertising Back Button Advertising Cookies Filter Button Consent Leg Interest Select All Vendors Select All Vendors Select All Hosts Select All Clear Filters Information storage and access Apply Save settings Allow All We're here to help", "source": "Repo:Web-Scraping:orignal.html", "section": "Web-Scraping", "hash": "19e374b3"}
{"text": "# CODE CELL import unittest from bs4 import BeautifulSoup import json def calculate_median(prices): sorted_prices = sorted(prices) n = len(sorted_prices) if n % 2 == 0: median = (sorted_prices[n // 2 - 1] + sorted_prices[n // 2]) / 2 else: median = sorted_prices[n // 2] return median class TestWebScraping(unittest.TestCase): def setUp(self): # Load the HTML content from a sample file with open('orignal.html', 'r', encoding='utf-8') as file: self.html_content = file.read() # Parse the HTML content using BeautifulSoup self.soup = BeautifulSoup(self.html_content, 'html.parser') def test_scrape_data(self): result_list = [] prices = [] # Fetching all the products information from the webpage products_items = self.soup.find_all('div', class_='oct-grid__row oct-grid__row--full-width oct-listers-hits') # Loop to find required item from the product list for item in products_items: # Finding Title from the products_items try: item_titles = item.find_all('h3', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--h3--variant-2 oct-teaser__title oct-teaser-with-listers'}) except: print(\"\") # Finding Price from the products_items try: item_prices = item.find_all('p', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--p--variant-subtext oct-teaser__productPrice'}) except: print(\"\") # Finding Offer from the products_items try: item_offer = item.find_all('div', {'class': 'oct-teaser__wrap'}) except: print(\"\") # Running a loop to capture information of each product for title, price, offer in zip(item_titles, item_prices, item_offer): title_text = title.get_text(strip=True) price_text = price.get_text(strip=True) offer_text = offer.get_text(strip=True) # \"result_list\" accumulates these dictionaries for all products on the page result_list.append({'Title': title_text, 'Price': price_text, 'Price_Unit': '£', 'Offer': offer_text}) # Extract numerical prices for median calculation numeric_price = float(price_text.replace('£', '').replace(',', '')) prices.append(numeric_price) # Assertions self.assertTrue(result_list) # Check that result_list is not empty self.assertTrue(prices) # Check that prices list is not empty self.assertEqual(len(result_list), len(prices)) # Check that lengths match def test_calculate_median(self): # Test calculate_median function with various inputs self.assertEqual(calculate_median([1, 2, 3]), 2) self.assertEqual(calculate_median([1, 2, 3, 4]), 2.5) self.assertEqual(calculate_median([5, 2, 8, 1, 9]), 5) if __name__ == '__main__': unittest.main()", "source": "Repo:Web-Scraping:test_web_scraping.ipynb", "section": "Web-Scraping", "hash": "9f2451fa"}
{"text": "{ \"Products\": [ { \"Title\": \"Boots Sleepeaze Tablets 50 mg - 20s\", \"Price\": \"£5 79\", \"Price_Unit\": \"£\", \"Offer\": \"Save 10 percent on selected Boots Pharmacy Medicines - Advantage Card Holders only\" }, { \"Title\": \"Boots Sleepeaze Herbal Plus Tablets - 30 Tablets\", \"Price\": \"£5 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Tablets 25 mg - 20s\", \"Price\": \"£3 90\", \"Price_Unit\": \"£\", \"Offer\": \"Save 10 percent on selected Boots Pharmacy Medicines - Advantage Card Holders only\" }, { \"Title\": \"Feather and Down Perfect Partners\", \"Price\": \"£7 50\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Botanics Peaceful Night Sleep Duo Gift Set\", \"Price\": \"£8 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kalms Night One-a-Night - 28 Tablets\", \"Price\": \"£9 99\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Tisserand The Little Box of Sleep 3 x 10ml\", \"Price\": \"£13 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Bach Rescue Remedy Night Liquid Melts 28 Capsules\", \"Price\": \"£7 00\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Nytol One-A-Night 50mg Tablets - 20 Tablets\", \"Price\": \"£8 60\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Nytol Herbal Simply Sleep One-A-Night Tablets\", \"Price\": \"£7 35\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Kalms Night - 56 Tablets\", \"Price\": \"£5 99\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Feather & Down Sweet Dreams Sleeping Bag Gift Set\", \"Price\": \"£14 25\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Kira Restful Sleep Valerian Root Extract 300 mg Tablets - 25 Tablets\", \"Price\": \"£6 90\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Lavender Pillow Mist 100ml\", \"Price\": \"£6 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Rescue Remedy Gummies Night 60s\", \"Price\": \"£14 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Botanics Pillow Mist Lavender & Sweet Marjoram 100ml\", \"Price\": \"£7 50\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Oral Strips - 21 Strips\", \"Price\": \"£7 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Bach Rescue Night Dropper 20ml\", \"Price\": \"£8 33\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Champneys Sleep Pillow Mist 50ml\", \"Price\": \"£12 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Dreamland Intelliheat Warming Throw - Grey\", \"Price\": \"£60", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "e88089a2"}
{"text": "\"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Champneys Sleep Pillow Mist 50ml\", \"Price\": \"£12 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Dreamland Intelliheat Warming Throw - Grey\", \"Price\": \"£60 00\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Dreamland Intelliheat Faux Fur Warming Throw - Alaskan Husky\", \"Price\": \"£99 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Boots Soft Silicone Earplugs - 3 Pairs\", \"Price\": \"£3 96\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Boots Mouldable Wax Earplugs - 5 Pairs\", \"Price\": \"£2 16\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Soap & Glory Perfect Zen Calming Bath Milk 500ml\", \"Price\": \"£6 99\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"This Works Deep Sleep™ Pillow Spray 75ml\", \"Price\": \"£21 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Dreamland Intelliheat Throw - Teal\", \"Price\": \"£59 99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"Lumie Sunrise Alarm\", \"Price\": \"£49 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Soap & Glory Perfect Zen Foaming Shower Oil 200ml\", \"Price\": \"£8 99\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Baylis & Harding Goodness Sleep Lavender & Bergamot Sleep Bath Soak 500ml\", \"Price\": \"£5 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 14ml\", \"Price\": \"£5 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Herringbone Throw - Emerald Green\", \"Price\": \"£74 99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"This Works Deep Sleep™ Pillow Spray 250ml\", \"Price\": \"£37 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Soap & Glory Perfect Zen Body Souffle 300ml\", \"Price\": \"£9 99\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Dreamland Hurry Home Warming Throw - Grey 160x120\", \"Price\": \"£74 99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"Soap & Glory Perfect Zen Warming Body Scrub 250ml\", \"Price\": \"£8", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "72916f7b"}
{"text": "3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Dreamland Hurry Home Warming Throw - Grey 160x120\", \"Price\": \"£74 99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"Soap & Glory Perfect Zen Warming Body Scrub 250ml\", \"Price\": \"£8 99\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Snoreeze Snoring Relief Oral Strips - 14 Applications\", \"Price\": \"£8 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Snugsie Giant Blanket Neutral\", \"Price\": \"£32 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Luxe Platinum/ Orchid\", \"Price\": \"£99 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Baylis & Harding Goodness Sleep Lavender & Bergamot Sleep Pillow Mist 100ml\", \"Price\": \"£7 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Feather & Down Sweet Dreams Sleep Essentials Set\", \"Price\": \"£14 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 42ml\", \"Price\": \"£14 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Soap & Glory The Rest Assured Sleep Mask\", \"Price\": \"£7 00\", \"Price_Unit\": \"£\", \"Offer\": \"Buy 1 get 2nd 1/2 price on selected Soap And Glory\" }, { \"Title\": \"Botanics Pure Essential Oil Lavender 10ml\", \"Price\": \"£6 50\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Naptime Warming Sherpa Blanket - Tartan Check Red Grey 180X135\", \"Price\": \"£89 99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"Feather & Down Sweet Dreams Melting Shower Cream 250ml\", \"Price\": \"£7 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Congestion Nasal Strips - 20 Strips\", \"Price\": \"£9 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Revive Me Multipurpose Heat Pad Standard Size\", \"Price\": \"£39 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hygge Days Faux Fur Warming Throw - Fallow Deer\", \"Price\": \"£99 99\", \"Price_Unit\": \"£\", \"Offer\": \"Only £99 99 on selected Dreamland\" }, { \"Title\": \"Snoreeze Snoring Relief Oral Device\", \"Price\": \"£37 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Feather & Down Pillow Spray 200ml\", \"Price\": \"£14 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Nytol Herbal Tablets 30 tablets\", \"Price\": \"£6 20\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Congestion Nasal Spray - 10ml\", \"Price\": \"£11", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "da564be5"}
{"text": "you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Nytol Herbal Tablets 30 tablets\", \"Price\": \"£6 20\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Congestion Nasal Spray - 10ml\", \"Price\": \"£11 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Snoreeze Lozenges - 16 lozenges\", \"Price\": \"£9 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Herringbone Throw - Navy Blue\", \"Price\": \"£74 99\", \"Price_Unit\": \"£\", \"Offer\": \"Save up to £20 on selected Dreamland - online only\" }, { \"Title\": \"BioEars Soft Silicone Earplugs with Activ Aloe - 3 pairs\", \"Price\": \"£5 55\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Feather & Down Soothing Sleep Butter 300ml\", \"Price\": \"£10 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Westlab Sleep Epsom Bath Salts with Lavender 1kg\", \"Price\": \"£7 49\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Botanics Pure Essential Oil Lavender 20ml\", \"Price\": \"£11 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Alpine Sleepdeep Sleeping Earplugs 1 Pair\", \"Price\": \"£12 95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Deep Sleep™ Bath Soak 200g\", \"Price\": \"£24 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Bach Rescue Remedy Night Spray 20ml – Flower Essences for Natural Night's Sleep\", \"Price\": \"£8 40\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Mute Medium - 3 pack (30 Night Supply) \", \"Price\": \"£19 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Rescue Balance & Positivity Capsules 30s\", \"Price\": \"£13 97\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Bach Rescue Peaceful Night Capsules 30s\", \"Price\": \"£10 63\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Snoreeze Snoring Relief Nasal Strips Small/Medium - 20 Applications\", \"Price\": \"£10 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Snoreeze Snoring Relief Nasal Spray 10ml\", \"Price\": \"£13 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Snugsie Wearable Blanket Silver\", \"Price\": \"£18 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Pillow Spray 30ml\", \"Price\": \"£19 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Silentnight Geltex Pillow\", \"Price\": \"£30 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw Pink 120X160Cm\", \"Price\": \"£69", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "3f616532"}
{"text": "\"£19 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Silentnight Geltex Pillow\", \"Price\": \"£30 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw Pink 120X160Cm\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Roller Ball - 10ml\", \"Price\": \"£8 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Baylis & Harding Goodness Sleep Lavender & Bergamot Sleep Body Wash 500ml\", \"Price\": \"£5 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"BetterYou Magnesium Sleep Body Lotion - 180ml\", \"Price\": \"£9 99\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"This Works Deep Sleep Pillow Spray 35ml\", \"Price\": \"£14 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Nytol Original Tablets 25mg - 20 Tablets\", \"Price\": \"£6 30\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Alpine Sleepdeep Mini Sleeping Earplugs 1 Pair\", \"Price\": \"£12 95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Giant Blanket Green\", \"Price\": \"£32 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Silent Power Pure Air Fan Heater\", \"Price\": \"£119 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Silentnight Snugsie Wearable Blanket Blush\", \"Price\": \"£18 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"BetterYou Magnesium Sleep Flakes - 750g\", \"Price\": \"£6 99\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Aroma Active Laboratories Sleep Salt Soak 500g\", \"Price\": \"£7 50\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/2 price across the Aroma Active range - online only\" }, { \"Title\": \"Aroma Active Laboratories Sleep Over Night Recovery Face Oil 30ml\", \"Price\": \"£7 50\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/2 price across the Aroma Active range - online only\" }, { \"Title\": \"Bach Rescue Remedy Night Dropper 10ml - Flower Essences for Natural Night's Sleep\", \"Price\": \"£6 33\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/3 on selected Bach Rescue Remedy - online only\" }, { \"Title\": \"Feather & Down Sweet Dreams Bath Essence 500ml\", \"Price\": \"£8 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Nytol Herbal Simply Sleep & Calm Elixir - 100ml\", \"Price\": \"£7 90\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Revive Me Back Heat Pad 5T 61X38 Cm\", \"Price\": \"£59 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hygge Days Faux Fur Warming Throw - Zebra\", \"Price\": \"£99 99\", \"Price_Unit\": \"£\", \"Offer\": \"Only £99 99 on selected Dreamland\" }, { \"Title\": \"This Works Deep Sleep™ Heavenly Candle\", \"Price\": \"£26", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "2c604093"}
{"text": "Revive Me Back Heat Pad 5T 61X38 Cm\", \"Price\": \"£59 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hygge Days Faux Fur Warming Throw - Zebra\", \"Price\": \"£99 99\", \"Price_Unit\": \"£\", \"Offer\": \"Only £99 99 on selected Dreamland\" }, { \"Title\": \"This Works Deep Sleep™ Heavenly Candle\", \"Price\": \"£26 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Pillow Mist\", \"Price\": \"£13 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Snoreeze Snoring Relief Nasal Strips Large - 20 Applications\", \"Price\": \"£10 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Snugsie Wearable Blanket Charcoal\", \"Price\": \"£18 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Mattress Protector King\", \"Price\": \"£19 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Pillow Spray 100ml\", \"Price\": \"£35 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector King 2 Controls 200X150Cm\", \"Price\": \"£149 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer King 2 Controls 200X150Cm\", \"Price\": \"£124 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Double 1 Control 190X137Cm\", \"Price\": \"£99 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aroma Active Laboratories Sleep Pulsepoint Rollerball 6ml\", \"Price\": \"£4 00\", \"Price_Unit\": \"£\", \"Offer\": \"Save 1/2 price across the Aroma Active range - online only\" }, { \"Title\": \"Silentnight V Shaped Support Pillow\", \"Price\": \"£17 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Serenity Weighted Eye Mask\", \"Price\": \"£17 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kalms Rhodiola Tablets- 20 Tablets\", \"Price\": \"£10 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Bath Oil 200ml\", \"Price\": \"£16 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw - Mustard 120X160Cm\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Silent Power Eco Fan Heater\", \"Price\": \"£94 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Dreamland Peaceful Dreams Warming Over Blanket Double Dual 6T 180X180\", \"Price\": \"£114 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Alpine Sleepdeep Multi Size Sleeping Earplugs 2 Pairs\", \"Price\": \"£17 95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Pillow\", \"Price\": \"£29 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Healthspan Night Time CBD Oil 260mg 10ml\", \"Price\": \"£19 95\", \"Price_Unit\": \"£\", \"Offer\": \"£18", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "fb24893f"}
{"text": "}, { \"Title\": \"Alpine Sleepdeep Multi Size Sleeping Earplugs 2 Pairs\", \"Price\": \"£17 95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Pillow\", \"Price\": \"£29 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Healthspan Night Time CBD Oil 260mg 10ml\", \"Price\": \"£19 95\", \"Price_Unit\": \"£\", \"Offer\": \"£18 95with Advantage Card\" }, { \"Title\": \"Dreamland Silent Power Comfort\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Dreamland Peaceful Dreams Warming Over Blanket Single 6T 180X135 Cm\", \"Price\": \"£84 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Double 2 Controls 190X137Cm\", \"Price\": \"£114 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Peaceful Dreams Warming Over Blanket King Dual 6T 215X225\", \"Price\": \"£124 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Indulgent Days Soft Velvet Warming Throw Geometric Peacock 120 X 160 Cm\", \"Price\": \"£79 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Versa 3 Black\", \"Price\": \"£169 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Silentnight Snugsie Kids Glow In The Dark Hoody Denim\", \"Price\": \"£22 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Oversized Hoody Charocal\", \"Price\": \"£27 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Wearable Blanket Sage\", \"Price\": \"£18 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Sealy Side Sleeper Pillow\", \"Price\": \"£25 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Superking 2 Controls 200X180Cm\", \"Price\": \"£139 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hunker Down Scandi Sherpa Full Bed Size Mattress Warmer Single 190X90Cm\", \"Price\": \"£79 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Deep Sleep™ Night Oil 120ml\", \"Price\": \"£26 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Feather & Down Breathe Well Pillow Spray\", \"Price\": \"£8 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Aromatherapy Associates Deep Relax Roller Ball 10ml\", \"Price\": \"£25 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector Double 1 Control 190X137Cm\", \"Price\": \"£124 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aromatherapy Associates Deep Relax Sleep Mist 50ml\", \"Price\": \"£32 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Klearvol Pillow Spray 100ml\", \"Price\": \"£9 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Klearvol Vapour Rub 50ml\", \"Price\": \"£4 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Dreamer Superior Cotton Mattress Warmer Single 150X80 Cm\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Supersoft Thermal Mattress Cover Double\", \"Price\": \"£19", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "85061dea"}
{"text": "99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Klearvol Vapour Rub 50ml\", \"Price\": \"£4 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Dreamer Superior Cotton Mattress Warmer Single 150X80 Cm\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Supersoft Thermal Mattress Cover Double\", \"Price\": \"£19 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Heat Genie Self Heating Mattress Topper King\", \"Price\": \"£39 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Heat Genie Self Heating Mattress Topper Single\", \"Price\": \"£28 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Oversized Hoody Charocal Glitter\", \"Price\": \"£27 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Deep Sleep Duvet 13 5 Tog Double\", \"Price\": \"£34 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Klearvol Vapour Rub & Pillow Spray Bundle\", \"Price\": \"£13 48\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"NytEase Stress + Tension Support Pillow Spray - 100ml\", \"Price\": \"£7 33\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Serenity Sweet Dreams Gift Set - Eye Mask, Pillow Mist and Ear Plugs Set\", \"Price\": \"£14 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Alpine Soft Silicone Earplugs 3 Pairs\", \"Price\": \"£8 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Massage Relief 10ml\", \"Price\": \"£15 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Cowshed Sleepy Bath Salts 300g\", \"Price\": \"£24 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Dreamland Naptime Warming Sherpa Blanket Beige Tartan Check 180X135 Cm\", \"Price\": \"£109 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector Single 190X90Cm\", \"Price\": \"£99 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Squishy Body Support Pillow\", \"Price\": \"£28 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Cool Touch Pillow\", \"Price\": \"£30 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Body Pillow - Pure White\", \"Price\": \"£59 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Acid Reflux Wedge Pillow\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Headspace Mind Giftcard - 6 months Pre-Paid Membership\", \"Price\": \"£30 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Cowshed Sleepy Calming Pillow Mist 100ml\", \"Price\": \"£18 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Tisserand Aromatherapy Dream Bath Sleep Better Bathtime Collection\", \"Price\": \"£15 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector Superking 2 Controls 200X180Cm\", \"Price\": \"£169 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw - Navy 120X160Cm\", \"Price\": \"£69", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "ccfe73af"}
{"text": "00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector Superking 2 Controls 200X180Cm\", \"Price\": \"£169 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw - Navy 120X160Cm\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aromatherapy Associates Relax Body Oil 100ml\", \"Price\": \"£52 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Snuggle Up Warming Throw - Black 120X160Cm\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Vegan Pillow Spray 10ml\", \"Price\": \"£12 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Silentnight Heat Genie Self Heating Mattress Topper Super King\", \"Price\": \"£44 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Deep Sleep Duvet 13 5 Tog King\", \"Price\": \"£39 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Duvet - 7 5 Tog - Double\", \"Price\": \"£25 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Duvet - 7 5 Tog - Single\", \"Price\": \"£23 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Re-balance Pillow Pair\", \"Price\": \"£23 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Honeycomb Cooling Pillow\", \"Price\": \"£54 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Twinings Superblends Unwind Tea Bags 20s\", \"Price\": \"£2 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Headspace Sleep Giftcard - 6 months Pre-Paid Membership\", \"Price\": \"£30 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Silent Power Protection Fan Heater\", \"Price\": \"£94 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Kally Sleep Ultimate Side Sleeper Pillow\", \"Price\": \"£49 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Snowed In Organic Cotton Warming Mattress Protector Double 2 Controls 190X137Cm\", \"Price\": \"£139 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Hurry Home Warming Throw - Mustard 160X120\", \"Price\": \"£94 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Feather & Down Sweet Dreams Sleep Butter 300ml\", \"Price\": \"£10 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Alpine Sleepsoft Earplugs 1 Pair\", \"Price\": \"£11 95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aromatherapy Associates Deep Relax Sleep Mist 10ml\", \"Price\": \"£25 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Candle\", \"Price\": \"£20 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Cosy Up Silky Soft Faux Fur Warming Throw Pink 160X120 Cm\", \"Price\": \"£109 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Sense Lunar White\", \"Price\": \"£219", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "a5f3bc6a"}
{"text": "Better Candle\", \"Price\": \"£20 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Cosy Up Silky Soft Faux Fur Warming Throw Pink 160X120 Cm\", \"Price\": \"£109 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Sense Lunar White\", \"Price\": \"£219 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Silentnight Luxury Anti-Snore Pillow\", \"Price\": \"£22 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Aromatherapy Associates 3 Step Introduction to Sleep\", \"Price\": \"£45 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Silentnight Heat Genie Self Heating Mattress Topper Double\", \"Price\": \"£37 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Kids Glow In The Dark Hoody Blush\", \"Price\": \"£22 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Snugsie Oversized Hoody Blush\", \"Price\": \"£27 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Sealy Dual Comfort Pillow\", \"Price\": \"£40 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Sealy Airflow Pillow\", \"Price\": \"£37 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Mattress Topper 8cm Super King\", \"Price\": \"£64 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Mattress Topper 8cm King\", \"Price\": \"£59 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Mattress Topper 8cm Double\", \"Price\": \"£49 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Mattress Topper 8cm Single\", \"Price\": \"£42 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Airmax Support Pillow Pair\", \"Price\": \"£29 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnightt Impress Memory Foam Mattress Topper Super King\", \"Price\": \"£149 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Mattress Topper King\", \"Price\": \"£139 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Mattress Topper Double\", \"Price\": \"£129 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Mattress Topper Small Double\", \"Price\": \"£124 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Impress Memory Foam Mattress Topper Single\", \"Price\": \"£109 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Deep Sleep Duvet 13 5 Tog Super King\", \"Price\": \"£44 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Deep Sleep Duvet 13 5 Tog Single\", \"Price\": \"£26 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy Pillow Pair\", \"Price\": \"£17 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy 15 Tog Duvet Super King\", \"Price\": \"£49 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Healthy Growth Waterproof Mattress Protector Double\", \"Price\": \"£17 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Healthy Growth Waterproof Mattress Protector Single\", \"Price\": \"£14 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Carmen Kingsize Heated Under Blanket\", \"Price\": \"£55 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Dreamer Superior Mattress Warmer King Dual 150x160 CM\", \"Price\": \"£119", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "0ca2c42e"}
{"text": "\"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Healthy Growth Waterproof Mattress Protector Single\", \"Price\": \"£14 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Carmen Kingsize Heated Under Blanket\", \"Price\": \"£55 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Dreamer Superior Mattress Warmer King Dual 150x160 CM\", \"Price\": \"£119 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Dreamer Superior Cotton Mattress Warmer Double Dual 150X137 Cm\", \"Price\": \"£99 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Up Silky Soft Faux Fur Warming Throw Terracotta 160 X 120 Cm\", \"Price\": \"£109 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Summer Breeze Duvet - 2 5 Tog - King\", \"Price\": \"£24 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Summer Breeze Duvet - 2 5 Tog - Double\", \"Price\": \"£21 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Summer Breeze Duvet - 2 5 Tog - Single\", \"Price\": \"£18 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Cooler Summer Pillow - 2 Pack\", \"Price\": \"£18 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Protector - King\", \"Price\": \"£19 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Protector - Double\", \"Price\": \"£18 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Topper - King\", \"Price\": \"£28 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Protector - Small Double\", \"Price\": \"£17 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Topper - Double\", \"Price\": \"£25 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Mattress Topper - Single\", \"Price\": \"£20 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Duvet - 7 5 Tog - King\", \"Price\": \"£28 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Anti Allergy Pillow - 2 Pack\", \"Price\": \"£18 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Lavender Scented Pillow\", \"Price\": \"£19 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Copper Pillow\", \"Price\": \"£19 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Anti-Snore Pillow\", \"Price\": \"£39 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Cooling Mattress Topper - King\", \"Price\": \"£89 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Cooling Pillow\", \"Price\": \"£49 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Sherpa Fleece Body Pillow - Pink\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Cooling Mattress Topper - Double\", \"Price\": \"£79 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Copper Mattress Topper - Double\", \"Price\": \"£64 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Sherpa Fleece Body Pillow - Grey\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Sherpa Fleece Body Pillow - Cream\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Dream Body Lotion 50ml\", \"Price\": \"£28", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "53a3b414"}
{"text": "\"\" }, { \"Title\": \"Kally Sleep Sherpa Fleece Body Pillow - Grey\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Sherpa Fleece Body Pillow - Cream\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Sleep Plus Dream Body Lotion 50ml\", \"Price\": \"£28 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Boots Soft Disposable Ear Plugs\", \"Price\": \"£5 60\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Soundasleep Speaker Pillow\", \"Price\": \"£29 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"This Works Deep Sleep™ Pillow Talk Set\", \"Price\": \"£27 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free This Works Overnight Cream 20ml when you spend £25 on selected This Works - online only, whilst stocks last\" }, { \"Title\": \"Silentnight Snugsie Giant Blanket Charcoal\", \"Price\": \"£32 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Tisserand Aromatherapy Sleep Better Massage & Body Oil 100ml\", \"Price\": \"£11 00\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Dreamland Revive Me Comfy Foot Warmer 5T 50 X 48\", \"Price\": \"£59 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Dreamland Cosy Up Silky Soft Faux Fur Warming Throw - Cream 160X120 Cm\", \"Price\": \"£109 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Cooler Summer Duvet - 7 5 Tog - King\", \"Price\": \"£26 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kalms Night Valerian Root Extract 96mg - 50 Tablets\", \"Price\": \"£4 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Lumie Bodyclock Spark 100 wake-up light alarm clock\", \"Price\": \"£99 00\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Feather & Down Pillow Spray Duo\", \"Price\": \"£4 87\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Feather & Down Sweet Dreams Pillow Spray 100ml\", \"Price\": \"£8 00\", \"Price_Unit\": \"£\", \"Offer\": \"Free candle when you buy 3 selected indulgent bathing products - whilst stocks last\" }, { \"Title\": \"Yankee Candle Sleep Diffuser - Silver Peaceful Dream\", \"Price\": \"£39 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Twinings Superblends Sleep - 30g\", \"Price\": \"£2 69\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Bach Rescue Peaceful Night Duo Capsule & Spray\", \"Price\": \"£10 00\", \"Price_Unit\": \"£\", \"Offer\": \"Clearance - when its gone its gone\" }, { \"Title\": \"EarHub Soft Foam Earplugs 10 Pairs\", \"Price\": \"£4 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Charge 4 - Rosewood\", \"Price\": \"£129 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Snoreeze Snoring Relief Throat Spray 23 5ml\", \"Price\": \"£9", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "fa7df898"}
{"text": "Foam Earplugs 10 Pairs\", \"Price\": \"£4 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Fitbit Charge 4 - Rosewood\", \"Price\": \"£129 99\", \"Price_Unit\": \"£\", \"Offer\": \"Receive £10 worth of points for every £60 spent across selected Electrical Beauty\" }, { \"Title\": \"Snoreeze Snoring Relief Throat Spray 23 5ml\", \"Price\": \"£9 80\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Foam Ear Plugs - 20s\", \"Price\": \"£5 76\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Boots Foam Earplugs - 3 Pairs with Carry Case\", \"Price\": \"£2 80\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Boots earplugs - cheapest free\" }, { \"Title\": \"Silentnight Supersoft Thermal Mattress Cover King\", \"Price\": \"£20 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Supersoft Thermal Mattress Cover Single\", \"Price\": \"£17 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Mediflow The Water Pillow\", \"Price\": \"£39 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy 15 Tog Duvet King\", \"Price\": \"£42 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy 15 Tog Duvet Double\", \"Price\": \"£37 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Warm & Cosy 15 Tog Duvet Single\", \"Price\": \"£32 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Mattress Protector Double & Pillow Pair\", \"Price\": \"£32 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Mattress Protector Single & Pillow Pair\", \"Price\": \"£26 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Pillow Protector Pair\", \"Price\": \"£14 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Waterproof Mattress Protector Double\", \"Price\": \"£17 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Carmen Double Heated Under Blanket\", \"Price\": \"£49 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Cooler Summer Duvet - 7 5 Tog - Double\", \"Price\": \"£23 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Cooler Summer Duvet - 7 5 Tog - Single\", \"Price\": \"£21 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Serenity Cosy Up Gift Set - Mini Hot Water Bottle and Scented Candle Set\", \"Price\": \"£17 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Serenity Erase and Rewind Gift Set - Hot Water Bottle and Eye Mask Set\", \"Price\": \"£22 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Silentnight Wellbeing Collection Weighted Eye Mask\", \"Price\": \"£11 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Knee Pillow\", \"Price\": \"£29 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Neck Pain Pillow\", \"Price\": \"£49 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Pillow Heathered Grey\", \"Price\": \"£59 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 42ml\", \"Price\": \"£14 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 14ml\", \"Price\": \"£5", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "8921911c"}
{"text": "Heathered Grey\", \"Price\": \"£59 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 42ml\", \"Price\": \"£14 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Boots Sleepeaze Snoring Relief Throat Spray - 14ml\", \"Price\": \"£5 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Kally Sleep Body Pillow - Stone Blue\", \"Price\": \"£59 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Anti-Ageing Copper Pillow\", \"Price\": \"£29 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Copper Mattress Topper - King\", \"Price\": \"£74 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep TENCEL™ Cooling Pillows - Twin Pack\", \"Price\": \"£49 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Kally Sleep Sports Recovery Pillow - Blue\", \"Price\": \"£69 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Withings Sleep Analyzer Under-Mattress\", \"Price\": \"£129 95\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Skinnydip x Sophie Hannah Good Night Sleep Spray and Eyemask Set\", \"Price\": \"£16 00\", \"Price_Unit\": \"£\", \"Offer\": \"\" }, { \"Title\": \"Tisserand Sleep Better Diffuser Oil 9ml\", \"Price\": \"£9 50\", \"Price_Unit\": \"£\", \"Offer\": \"3 for 2 on selected Vitamins and Supplements - cheapest free\" }, { \"Title\": \"Treets Wellbeing Calming Pillow Mist 130ml\", \"Price\": \"£4 99\", \"Price_Unit\": \"£\", \"Offer\": \"\" } ], \"Median\": 23 0 }", "source": "Repo:Web-Scraping:output.json", "section": "Web-Scraping", "hash": "66fc18e1"}
{"text": "#!/usr/bin/env python # coding: utf-8 # In[1]: import unittest from bs4 import BeautifulSoup import json def calculate_median(prices): sorted_prices = sorted(prices) n = len(sorted_prices) if n % 2 == 0: median = (sorted_prices[n // 2 - 1] + sorted_prices[n // 2]) / 2 else: median = sorted_prices[n // 2] return median class TestWebScraping(unittest.TestCase): def setUp(self): # Load the HTML content from a sample file with open('orignal.html', 'r', encoding='utf-8') as file: self.html_content = file.read() # Parse the HTML content using BeautifulSoup self.soup = BeautifulSoup(self.html_content, 'html.parser') def test_scrape_data(self): result_list = [] prices = [] # Fetching all the products information from the webpage products_items = self.soup.find_all('div', class_='oct-grid__row oct-grid__row--full-width oct-listers-hits') # Loop to find required item from the product list for item in products_items: # Finding Title from the products_items try: item_titles = item.find_all('h3', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--h3--variant-2 oct-teaser__title oct-teaser-with-listers'}) except: print(\"\") # Finding Price from the products_items try: item_prices = item.find_all('p', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--p--variant-subtext oct-teaser__productPrice'}) except: print(\"\") # Finding Offer from the products_items try: item_offer = item.find_all('div', {'class': 'oct-teaser__wrap'}) except: print(\"\") # Running a loop to capture information of each product for title, price, offer in zip(item_titles, item_prices, item_offer): title_text = title.get_text(strip=True) price_text = price.get_text(strip=True) offer_text = offer.get_text(strip=True) # \"result_list\" accumulates these dictionaries for all products on the page result_list.append({'Title': title_text, 'Price': price_text, 'Price_Unit': '£', 'Offer': offer_text}) # Extract numerical prices for median calculation numeric_price = float(price_text.replace('£', '').replace(',', '')) prices.append(numeric_price) # Assertions self.assertTrue(result_list) # Check that result_list is not empty self.assertTrue(prices) # Check that prices list is not empty self.assertEqual(len(result_list), len(prices)) # Check that lengths match def test_calculate_median(self): # Test calculate_median function with various inputs self.assertEqual(calculate_median([1, 2, 3]), 2) self.assertEqual(calculate_median([1, 2, 3, 4]), 2.5) self.assertEqual(calculate_median([5, 2, 8, 1, 9]), 5) if __name__ == '__main__': unittest.main() # In[ ]:", "source": "Repo:Web-Scraping:test_web_scraping.py", "section": "Web-Scraping", "hash": "77820388"}
{"text": "<h1>Boots Website Scraper</h1> <p>This Python script is designed to scrape product information from the Boots website. It utilizes the BeautifulSoup library for HTML parsing and requests to send HTTP requests to the server. Because the request for the website content was unsuccessful, a copy of the website in .html format was obtained and used instead.</p> <h2>Prerequisites</h2> <p>Before running the script, make sure you have the required Python libraries installed. You can install them using the following command:</p> <pre><code>pip install beautifulsoup4</code></pre> <h2>Usage</h2> <ol> <li>Clone the repository:</li> <pre><code>git clone https://github.com/your-username/Web-Scraping.git</code></pre> <li>Navigate to the project directory:</li> <pre><code>cd Web-Scraping</code></pre> <li>Run the script to download the JSON file directly:</li> <pre><code>python boot_web_scraping.py</code></pre> </ol> <h2>Functions</h2> <h3><code>calculate_median(prices)</code></h3> <p>Calculates the median value of a list of prices.</p> <h4>Parameters</h4> <ul> <li><code>prices</code> (list of float): A list of numerical values representing prices.</li> </ul> <h4>Returns</h4> <ul> <li><code>float</code>: The median value of the provided prices.</li> </ul> <h3><code>scrape_boots_website(url)</code></h3> <p>Scrapes product information from the Boots website.</p> <h4>Parameters</h4> <ul> <li><code>url</code> (str): The URL of the Boots website page to be scraped.</li> </ul> <h4>Returns</h4> <ul> <li><code>BeautifulSoup</code>: A BeautifulSoup object representing the parsed HTML content of the webpage.</li> </ul> <h2>Script Flow</h2> <ol> <li>The script opens the <code>orignal.html</code> file and retrieves the HTML content.</li> <li>The HTML content is then parsed using BeautifulSoup.</li> <li>Product information, including Title, Price, and Offer, is extracted from the parsed HTML. As the Price_Unit was hard coded, so no extraction needed there.</li> <li>The script calculates the median price from the extracted numerical prices.</li> <li>The results are saved in a JSON file named <code>output.json</code>.</li> </ol> <h2>File Input</h2> <p>The script expects an HTML file named <code>orignal.html</code> in the project directory. Make sure to provide the correct file path or name in the script.</p> <h2>File Output</h2> <p>The script generates a JSON file named <code>output.json</code> containing a list of products and their details, along with the calculated median price.</p> ---------------------------------------- ---------------------------------------- # Web Scraping Unit Test This unit test script is designed to validate the functionality of the Boots Website Scraper. It uses the `unittest` library to conduct tests on the scraping and calculation functions. ## Test Cases ### Test Case 1: `test_scrape_data` This test ensures that the scraping function retrieves data from the HTML content correctly. It checks the following: 1. `result_list` is not empty. 2. `prices` list is not empty. 3. The lengths of `result_list` and `prices` match. ### Test Case 2: `test_calculate_median` This test verifies the correctness of the `calculate_median` function by testing it with different inputs. ## How to Run Tests 1. Ensure you have the required Python libraries installed. You can install them using the following command: ```bash pip install beautifulsoup4 ``` 2. Clone the repository: ```bash git clone https://github.com/your-username/Web-Scraping.git ``` 3. Navigate to the project directory: ```bash cd Web-Scraping ``` 4. Run the unit tests: ```bash python test_web_scraping.py ``` ## Additional Information", "source": "Repo:Web-Scraping:README.md", "section": "Web-Scraping", "hash": "3ead55b4"}
{"text": "have the required Python libraries installed. You can install them using the following command: ```bash pip install beautifulsoup4 ``` 2. Clone the repository: ```bash git clone https://github.com/your-username/Web-Scraping.git ``` 3. Navigate to the project directory: ```bash cd Web-Scraping ``` 4. Run the unit tests: ```bash python test_web_scraping.py ``` ## Additional Information - The HTML content for testing is loaded from the `orignal.html` file in the project directory. - The script uses BeautifulSoup for HTML parsing and includes a `calculate_median` function for calculating the median value of a list of prices. - The results of the scraping are stored in a list named `result_list`. - The numerical prices are stored in a list named `prices`. - The script includes assertions to ensure the correctness of the scraping and calculation functions.", "source": "Repo:Web-Scraping:README.md", "section": "Web-Scraping", "hash": "ddc8123c"}
{"text": "# CODE CELL from bs4 import BeautifulSoup # For collecting data import json # To create a JSON file import requests # For sending request to the server to get data # CODE CELL # Define a function to calculate median value def calculate_median(prices): \"\"\" Calculates the median value of a list of prices. Parameters: - prices (list of float): A list of numerical values representing prices. Returns: - float: The median value of the provided prices. \"\"\" sorted_prices = sorted(prices) n = len(sorted_prices) if n % 2 == 0: median = (sorted_prices[n//2 - 1] + sorted_prices[n//2]) / 2 else: median = sorted_prices[n//2] return median # Use this function with a working URL def scrape_boots_website(url): \"\"\" Scrapes product information from the Boots website. Parameters: - url (str): The URL of the Boots website page to be scraped. Returns: - BeautifulSoup: A BeautifulSoup object representing the parsed HTML content of the webpage. Example: >>> url = \"https://www.boots.com/health-pharmacy/medicines-treatments/sleep?paging.index=0&amp;paging.size=24&amp;sortBy=mostRelevant&amp;criteria.category=wellness---sleep&amp;criteria.brand=Bach+Rescue+Remedy---Boots\" >>> soup = scrape_boots_website(url) >>> # Countinue with Ln [4] \"\"\" response = requests.get(url) soup = BeautifulSoup(response.text, 'html.parser') return soup # CODE CELL # Give file name or path, depending on the location of your file file_path = 'orignal.html' # Read the contents of the HTML file with open(file_path, 'r', encoding='utf-8') as file: html_content = file.read() # Parsing the HTML content to extract different data's soup = BeautifulSoup(html_content, 'html.parser') # CODE CELL # Creating two list for storing respected values result_list = [] # For appending all the items prices = [] # For appending all the \"numeric_price\" # Fetching all the products information from the webpage products_items = soup.find_all('div', class_='oct-grid__row oct-grid__row--full-width oct-listers-hits') # Loop to find required item from the product list for item in products_items: # Finding Title from the products_items try: item_titles = item.find_all('h3', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--h3--variant-2 oct-teaser__title oct-teaser-with-listers'}) except: print(\"\") # Finding Price from the products_items try: item_prices = item.find_all('p', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--p--variant-subtext oct-teaser__productPrice'}) except: print(\"\") # Finding Offer from the products_items try: item_offer = item.find_all('div', {'class': 'oct-teaser__wrap'}) except: print(\"\") # Running a loop to capture information of each product for title, price, offer in zip(item_titles, item_prices, item_offer): title_text = title.get_text(strip=True) price_text = price.get_text(strip=True) offer_text = offer.get_text(strip=True) # \"result_list\" accumulates these dictionaries for all products on the page result_list.append({'Title': title_text, 'Price': price_text, 'Price_Unit': '£', 'Offer': offer_text}) # Extract numerical prices for median calculation numeric_price = float(price_text.replace('£', '').replace(',', '')) prices.append(numeric_price) # CODE CELL # Calculate the median price median_price = calculate_median(prices) # Save the information in a JSON file output_data = { 'Products': result_list, 'Median': median_price } # CODE CELL # Dumping the JSON file, named \"output.json\" with open('output.json', 'w', encoding='utf-8') as output_file: json.dump(output_data, output_file, indent=2, ensure_ascii=False)", "source": "Repo:Web-Scraping:boot_web_scraping.ipynb", "section": "Web-Scraping", "hash": "ff5d7683"}
{"text": "#!/usr/bin/env python # coding: utf-8 # In[1]: from bs4 import BeautifulSoup # For collecting data import json # To create a JSON file import requests # For sending request to the server to get data # In[2]: # Define a function to calculate median value def calculate_median(prices): \"\"\" Calculates the median value of a list of prices. Parameters: - prices (list of float): A list of numerical values representing prices. Returns: - float: The median value of the provided prices. \"\"\" sorted_prices = sorted(prices) n = len(sorted_prices) if n % 2 == 0: median = (sorted_prices[n//2 - 1] + sorted_prices[n//2]) / 2 else: median = sorted_prices[n//2] return median # Use this function with a working URL def scrape_boots_website(url): \"\"\" Scrapes product information from the Boots website. Parameters: - url (str): The URL of the Boots website page to be scraped. Returns: - BeautifulSoup: A BeautifulSoup object representing the parsed HTML content of the webpage. Example: >>> url = \"https://www.boots.com/health-pharmacy/medicines-treatments/sleep?paging.index=0&amp;paging.size=24&amp;sortBy=mostRelevant&amp;criteria.category=wellness---sleep&amp;criteria.brand=Bach+Rescue+Remedy---Boots\" >>> soup = scrape_boots_website(url) >>> # Countinue with Ln [4] \"\"\" response = requests.get(url) soup = BeautifulSoup(response.text, 'html.parser') return soup # In[3]: # Give file name or path, depending on the location of your file file_path = 'orignal.html' # Read the contents of the HTML file with open(file_path, 'r', encoding='utf-8') as file: html_content = file.read() # Parsing the HTML content to extract different data's soup = BeautifulSoup(html_content, 'html.parser') # In[4]: # Creating two list for storing respected values result_list = [] # For appending all the items prices = [] # For appending all the \"numeric_price\" # Fetching all the products information from the webpage products_items = soup.find_all('div', class_='oct-grid__row oct-grid__row--full-width oct-listers-hits') # Loop to find required item from the product list for item in products_items: # Finding Title from the products_items try: item_titles = item.find_all('h3', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--h3--variant-2 oct-teaser__title oct-teaser-with-listers'}) except: print(\"\") # Finding Price from the products_items try: item_prices = item.find_all('p', {'class': 'oct-text oct-text--standard oct-text--size_m oct-aem-text oct-aem-text--p--variant-subtext oct-teaser__productPrice'}) except: print(\"\") # Finding Offer from the products_items try: item_offer = item.find_all('div', {'class': 'oct-teaser__wrap'}) except: print(\"\") # Running a loop to capture information of each product for title, price, offer in zip(item_titles, item_prices, item_offer): title_text = title.get_text(strip=True) price_text = price.get_text(strip=True) offer_text = offer.get_text(strip=True) # \"result_list\" accumulates these dictionaries for all products on the page result_list.append({'Title': title_text, 'Price': price_text, 'Price_Unit': '£', 'Offer': offer_text}) # Extract numerical prices for median calculation numeric_price = float(price_text.replace('£', '').replace(',', '')) prices.append(numeric_price) # In[5]: # Calculate the median price median_price = calculate_median(prices) # Save the information in a JSON file output_data = { 'Products': result_list, 'Median': median_price } # In[6]: # Dumping the JSON file, named \"output.json\" with open('output.json', 'w', encoding='utf-8') as output_file: json.dump(output_data, output_file, indent=2, ensure_ascii=False)", "source": "Repo:Web-Scraping:boot_web_scraping.py", "section": "Web-Scraping", "hash": "713597a3"}
{"text": "# CODE CELL import re import string import nltk from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer # CODE CELL def preprocess_text(df, column): # Remove special characters and numbers df[column] = df[column].apply(lambda text: re.sub(r'[^a-zA-Z\\s]', '', str(text))) # Convert to lowercase df[column] = df[column].str.lower() # Remove multiple occurrences of 'i' characters df[column] = df[column].apply(lambda text: re.sub(r'(i{2,})', 'i', str(text))) # Remove single alphabets df[column] = df[column].apply(lambda text: re.sub(r'\\b[a-zA-Z]\\b', '', str(text))) # Tokenize the text df[column] = df[column].apply(lambda text: nltk.word_tokenize(text)) # Remove stopwords stop_words = set(stopwords.words('english')) df[column] = df[column].apply(lambda tokens: [token for token in tokens if token not in stop_words]) # Lemmatize the tokens lemmatizer = WordNetLemmatizer() df[column] = df[column].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens]) # Remove 'mmddyyy' df[column] = df[column].apply(lambda text: re.sub(r'(\\b[mmddyyy]+\\b)', '', str(text))) # Remove extra whitespace df[column] = df[column].apply(lambda text: re.sub(r'\\s+', '', str(text))) # Join the tokens back into a string df[column] = df[column].apply(lambda tokens: ''.join(tokens)) return df", "source": "Repo:Detecting-Data-Drift-_Automated:preprocessing_module.ipynb", "section": "Detecting-Data-Drift-_Automated", "hash": "64b73c3b"}
{"text": "# CODE CELL import os import re import gensim import string import nltk import pickle import numpy as np import pandas as pd from numpy import percentile import matplotlib.pyplot as plt from gensim.models import Word2Vec from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer from sklearn.metrics.pairwise import cosine_similarity from sklearn.feature_extraction.text import TfidfVectorizer # CODE CELL import preprocessing_module # CODE CELL import warnings warnings.filterwarnings(\"ignore\") import pandas as pd import os # Step 1: Load the corpus content = [] file_names = [] path = '/mnt/c/Users/alamm9/Desktop/text_file_train_test/train_set' for filename in os.listdir(path): with open(os.path.join(path, filename), 'r') as file: content.append(file.read()) file_names.append(filename) # Step 2: Create a dataframe from the corpus df_train = pd.DataFrame({'Filename': file_names, 'Content': content}) # CODE CELL df_train = preprocessing_module.preprocess_text(df_train, \"Content\") # CODE CELL from sklearn.model_selection import train_test_split # Splitting the dataset df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=40) # CODE CELL def w2v(df_train,df): model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5, min_count=1, workers=4) # with open(\"model.pkl\", \"wb\") as f: # pickle.dump(model, f) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # with open(\"train_embeddings(w2v).pkl\", \"wb\") as f: # pickle.dump(train_embeddings, f) # print(train_embeddings[0]) # print(len(train_embeddings)) # print(train_embeddings[0].shape) # Transform the test set embeddings using the learned transformation from the training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) #trian 80% and validation 20% print(\"similarity_matrix\",similarity_matrix.shape) # print(similarity_matrix[0]) # Calculate mean similarity for each row mean_similarity = np.mean(similarity_matrix, axis=0) # print(mean_similarity) # print(mean_similarity.shape) threshold = np.percentile(mean_similarity, [10]) print(\"threshold(w2v):\", threshold) # with open(\"threshold(w2v).pkl\", \"wb\") as f: # pickle.dump(threshold, f) #************************************************************************************************************************ def tfidf(df_train,df): # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() # Fitting vectorizer in the training dataset train = vectorizer.fit_transform(df_train[\"Content\"]) # with open(\"train(tfidf).pkl\", \"wb\") as f: # pickle.dump(train, f) # print(\"train(tfidf):\", train) # with open(\"vectorizer.pkl\", \"wb\") as f: # pickle.dump(vectorizer, f) # print(\"vectorizer(tfidf):\", vectorizer) # Apply the same vectorizer on the test documents test = vectorizer.transform(df[\"Content\"]) similarity_matrix1 = cosine_similarity(train, test) #trian 80% and validation 20% print(\"similarity_matrix\",similarity_matrix1.shape) # Calculate mean similarity for each row mean_similarity1 = np.mean(similarity_matrix1, axis=0) # print(mean_similarity1) # print(mean_similarity1.shape) threshold_tfidf = np.percentile(mean_similarity1, 10) print(\"threshold(tfidf):\", threshold_tfidf) # with open(\"threshold(tfidf).pkl\", \"wb\") as f: # pickle.dump(threshold_tfidf, f) # CODE CELL #tfidf(df_train,df_validation) # CODE CELL #w2v(df_train,df_validation)", "source": "Repo:Detecting-Data-Drift-_Automated:train_module.ipynb", "section": "Detecting-Data-Drift-_Automated", "hash": "88a6e74b"}
{"text": "# CODE CELL import pandas as pd from gensim.models import Word2Vec from sklearn.metrics.pairwise import cosine_similarity import numpy as np import matplotlib.pyplot as plt import re import string import nltk import pandas as pd from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer import pickle # CODE CELL import warnings warnings.filterwarnings(\"ignore\") import pandas as pd import os # Step 1: Load the corpus content = [] file_names = [] path = '/mnt/c/Users/alamm9/Desktop/text_file_train_test/train_set' for filename in os.listdir(path): with open(os.path.join(path, filename), 'r') as file: content.append(file.read()) file_names.append(filename) # Step 2: Create a dataframe from the corpus df_train = pd.DataFrame({'Filename': file_names, 'Content': content}) # CODE CELL content = [] file_names = [] path = '/mnt/c/Users/alamm9/Desktop/text_file_train_test/In_sample_test' for filename in os.listdir(path): with open(os.path.join(path, filename), 'r') as file: content.append(file.read()) file_names.append(filename) # Step 2: Create a dataframe from the corpus df_test = pd.DataFrame({'Filename': file_names, 'Content': content}) # CODE CELL content = [] file_names = [] path = '/mnt/c/Users/alamm9/Desktop/text_file_train_test/Out_of_sample_test' for filename in os.listdir(path): with open(os.path.join(path, filename), 'r') as file: content.append(file.read()) file_names.append(filename) # Step 2: Create a dataframe from the corpus df_test_out = pd.DataFrame({'Filename': file_names, 'Content': content}) # CODE CELL df_foreign=create_dataframe_from_files(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake.csv\") # CODE CELL df_foreign = pd.read_csv(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake.csv\") # CODE CELL def preprocess_text(df, column): # Remove special characters and numbers df[column] = df[column].apply(lambda text: re.sub(r'[^a-zA-Z\\s]', '', str(text))) # Convert to lowercase df[column] = df[column].str.lower() # Remove multiple occurrences of 'i' characters df[column] = df[column].apply(lambda text: re.sub(r'(i{2,})', 'i', str(text))) # Remove single alphabets df[column] = df[column].apply(lambda text: re.sub(r'\\b[a-zA-Z]\\b', '', str(text))) # Tokenize the text df[column] = df[column].apply(lambda text: nltk.word_tokenize(text)) # Remove stopwords stop_words = set(stopwords.words('english')) df[column] = df[column].apply(lambda tokens: [token for token in tokens if token not in stop_words]) # Lemmatize the tokens lemmatizer = WordNetLemmatizer() df[column] = df[column].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens]) # Remove 'mmddyyy' df[column] = df[column].apply(lambda text: re.sub(r'(\\b[mmddyyy]+\\b)', '', str(text))) # Remove extra whitespace df[column] = df[column].apply(lambda text: re.sub(r'\\s+', '', str(text))) # Join the tokens back into a string df[column] = df[column].apply(lambda tokens: ''.join(tokens)) return df # CODE CELL df_train = preprocess_text(df_train, \"Content\") #df_test = preprocess_text(df_test, \"Content\") #df_foreign = preprocess_text(df_foreign, \"text\") #df_test_out = preprocess_text(df_test_out, \"Content\") # CODE CELL #df_train[\"Content\"] # CODE CELL from sklearn.model_selection import train_test_split # Splitting the dataset df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=40) # CODE CELL from termcolor import colored # CODE CELL import gensim import numpy as np from sklearn.metrics.pairwise import cosine_similarity from numpy import percentile import os import joblib import h5py def w2v(df_train,df, tfidf=bool):# , percentile_value if (tfidf==0): # Train Word2Vec model model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5, min_count=1, workers=4) # Obtain document embeddings for the training set train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) #trian 80% and validation 20%", "source": "Repo:Detecting-Data-Drift-_Automated:word2vec.ipynb", "section": "Detecting-Data-Drift-_Automated", "hash": "ef808293"}
{"text": "model.wv], axis=0) for doc_tokens in df_train[\"Content\"]] # Transform the test set embeddings using the learned transformation from the training set test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df[\"Content\"]] # Calculate cosine similarity matrix similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) #trian 80% and validation 20% # Calculate mean similarity for each row mean_similarity = np.mean(similarity_matrix, axis=0) threshold = np.percentile(mean_similarity, [10]) else: # Initialize the TF-IDF vectorizer with open(\"vectorizer.pkl\", \"rb\") as f: vectorizer = pickle.load(f) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_unseen[\"Content\"]) with open(\"train(tfidf).pkl\", \"rb\") as f: train = pickle.load(f) with open(\"threshold(tfidf).pkl\", \"rb\") as f: threshold = pickle.load(f) similarity_matrix = cosine_similarity(train, test) #trian 80% and validation 20% print(\"similarity_matrix\",similarity_matrix.shape) # Calculate mean similarity for each row mean_similarity = np.mean(similarity_matrix, axis=0) threshold_tfidf = np.percentile(mean_similarity, 10) #print(\"threshold(tfidf):\", threshold_tfidf) # # Determine data drift and print document information messages = [] for doc_idx, similarity_score in enumerate(mean_similarity): is_drift = similarity_score <= threshold drift_status = \"𝗗𝗮𝘁𝗮 𝗗𝗿𝗶𝗳𝘁\" if is_drift else \"No Data Drift\" similarity_str = \"{:}\".format(similarity_score) message = f\"Document {doc_idx + 1:<4}: Mean Similarity = {similarity_str:<20} {drift_status}\" messages.append(message) # Combine all messages into a single string result = (messages) return result # CODE CELL w2v(df_train,df_test) # CODE CELL # # Plot the heatmap # plt.figure(figsize=(8, 6)) # heatmap = plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest') # plt.colorbar(heatmap) # plt.title('Cosine Similarity Heatmap') # plt.xlabel('Training Set') # plt.ylabel('Test Set') # plt.show() # CODE CELL # # Plot the histogram # plt.figure(figsize=(8, 6)) # plt.hist(cosine_similarity_values, bins=10, color='skyblue') # plt.xlabel('Cosine Similarity') # plt.ylabel('Frequency') # plt.title('Histogram of Cosine Similarity') # plt.show() # CODE CELL from sklearn.feature_extraction.text import TfidfVectorizer import pickle # def tfidf(train, column_train): # # Initialize the TF-IDF vectorizer # vectorizer = TfidfVectorizer() # # Fit the vectorizer on the training documents # train = vectorizer.fit_transform(train[column_train]) # # Save the train variable to a file # with open('train_tfidf.pkl', 'wb') as file: # pickle.dump(train, file) # CODE CELL # tfidf(df_train, \"Content\") # CODE CELL from sklearn.feature_extraction.text import TfidfVectorizer # CODE CELL # Initialize the TF-IDF vectorizer vectorizer = TfidfVectorizer() train = vectorizer.fit_transform(df_train[\"Content\"]) # with open(\"train(tfidf).pkl\", \"wb\") as f: # pickle.dump(train, f) print(\"train(tfidf):\", train) # with open(\"vectorizer.pkl\", \"wb\") as f: # pickle.dump(vectorizer, f) print(\"vectorizer(tfidf):\", vectorizer) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_validation[\"Content\"]) similarity_matrix1 = cosine_similarity(train, test) #trian 80% and validation 20% print(\"similarity_matrix\",similarity_matrix1.shape) # Calculate mean similarity for each row mean_similarity1 = np.mean(similarity_matrix1, axis=0) # print(mean_similarity1) # print(mean_similarity1.shape) threshold_tfidf = np.percentile(mean_similarity1, 10) print(\"threshold(tfidf):\", threshold_tfidf) # with open(\"threshold(tfidf).pkl\", \"wb\") as f: # pickle.dump(threshold_tfidf, f)", "source": "Repo:Detecting-Data-Drift-_Automated:word2vec.ipynb", "section": "Detecting-Data-Drift-_Automated", "hash": "5f7dff9d"}
{"text": "# Detecting-Data-Drift-_Automated # Data Drift Analysis ## Abstract Data drift refers to a shift in the distribution of unseen input data compared to the training data. In the context of deploying a production pipeline, maintaining similarity between processed reports and the trained data is crucial. Analyzing reports from different domains (e.g., radiotherapy) poses a risk of poor quality abstractions, potentially leading to incorrect clinical inferences and risking patient safety. Data drift may also stem from differences in report layouts. This project aims to assess data drift in unseen reports compared to the training dataset. ## Workflow The project primarily involves processing OCR reports, converting them into text files, and conducting operations to detect data drift. The pipeline includes the following steps: 1. **Analysis Module**: Imports `datadrift_module`. 2. **Datadrift Module**: Imports `preprocessing_module`. Utilizes `threshold`, `train`, and `model` from `train_module`. 3. **Train Module**: Calculates thresholds, models, train embeddings, and vectors for TF-IDF and Word2Vec vectorization. ## Steps/Operations This section describes the functions and features of four core modules: ### 1. Train Module This module calculates and saves thresholds, models, train embeddings, and vectors for TF-IDF and Word2Vec vectorization using the training and validation datasets. #### a. Word2Vec Function - Calculates the Word2Vec vectorizer model and train_embeddings. - Computes cosine mean similarity and determines the 10th percentile as the threshold. #### b. TF-IDF Function - Computes TF-IDF vectorization. - Calculates cosine mean similarity and establishes the 10th percentile as the threshold. ### 2. Preprocessing Module This module preprocesses both the training and unseen datasets to prepare the text data for further operations. Customization is possible based on specific requirements. ### 3. Datadrift Module The central module for data drift calculation. The `predict_data_drift` function takes unseen dataset and a boolean result (for TF-IDF or Word2Vec calculation). - Preprocesses the unseen dataset and checks if TF-IDF or Word2Vec calculation is required. - Utilizes previously saved variables (thresholds, vectorizers, train data) from the Train Module to compute data drift. - Notifies if data drift exists between each unseen document and the training document. ### 4. Analysis Module Importing the Datadrift Module, this module calls the `predict_data_drift` function with parameters (i.e., location of the unseen dataset) to analyze and report the results. ## Note Modules can be adapted and customized as needed to fit specific project requirements. The descriptions provide an overview of their responsibilities and intended functionalities.", "source": "Repo:Detecting-Data-Drift-_Automated:README.md", "section": "Detecting-Data-Drift-_Automated", "hash": "94570c78"}
{"text": "# CODE CELL import datadrift_module # CODE CELL datadrift_module.predict_data_drift(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/Out_of_sample_test\") # CODE CELL datadrift_module.predict_data_drift(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/Out_of_sample_test\",tfidf=1)", "source": "Repo:Detecting-Data-Drift-_Automated:analysis_module.ipynb", "section": "Detecting-Data-Drift-_Automated", "hash": "e7b348c8"}
{"text": "# CODE CELL import os import shutil # Set the path to the directory containing the pdf and text sub-folders pdf_data_dir = \"/mnt/c/Users/alamm9/Desktop/pdf_file/\" text_data_dir = \"/mnt/c/Users/alamm9/Desktop/text_file/\" # Get a list of all the files in the train and test sub-folders train_set_pdf_files = os.listdir(os.path.join(pdf_data_dir, \"train_set_pdf_files\")) In_sample_test = os.listdir(os.path.join(pdf_data_dir, \"In_sample_test\")) Out_of_sample_test = os.listdir(os.path.join(pdf_data_dir, \"Out_of_sample_test\")) text_files = os.listdir(os.path.join(text_data_dir)) # Path to directory for train and test text files trainSet = \"/mnt/c/Users/alamm9/Desktop/text_file_train_test/train_set\" InSample_test = \"/mnt/c/Users/alamm9/Desktop/text_file_train_test/In_sample_test\" OutSample_test = \"/mnt/c/Users/alamm9/Desktop/text_file_train_test/Out_of_sample_test\" for filename in text_files: if filename.endswith(\"_ocrtext.txt\"): file_key = filename.split('_ocrtext.txt')[0] pdf_filename =file_key +'.pdf' txt_filename = filename if pdf_filename in train_set_pdf_files: shutil.copy(text_data_dir+'/'+txt_filename , trainSet + '/'+ txt_filename ) elif pdf_filename in In_sample_test: shutil.copy(text_data_dir+'/'+txt_filename , InSample_test + '/'+ txt_filename ) elif pdf_filename in Out_of_sample_test: shutil.copy(text_data_dir+'/'+txt_filename , OutSample_test + '/'+ txt_filename )", "source": "Repo:Detecting-Data-Drift-_Automated:Data Drift Data Preprocessing.ipynb", "section": "Detecting-Data-Drift-_Automated", "hash": "a0e6ed6d"}
{"text": "# CODE CELL import os import re import gensim import string import nltk import pickle import numpy as np import pandas as pd from numpy import percentile import matplotlib.pyplot as plt from gensim.models import Word2Vec from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer from sklearn.metrics.pairwise import cosine_similarity # CODE CELL import preprocessing_module # CODE CELL def predict_data_drift(unseen_report, tfidf=bool): # Create a dataframe for the unseen report using the create_dataframe function content = [] file_names = [] for filename in os.listdir(unseen_report): with open(os.path.join(unseen_report, filename), 'r') as file: content.append(file.read()) file_names.append(filename) df_unseen = pd.DataFrame({'Filename': file_names, 'Content': content}) # Preprocess the text in the dataframe using the preprocess_text function df_unseen = preprocessing_module.preprocess_text(df_unseen, 'Content') if (tfidf==1): print(\"This is TF-IDF Calculation\") # Initialize the TF-IDF vectorizer with open(\"vectorizer.pkl\", \"rb\") as f: vectorizer = pickle.load(f) # Apply the same vectorizer on the test documents test = vectorizer.transform(df_unseen[\"Content\"]) # Initialize train vectorizer with open(\"train(tfidf).pkl\", \"rb\") as f: train = pickle.load(f) # Initialize threshold vectorizer with open(\"threshold(tfidf).pkl\", \"rb\") as f: threshold = pickle.load(f) # Calculate Cosine Similarity similarity_matrix = cosine_similarity(train, test) # Calculate mean similarity for each row (i.e.; via test dataset) mean_similarity = np.mean(similarity_matrix, axis=0) else: print(\"This is Word2Vec Calculation\") # Initialize the train embeddings with open(\"train_embeddings(w2v).pkl\", \"rb\") as f: train_embeddings = pickle.load(f) # Initialize the threshold with open(\"threshold(w2v).pkl\", \"rb\") as f: threshold = pickle.load(f) # Initialize the Word2Vec Embedding with open(\"model.pkl\", \"rb\") as f: model = pickle.load(f) # Applying same Word2Vec Embedding on the test document unseen_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_unseen[\"Content\"]] # Calculate cosine similarity similarity_matrix = cosine_similarity(train_embeddings, unseen_embeddings) # Calculate mean similarity for each row (i.e.; via test dataset) mean_similarity = np.mean(similarity_matrix, axis=0) # Determine data drift and print document information messages = [] for doc_idx, similarity_score in enumerate(mean_similarity): is_drift = similarity_score <= threshold drift_status = \"𝗗𝗮𝘁𝗮 𝗗𝗿𝗶𝗳𝘁\" if is_drift else \"No Data Drift\" similarity_str = \"{:}\".format(similarity_score) message = f\"Document {doc_idx + 1:<4}: Mean Similarity = {similarity_str:<20} {drift_status}\" messages.append(message) result = (messages) return result", "source": "Repo:Detecting-Data-Drift-_Automated:datadrift_module.ipynb", "section": "Detecting-Data-Drift-_Automated", "hash": "09218f9c"}
{"text": "MCO008-051008 Data Analytics in Supply Chain Management Fall 2022 Prof. Dr.-Ing. Hendro Wicaksono Group Project Work Group Members 1 Abhishek Mathur 30006043 2 Mehr un Nisa 30005960 3 Meriel Wanyonyi 30006061 4 Mohammad Tanzil Alam 30006050 Date: 09thDecember 2022 Table of Contents MCO008-051008 Data Analytics in Supply Chain Management Fall 2022 ............................... I Prof. Dr.-Ing. Hendro Wicaksono................................................................................................ I Table of Contents .......................................................................................................................II List of Figures .......................................................................................................................... IV List of Tables ............................................................................................................................. V 1 Supply Chain Scenario 1 : Online Retail .................................................................. 1 1.1 Scenario Description.............................................................................................................. 1 1.2 Dataset Description................................................................................................................ 2 1.3 Exploratory Data Analysis ..................................................................................................... 2 1.4 Data Preprocessing ................................................................................................................ 5 1.5 Model Building and Result Description ................................................................................ 7 1.6 Discussion ............................................................................................................................ 14 1.7 Business and Technological Implication ............................................................................. 14 1.7.1 Business Implications .......................................................................................................... 14 1.7.2 Business Disruptions ........................................................................................................... 15 1.8 Business Solutions ............................................................................................................... 16 1.9 Business Model Canvas ....................................................................................................... 17 2 Supply Chain Scenario 2: DataCo SupplyChain Dataset ........................................ 18 2.1 Scenario Description............................................................................................................ 18 2.2 Dataset Description.............................................................................................................. 18 2.3 Exploratory Data Analysis ................................................................................................... 21 2.4 Data Preprocessing .............................................................................................................. 25 2.5 Model Building and Result Description .............................................................................. 27 2.6 Discussion ............................................................................................................................ 32 2.7 Business and Technological Implication ............................................................................. 32 2.8 Business Challenges ............................................................................................................ 33 2.9 Business Model Canvas ....................................................................................................... 34 3 Supply Chain Scenario 3: CNC-Milling Machine_Production data ....................... 35 3.1 Scenario Description............................................................................................................ 35 3.2 Dataset Description.............................................................................................................. 35 3.2.1 Dependent Variables: Processing time and average power consumption ........................ 36 3.3 Exploratory Data Analysis ................................................................................................... 36 3.4 Data Preprocessing .............................................................................................................. 44 3.5 Model Building and Result Description .............................................................................. 44 3.6 Discussion ............................................................................................................................ 48 3.7 Business and Technological Implication ............................................................................. 49 3.7.1 Business Disruptions: .......................................................................................................... 49 4 Conclusions Future Outlook, and Reflection .......................................................... 50 Bibliography ............................................................................................................................. 51", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "d6dd354b"}
{"text": "power consumption ........................ 36 3.3 Exploratory Data Analysis ................................................................................................... 36 3.4 Data Preprocessing .............................................................................................................. 44 3.5 Model Building and Result Description .............................................................................. 44 3.6 Discussion ............................................................................................................................ 48 3.7 Business and Technological Implication ............................................................................. 49 3.7.1 Business Disruptions: .......................................................................................................... 49 4 Conclusions Future Outlook, and Reflection .......................................................... 50 Bibliography ............................................................................................................................. 51 List of Figures Figure 1. 10 Rows..................................................................................................................................................... 3 Figure 2. 10 customers............................................................................................................................................ 3 Figure 3. Bottom five countries ................................................................................................................................ 4 Figure 4. Seasonality ................................................................................................................................................ 4 Figure 5. Weekly Sales ............................................................................................................................................. 5 Figure 6. Data Preporcessing .................................................................................................................................... 5 Figure 7. Preporcessed Data ..................................................................................................................................... 6 Figure 8. No data with negative columns ................................................................................................................. 6 Figure 9. Dataset Info ............................................................................................................................................... 6 Figure 10. Dataset Info ............................................................................................................................................. 7 Figure 11 : RFM values calculated per customer ..................................................................................................... 9 Figure 12 RFM score(1-4) calculated per customer ............................................................................................... 11 Figure 13: Loyalty Levels defined ......................................................................................................................... 12 Figure 14: Bar Graph for count of customers as per Loyalty Level ....................................................................... 12 Figure 15: Bar Graph for count of customers as per Loyalty Level ....................................................................... 13 Figure 16: Graph for Elbow method ...................................................................................................................... 13 Figure 17 Kmeans clusters as result ....................................................................................................................... 14 Figure 18 Bussiness Model Canvas of Online Retail ............................................................................................. 17 Figure 19 ................................................................................................................................................................. 21 Figure 20:Correlation Heatmap .............................................................................................................................. 22 Figure 21: Box plot of the variables of the dataset ................................................................................................. 23 Figure 22: Histogram plot of each variable ............................................................................................................ 24 Figure 23: Scatter plot for sales and late delivery .................................................................................................. 25 Figure 24 : Category library ................................................................................................................................... 27 Figure 25: Confusion Matrix Logitstic Regression .............................................................................................. 28 Figure 26: XG Booster ........................................................................................................................................... 30 Figure 27: Kernel SVM Confusion Matrix ............................................................................................................ 31 Figure 28 Bussiness Model Canvas ........................................................................................................................ 34 Figure 29: Statistic about the data type of the dataset. ........................................................................................... 37 Figure 30: Statistic about missing values ............................................................................................................... 38 Figure 31: Correlation heatmap of every variable. ................................................................................................. 39 Figure 32: Scatter plot of the target variables against some predictors. ................................................................. 40 Figure 33: Box plot of the independent and dependent variables. ......................................................................... 41 Figure 34: Histogram .............................................................................................................................................. 42 Figure 35: line plot to see evolution of average power consumption ..................................................................... 42 Figure 36: Bar Plot between average power consumption and processing time .................................................... 43 Figure 37: Average power consumption over time for the same material of 136659600 mm3 ............................. 43 Figure 38: Feature Importance ............................................................................................................................... 47 List of Tables Table 1: Data set description of Online Retail .......................................................................................................... 2 Table 2: Reveal the output after encoding. ............................................................................................................. 27 Table 3 .................................................................................................................................................................... 30 Table 4: Kernel SVM Results ................................................................................................................................. 31 Table 5: Comparison Table ..................................................................................................................................... 31 Table 6 CNC Machine dataset ................................................................................................................................ 35 Table 7: Different type of performance Metrics for Random Forest ...................................................................... 45 Table 8: Different type of performance Metrics for Decision Tree ........................................................................ 46 Table 9: Different type of performance Metrics for Support Vector ...................................................................... 46 Table 10: Different type of performance Metrics for all models after cleaning ..................................................... 47 Table 11: Different type of performance Metrics for all models before cleaning .................................................. 48", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "17c7a1be"}
{"text": "Different type of performance Metrics for Decision Tree ........................................................................ 46 Table 9: Different type of performance Metrics for Support Vector ...................................................................... 46 Table 10: Different type of performance Metrics for all models after cleaning ..................................................... 47 Table 11: Different type of performance Metrics for all models before cleaning .................................................. 48 DASCM_Report_Group 05 1 1 Supply Chain Scenario 1 : Online Retail This is the model of the online retail. It consists of different sectors to ensure smooth run of buying, selling and delivering of products to end consumers.an online retail consists of key partners who are the online sellers and transportation companies to ensure delivery of demanded items to respective consumers. Day and time of demand can be an indicator ,the earliest date taken into consideration and deliver fast. Key activities are receivig all orders from customers and ensure efficient delivery to them.the channels to be used are mostly social media shopping apps and websites, consumers are taken into consideration to ensure their satisfaction is met. The dataset would be analysed to check the demand of the product and improve where necessary. 1.1 Scenario Description Online retail deals with online sale of products according to their demand rate. The demand is focused on country wise demand, month and day of demand. Our main objectives are customer retention for customers who demand more. This can be manifested through gifts vouchers, a customer will be willing to purchase more to get the benefits associated with it. As every business every customer demands differently, there are the low demanding population. Marketing campaign through advertisement to educate people on the product, discounts can be offered for specific purchases. Businesses have an aim of a profit, and this can be boosted by more sales to customers. To understand this a dataset of online retail of 13 months was analysed to see its relevance. It was analysed on three main aspects frequency, recency and monetary. Frequency is the count of Invoice number of transactions carried out for the 13 months, monetary is the sum of total sales while recency is the difference between latest date and last invoice date. DASCM_Report_Group 05 2 1.2 Dataset Description Table 1: Data set description of Online Retail 1.3 Exploratory Data Analysis The is having 541909 rows and 10 columns before pre processing. fields description Invoice number Number assigned to customers to track payments. Invoice Date Represents time stamped time and date goods have been billed. Invoice Time Period which invoices are to be issued Stock Code It is an abbreviation that identifies a particular security on a product. Description It shows what the product is made up of. Quantity It shows the amount ordered. Unit Price It is the price of each amount in certain unit ordered Total sales It is the summation of product of unit sales and unit prices of each item. Customer ID It is the unique means of identification allocated by customer in relation of services offered. Country It shows countries demand of particular products.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "73d23153"}
{"text": "price of each amount in certain unit ordered Total sales It is the summation of product of unit sales and unit prices of each item. Customer ID It is the unique means of identification allocated by customer in relation of services offered. Country It shows countries demand of particular products. DASCM_Report_Group 05 3 Figure 1. 10 Rows Finding the top 10 customers who are recurrently purchase stuffs from the online retail. Figure 2. 10 customers Bottom five countries having maximum customer counts. DASCM_Report_Group 05 4 Figure 3. Bottom five countries Plot of different months, indicating most of the sales is done in October and November and least is on January, February and December. Figure 4. Seasonality Sales count on different days. DASCM_Report_Group 05 5 Figure 5. Weekly Sales 1.4 Data Preprocessing For further execution of the project, we have joined two columns and merged them into one that is “InvoiceDate” and then we have to change the dtype of them to datetime. Then we have drop the column InvoiceTime, and extract the month and day out of the newly created InvoiceDate Figure 6. Data Preporcessing DASCM_Report_Group 05 6 After preprocessing we are left with 406829 rows and 11 columns Figure 7. Preporcessed Data Removing the negative columns from the Quantity to assuming that there is no negative quantity. Figure 8. No data with negative columns Figure 9. Dataset Info DASCM_Report_Group 05 7 Figure 10. Dataset Info 1.5 Model Building and Result Description 1.5.1 RFM RFM is a technique which is used for analysing customer value. stands for Recency, Frequency and Monetary. It is usually used in direct marketing and database marketing and has received specific attention in retail and professional services industries. RMF stands for: Recency- How recently did the customer purchase? Frequency- How often do they purchase? Monetary- How much do they spend? Customer purchases is represented by our dataset table with columns for the customer ID, date of purchase, Invoice Number and purchase value. One approach to RFM is to assign a score for each dimension on a scale from 1 to 10. In our approach, scale from 1-4 is chosen where 1 being the excellent value and 4 being the bad value. The minimum score represents the preferred behaviour, and a formula could be used to calculate the three scores for each customer. For example, a service-based business could use these calculations:", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "10c7115f"}
{"text": "In our approach, scale from 1-4 is chosen where 1 being the excellent value and 4 being the bad value. The minimum score represents the preferred behaviour, and a formula could be used to calculate the three scores for each customer. For example, a service-based business could use these calculations: DASCM_Report_Group 05 8 I) Recency = Latest date(2011, 12, 10) – Last Purchase date for each customer. II) Frequency = the number of purchases by the customer in the last 12 months. III) Monetary = the sum of values of all purchases by each customer. Alternatively, categories can be defined for each attribute. For instance, Recency might be broken into three categories: customers with purchases within the last 90 days; between 91 and 365 days; and longer than 365 days. Such categories may be derived from business rules or using data mining techniques to find meaningful breaks. To create RFM Modelling scores for each customer, following steps were taken: I) Recency is calculated as Latest Date subtracting Last Inovice Data. Latest date is taken as ‘2011-12-10’ as last invoice date was 2011-12-09. This is to calculate the number of days from recent purchase. II) Frequency is calculated as count of invoice no. of transaction(s). III) Monetary is calculated as Sum of Total amount of purchase by each customer. First, converted the Latest_Date column to datetime datatype. Latest_Date = dt.datetime(2011,12,10) rfm_normalized = df.groupby('CustomerID').agg({'InvoiceDate': lambda x: (Latest_Date - x.max()).days, 'InvoiceNo': lambda x: len(x), 'Totalsale': lambda x: x.sum()}) Converted Invoice Date into type int. rfm_normalized['InvoiceDate'] = rfm_normalized['InvoiceDate'].astype(int). For simplicity, we have renamed column names to Recency, Frequency and Monetary and then normaloized the result. rfm_normalized.rename(columns={'InvoiceDate': 'Recency', 'InvoiceNo': 'Frequency', 'Totalsale': 'Monetary'}, inplace=True) rfm_normalized=rfm_normalized.reset_index() In below screenshot, we can see the resulted RFM values for each customer. DASCM_Report_Group 05 9 Figure 11 : RFM values calculated per customer As per the result above, we observed there were still some abnormalities in the dataset that needed to be treated. In recency, values are calculated as subtraction of last purchase date from the lasted date. That can be either zero or more than zero. However, some values were coming as negative values. Hence, we decided to delete those data because such data were very less. Same assumption and strategy were applied for monetary. m=rfm_normalized[rfm_normalized['Monetary']<=0].index rfm_normalized.drop(m , inplace=True) mm=rfm_normalized['Monetary']<=0 mm.sum() r=rfm_normalized[rfm_normalized['Recency']<0].index rfm_normalized.drop(r , inplace=True) rr=rfm_normalized['Recency']<0 rr.sum() So as per the above logic, values below zero were eliminated from both Recency and Monetary. 1.5.2 RFM Model Analysis To analyse the RFM features, we decided to divide them into quantiles and to label them as 1- 4 score. So, we split the data into four segment using Quantile as below. quantile = rfm_normalized.quantile(q = [0.25,0.50,0.75]) quantile = quantile.to_dict()", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "48772995"}
{"text": "eliminated from both Recency and Monetary. 1.5.2 RFM Model Analysis To analyse the RFM features, we decided to divide them into quantiles and to label them as 1- 4 score. So, we split the data into four segment using Quantile as below. quantile = rfm_normalized.quantile(q = [0.25,0.50,0.75]) quantile = quantile.to_dict() DASCM_Report_Group 05 10 quantile { 'CustomerID': {0.25: 13817.0, 0.5: 15303.0, 0.75: 16776.0}, 'Recency': {0.25: 22.0, 0.5: 63.0, 0.75: 163.0}, 'Frequency': {0.25: 17.0, 0.5: 41.0, 0.75: 99.0}, 'Monetary': {0.25: 305.78, 0.5: 663.65, 0.75: 1625.0500000000002} } Now here, for each customer, we have received quantile. Next, we have given them score. Lower the recency score, it is assumed to be good for the company because the customer has recently purchased something. def RScoring(x,p,d): if x <= d[p][0.25]: return 1 elif x <= d[p][0.50]: return 2 elif x <= d[p][0.75]: return 3 else: return 4 Higher value of frequency and monetary lead to a good consumer. Hence, here we given higher value as 1 in reverse way. def FnMScoring(x,p,d): if x <= d[p][0.25]: return 4 elif x <= d[p][0.50]: return 3 elif x <= d[p][0.75]: return 2 else: return 1 DASCM_Report_Group 05 11 Once we have got the RFM score for each customer, we have calculated and added R,F and M segments values columns in the existing dataset to show R,F,M segment values. rfm_normalized[\"R\"] = rfm_normalized['Recency'].apply(RScoring,args=('Recency',quantile,)) rfm_normalized[\"F\"] = rfm_normalized['Frequency'].apply(FnMScoring,args=('Frequency',quantile,)) rfm_normalized[\"M\"] = rfm_normalized['Monetary'].apply(FnMScoring,args=('Monetary',quantile,)) Below figure shows us the result using 5 rows from dataset. Figure 12 RFM score(1-4) calculated per customer DASCM_Report_Group 05 12 To find loyalty level of each customer, we have used RFM scores. We have added RFM scores of recency, frequency and Monetary for each customer and created a new field for that. As per assumption, we have given importance to each recency, frequency, and monetary values. So as per logic create in above sections, if the total score comes less (minimum is 1+1+1 = 3), mean that customer is the most loyal one. Similarly, if the customer gets total RFM score of 12(maximum 4+4+4 = 12), mean that customer is of least important for the product companies. Also, we have assigned each customer with Labels as Platinum, Gold, Silver and Bronze. Below is the resulted screenshot of the dataset for 5 customers. Figure 13: Loyalty Levels defined Below Graph shows the classification of customers on the basic of the four labels. Figure 14: Bar Graph for count of customers as per Loyalty Level", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "70b4e012"}
{"text": "with Labels as Platinum, Gold, Silver and Bronze. Below is the resulted screenshot of the dataset for 5 customers. Figure 13: Loyalty Levels defined Below Graph shows the classification of customers on the basic of the four labels. Figure 14: Bar Graph for count of customers as per Loyalty Level DASCM_Report_Group 05 13 It is important for any retail market to know their top best customers. So below we can see the top 10 customers that belongs to the Platinum category. Figure 15: Bar Graph for count of customers as per Loyalty Level 1.5.3 KMeans Clustering Before implementing the Kmeans Clustering algorithm we need to decide the number of clusters to put inside algorithm as input. So, we will be finding the minimum number of clusters required by using Elbow method. After applying Elbow Method on Recency, Frequency and Monetary, below graph is what we get. And we can clearly see that an elbow is forming at point 3. So we will take that as our Kmeans values. Figure 16: Graph for Elbow method DASCM_Report_Group 05 14 Once the algorithm is performed, below clusters we get. Figure 17 Kmeans clusters as result As a result, a trend can be seen in the 3 respective clusters. Cluster 2 with low recency values, high frequency and monetary values is assumed to be the best cluster of customers. Whereas cluster 0 and 1 have comparatively high recency and low frequency and monetary values. 1.6 Discussion As per the results and assumptions we had, taking all features of RFM into account, we have segregated the customers. With the help of the results, important and least important customers can be identified. Based on that, company can decide to lure them with gifts/discounts to reduce the churn rate. Using Kmeans cluster, we have divided customers into 3 clusters and based on it, business can decide varies strategies to retain its customers. 1.7 Business and Technological Implication 1.7.1 Business Implications It is an effect of a policy or action that will have on operations and financial wellbeing of a Company. Online retail dataset noticed some of the activities which had shown its operations. They are as follows; Recency and frequency have an effect on monetary gains.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "97aec33b"}
{"text": "Implication 1.7.1 Business Implications It is an effect of a policy or action that will have on operations and financial wellbeing of a Company. Online retail dataset noticed some of the activities which had shown its operations. They are as follows; Recency and frequency have an effect on monetary gains. DASCM_Report_Group 05 15 The least recent and least frequent customer spend more. Customer ID 12346.0 has a recency of 325 and frequency of 1 but the spending is 77183.60. Comparing it to customer ID 12347.0 has a frequency of 182 and recency of 39 ,the monetary level is at 4310.0. Combination of recency , monetary and, frequency assign loyalty levels to customers Different customers have different purchase levels and through calculation of their recency, monetary, and frequency, loyalty levels are assigned accordingly. The loyalty levels is categorized into bronze, silver, gold and platinum. 1.7.2 Business Disruptions This is the process by which a product becomes popular enough to replace a traditional or common product and services. Online retail has easened a lot of manpower with necessarily going to stores. This can be shown as follows; ➢ Online Shopping This is whereby consumers directly buy goods, services over the Internet. Consumers visit different social apps from the comfort of their house. A variety is provided to cater different demands, this saves time that could have been used to go to online stores. ➢ On demand delivery Customers have a choice to decide not just where they would like their products to be delivered but also when. This happens within the shortest delivery time possible. ➢ E-commerce businesses After a consumer knows what he or she wants ,transfer of money and data is done for the transaction to be complete.Cancellation of orders can be made swiftly in case of consumer changes. There exist three types of E-commerce like business to business, business to consumer and consumer to consumer. Because of such services faster buying process, affordability in advertising and marketing, flexibility for customers , products and prices can be compared. The buyer and market demands are quickly responded to. ➢ Personalisation and individuality.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "d37202f7"}
{"text": "three types of E-commerce like business to business, business to consumer and consumer to consumer. Because of such services faster buying process, affordability in advertising and marketing, flexibility for customers , products and prices can be compared. The buyer and market demands are quickly responded to. ➢ Personalisation and individuality. DASCM_Report_Group 05 16 Consumers are always considered right and a business tries to satisfy what is demanded. This is the right quality,right quantity, right time and right price. Customers feedback is taken into consideration as it reflects the progress of the business. 1.8 Business Solutions ➢ Gift vouchers for regular customers From analysis of the dataset cluster 2 has the highest monetary returns of 121841.620000, recency of 18.818182 and a frequency of 1853.818182. Since it has the highest monetary returns than cluster 0 and 1, gift vouchers can be given to the customers falling under that cluster to retain them and gain more revenue. ➢ Advertisement and discounts for less demanding customers and churn rate reduction. Customers falling under cluster 0 have the lowest monetary returns .the business cannot loose on these customers so to boost their sales, more advertisement can be done for more product knowledge, satisfactory discounts can be offered for a specific purchase. Customers would spend more knowing the benefits associated with it. The churn rate is the rate at which customers stop doing business with a company over a given period of time. It happens mostly for subscription goods where consumers end it after expiry of free trials. Subscription fees can be made cheaper for customers to ensure longer stay, the lower the churn rate the higher the profits, sales and growth of a company. ➢ Aftersales Services These are supports provided to customers after purchase of products or services. This is a business strategy as it leads to brand loyalty, customer satisfaction and a possibility of reaching bigger markets.after sales services include warranty information, product or service training, repairs and upgrades, return and exchange policy details , issue of coupons and surveys. DASCM_Report_Group 05 17 1.9 Business Model Canvas Figure 18 Bussiness Model Canvas of Online Retail", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "ddb1c3f4"}
{"text": "to brand loyalty, customer satisfaction and a possibility of reaching bigger markets.after sales services include warranty information, product or service training, repairs and upgrades, return and exchange policy details , issue of coupons and surveys. DASCM_Report_Group 05 17 1.9 Business Model Canvas Figure 18 Bussiness Model Canvas of Online Retail DASCM_Report_Group 05 18 2 Supply Chain Scenario 2: DataCo SupplyChain Dataset 2.1 Scenario Description DataCo SupplyChain is an e-commerce company deals with the products related to Footwear, apparel, fitness, fan shop, sports items, and electronics. The customers market for this company is global which means people from all over the world are ordering from this company. The company is facing two major issues, in order to tackle them the below mentioned data set will be analysed. 2.1.1 In order to improve sales in the months where the sale frequency is low. With the help of available data of three years ,firstly, the identification of the months with lower sales are required. Secondly the ideal time for advertisement has to be identified in order to boost the sales. 2.1.2 The company is facing an issue of late deliveries and in order to tackle the problem firstly , the identification of the factors which are causing late deliveries has to identified. Secondly, the decision for reducing the late delivery risk has to be taken. 2.2 Dataset Description Table 2:Description of the dataset from DataCo SupplyChain(Constante et al., 2019) FIELDS DESCRIPTION Type Type of transaction made Days for shipping (real) Actual shipping days of the purchased product Days for shipment (scheduled) Days of scheduled delivery of the purchased product Benefit per order Earnings per order placed Sales per customer Total sales per customer made per customer Delivery Status Delivery status of orders: Advance shipping , Late delivery , Shipping canceled , Shipping on time Late_delivery_risk Categorical variable that indicates if sending is late (1), it is not late (0).", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "4cfb7f56"}
{"text": "Benefit per order Earnings per order placed Sales per customer Total sales per customer made per customer Delivery Status Delivery status of orders: Advance shipping , Late delivery , Shipping canceled , Shipping on time Late_delivery_risk Categorical variable that indicates if sending is late (1), it is not late (0). DASCM_Report_Group 05 19 Category Id Product category code Category Name Description of the product category Customer City City where the customer made the purchase Customer Country Country where the customer made the purchase Customer Email Customer's email Customer Fname Customer name Customer Id Customer ID Customer Lname Customer lastname Customer Password Masked customer key Customer Segment Types of Customers: Consumer , Corporate , Home Office Customer State State to which the store where the purchase is registered belongs Customer Street Street to which the store where the purchase is registered belongs Customer Zipcode Customer Zipcode Department Id Department code of store Department Name Department name of store Latitude Latitude corresponding to location of store Longitude Longitude corresponding to location of store Market Market to where the order is delivered : Africa , Europe , LATAM , Pacific Asia , USCA Order City Destination city of the order Order Country Destination country of the order Order Customer Id Customer order code order date (DateOrders) Date on which the order is made Order Id Order code Order Item Cardprod Id Product code generated through the RFID reader Order Item Discount Order item discount value Order Item Discount Rate Order item discount percentage Order Item Id Order item code Order Item Product Price Price of products without discount Order Item Profit Ratio Order Item Profit Ratio DASCM_Report_Group 05 20 Order Item Quantity Number of products per order Sales Value in sales Order Item Total Total amount per order Order Profit Per Order Order Profit Per Order Order Region Region of the world where the order is delivered : Southeast Asia ,South Asia ,Oceania ,Eastern Asia, West Asia , West of USA , US Center , West Africa, Central Africa ,North Africa ,Western Europe ,Northern , Caribbean , South America ,East Africa ,Southern Europe , East of USA ,Canada ,Southern Africa , Central Asia , Europe , Central America, Eastern Europe , South of USA Order State State of the region where the order is delivered Order Status Order Status : COMPLETE , PENDING , CLOSED , PENDING_PAYMENT ,CANCELED , PROCESSING ,SUSPECTED_FRAUD ,ON_HOLD ,PAYMENT_REVIEW Product Card Id Product code Product Category Id Product category code Product Description Product Description Product Image Link of visit and purchase of the product Product Name Product Name Product Price Product Price Product Status Status of the product stock :If it is 1 not available , 0 the product is available Shipping date (DateOrders) Exact date and time of shipment Shipping Mode The following shipping modes are presented : Standard Class , First Class , Second Class , Same Day", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "10f9398f"}
{"text": "Product Price Product Status Status of the product stock :If it is 1 not available , 0 the product is available Shipping date (DateOrders) Exact date and time of shipment Shipping Mode The following shipping modes are presented : Standard Class , First Class , Second Class , Same Day DASCM_Report_Group 05 21 2.2.1 Dependent Variable: Late Delivery Risk 2.2.2 Independent Variable: • Days for shipping (real) • Days for shipment (scheduled) • Order Country Customer Id • Product Card Id • Product Price • Shipping Mode • Order Item Quantity 2.3 Exploratory Data Analysis Figure 19 Before building any model it’s important to analyze the raw data and make appropriate changes if it needed. In the next paragraphs, we will review more in details those steps. DASCM_Report_Group 05 22 Our dataset contains 180519 transactions using different type of payment and 53 variables. Most of the variables are non-numerical and the rest are in float and integer data type. To continue we checked if there were any missing values. In Fig 1. we present that information in percentage. It is noticed that 2 variables Customer Zipcode and Customer Lname have less than 0% of missing data points meanwhile Order Zipcode and Product Description present a high number of nonexistent records 86% and 100% correspondently. We wanted to see how correlated each variable were between them, most importantly with our target attribute. To analyze that, we plotted a correlation heatmap (See Fig 2.). We couldn’t find any feature that has a strong correlation with our target. Apart from Days for shipping (real) all of them possess a low relationship. Other interesting detail we noticed is that some variables got a perfect positive relationship. Meaning that they showed similar values. Based on a Box plot (See Fig 3.) some variables such as Benefit per order, Sales per customer, Order Item Profit Ratio exhibit a high amount of outliers while Customer Id, Order customer Id, Department Id, sales have a low amount. Figure 20:Correlation Heatmap DASCM_Report_Group 05 23 In order to see the distribution for each variable we plotted a histogram (See Fig 4.). Analyzing each plot, we could identify that some variables are right-skewed meaning that the extreme values affect the mean more than the median (Sales per customer, Order Item Discount, Order Item Total). Also, there were variables that are left-skewed, the median is more affected by the extreme values than the mean (Benefit per order, Order Item Profit Ratio, Order Profit Per Order). For others the distribution is approximately symmetrical meaning that the mean and median are equal. This is the case for Order Item Id which reveal a uniform distribution because there’s no peak. Figure 21: Box plot of the variables of the dataset", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "08c93c0a"}
{"text": "Order Item Profit Ratio, Order Profit Per Order). For others the distribution is approximately symmetrical meaning that the mean and median are equal. This is the case for Order Item Id which reveal a uniform distribution because there’s no peak. Figure 21: Box plot of the variables of the dataset DASCM_Report_Group 05 24 Figure 22: Histogram plot of each variable To continue with the process of exploratory analysis, we went with a scatter chart to show the linear relation between our target and 3 other variables also between Sales and other variables. we can observe that (See Fig 5.) Product Price, Order Item Product Price and Order Item Discount have a positive linear relation with Sales. However, when we look at Late_delivery_risk there’s no clear linear relation with the variables, because it doesn’t cover the data points in the regression line. DASCM_Report_Group 05 25 2.4 Data Preprocessing Now that we were done with the analysis, it was time to clean the dataset. First, we started with the missing values. As stated before, Customer Zipcode and Customer Lname comprise of a small number of missing values so we chose to drop the rows where those values were missing. Being that there were so many data points in Order Zipcode and Product Description both columns were dropped. Product Status only had one unique value across samples, it was removed because it was non- informative and could have break the models. Looking back at our heatmap graph (See Fig 23.) all the variables that have a Pearson correlation of 1 are identical (see the list below). So, in order to avoid harmful bias and make the learning algorithm faster we drop one between those 2 predictors and kept the other one. Figure 23: Scatter plot for sales and late delivery", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "5717ff34"}
{"text": "all the variables that have a Pearson correlation of 1 are identical (see the list below). So, in order to avoid harmful bias and make the learning algorithm faster we drop one between those 2 predictors and kept the other one. Figure 23: Scatter plot for sales and late delivery DASCM_Report_Group 05 26 Looking back at our heatmap graph (See Fig 23.) all the variables that have a Pearson correlation of 1 are identical (see the list below). So, in order to avoid harmful bias and make the learning algorithm faster we drop one between those 2 predictors and kept the other one. Benefit_per_order and Order_Profit_Per_Order Sales_per_customer and Order_Item_Total Category_Id and Product_Category_Id Customer_Id and Order_Customer_Id Order_Item_Cardprod_Id and Product_Card_Id Order_Item_Product_Price and Product_Price We also renamed the following columns order date (DateOrders) shipping date (DateOrders) to Order Date and Shipping Date respectively just to make simpler to read and being that they were considered for the independent variables. Being done with the cleaning part it was time to select the independent and dependent variables in order to build the model. For our model we took Days for shipping (real), Days for shipment (scheduled), Order Country, Customer Id, Product Card Id, Product Price, Shipping Mode , Order Item Quantity, Type, as independent variables and Late_delivery_risk as the target variable. As you can see some variables are categorical and for the machine learning to build the model those variables need to be converted in numerical. In order to achieve that we used target encoding. This method consists in converting the categorical data points into the mean of the target variable in our case Late_delivery_risk. For each category it calculates the mean of the target variable and the value obtained replaced the categorical data point. Furthermore, we used Category library to realize that as shown below. DASCM_Report_Group 05 27 Figure 24 : Category library Table 2: Reveal the output after encoding. id Days for shipping (real) Days for shipment (scheduled) Order Country Customer Id Product Card Id Product Price Shipping Mode Order Item Quantity Type Late_delivery_risk 0 3 4 0.553784 20755 1360 327.75 0.380723 1 0.572175 0 1 5 4 0.561271 19492 1360 327.75 0.380723 1 0.485425 1 2 4 4 0.561271 19491 1360 327.75 0.380723 1 0.566323 0 3 3 4 0.542897 19490 1360 327.75 0.380723 1 0.572175 0 4 2 4 0.542897 19489 1360 327.75 0.380723 1 0.575332 0 2.5 Model Building and Result Description At this stage we separate the dependent and independent variables in X and y. We split the dataset into training and testing set, using Sklearn library. We considered 75% of the dataset for training and the rest for testing. # Splitting the dataset into the Training set and Test set from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0) We started with multiple logistic regression algorithm; we first scaled the data using Sklearn then proceed to run the model into the training set. Then we predicted the test set.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "cab0faf6"}
{"text": "and Test set from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0) We started with multiple logistic regression algorithm; we first scaled the data using Sklearn then proceed to run the model into the training set. Then we predicted the test set. DASCM_Report_Group 05 28 In order to evaluate the model, we used 5 different types of metrics which are: Accuracy, Precision, Recall, F1 score and the Area Under Curve. Figure 25: Confusion Matrix Logitstic Regression Most of those metrics can also be obtained from the confusion matrix (See fig 28) There are two class labels: late and not late. The x-axis is labeled Predicted label - meaning our model's predictions given X_test. The y-axis is labeled Actual label - meaning the class labels for our X_test observations. The model made a total of 45127 predictions (equivalent to 24777+19268+0+1082) on if those 45127 shipments should be late or not Out of the 45127 predictions, 19268 (equivalent to 0 + 19268) were for the late class. Out of the 45127 predictions, 25859 (equivalent to 24777 + 1082) were for the not late class. Let's explain each of the numbers in the boxes: `For 19268 observations (from X_test) the model predicted late and the label was actually late. For 0 observations (from X_test) the model predicted late but the label was actually not late. For 1082 observations (from X_test) the model predicted not late but the label was actually late. For 24777 observations (from X_test) the model predicted not late and it was actually not late. DASCM_Report_Group 05 29 Precision: for our predictions of the positive class (not late in our example), how often is it correct? In sklearn, we used the precision_score method from the metrics module. Precision: 0.958158 Recall: when the actual label is positive (not late in our example), how often did we predict correctly? Recall: 1.0 In sklearn, we used the recall method from the metrics module. F1 Score: combines precision and recall into one metric by calculating the mean between those two. F1 Score: 0.978632 Accuracy: It measures how many observations, both positive and negative, were correctly classified. Accuracy: 0.976023 Area under the ROC Curve (AUC): is the probability that the model ranks a random positive example more highly than a random negative example. ROC AUC: 0.973415 DASCM_Report_Group 05 30 Figure 26: XG Booster Next method was XGBooster . There’s no need for feature scaling, however we did label encoding on the training dataset using scikit learn, then we run the model on training set. In Fig 29 we show the confusion matrix. It’s the same interpretation that we did for Logistic Regression. We got the following results Table 3 Accuracy Precision Recall F1 score ROC AUC 0.975846 0.958074 0.999758 0.978472 0.973245 The final model used was Kernel SVM Same as Logistic Regression we did Standard Scaling then run the model in the training set.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "967577bb"}
{"text": "same interpretation that we did for Logistic Regression. We got the following results Table 3 Accuracy Precision Recall F1 score ROC AUC 0.975846 0.958074 0.999758 0.978472 0.973245 The final model used was Kernel SVM Same as Logistic Regression we did Standard Scaling then run the model in the training set. DASCM_Report_Group 05 31 Figure 27: Kernel SVM Confusion Matrix We got the following results for the metrics. Table 4: Kernel SVM Results Accuracy Precision Recall F1 score ROC AUC 0.975846 0.958074 0.999758 0.978472 0.973245 Table 5: Comparison Table Models Accuracy Precision Recall F1 score ROC AUC Logistic Regression 0.976023 0.958158 1.0 0.978632 0.973415 Kernel SVM 0.975846 0.958074 0.999758 0.978472 0.973245 XGBooster 0.977242 0.960593 0.999556 0.979687 0.974815 Based on Table 5 we can say that XGBooster performed the best in all metrics except Recall, where Logistic Regression shine with a score of 1.0. While Kernel SVM performed the worst but still a good score in general.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "8097089d"}
{"text": "0.975846 0.958074 0.999758 0.978472 0.973245 XGBooster 0.977242 0.960593 0.999556 0.979687 0.974815 Based on Table 5 we can say that XGBooster performed the best in all metrics except Recall, where Logistic Regression shine with a score of 1.0. While Kernel SVM performed the worst but still a good score in general. DASCM_Report_Group 05 32 2.6 Discussion It have been observed that when compared with other classification machine learning models XGBooster did a good job of identifying orders with later delivery. For further study, all the machine learning models can be compared with different datasets to confirm whether the same machine learning models are performing better or not. We could also try to tune the parameters in order to get a better result. Something interesting would be to combine different features to see if we could boost the performance. 2.7 Business and Technological Implication With the help of the current analysis, businesses can gain valuable insights into their operations, customers, and markets. This information can be used to inform strategic decisions, identify potential areas of improvement, and develop effective marketing strategies. Additionally, businesses can use the analysis to determine customer needs and preferences, optimize pricing and product offerings, and develop new products and services. By leveraging the data gathered through analysis, businesses can gain a competitive edge and increase their profitability based on the following: Firstly, by carefully analysing the right times for marketing and advertising, the declining revenue can be increased. This analysis can help to determine the best times to reach out to potential customers and create more awareness of the product or service. With the right marketing strategy, businesses can gain more customers and increase their revenue. Secondly, through the investigation of the late delivery reasons with classification model, it has been found that the standard type of shipment and the payment methods are the two primary causes of late shipments. This is due to the fact that the standard type of shipment is not as efficient as the express type, and the payment methods can cause delays in the processing of the order. As a result, the customer’s order may not be delivered on time. To address this issue, companies must remodel their internal processing by improving the promised delivery date more accurately so that the actual delivery shall not vary from that. Moreover, company cannot rely on the express shipment considering the sustainability and Co2 emission factors. Hence, less Late deliveries results in customer satisfaction", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "a6584228"}
{"text": "address this issue, companies must remodel their internal processing by improving the promised delivery date more accurately so that the actual delivery shall not vary from that. Moreover, company cannot rely on the express shipment considering the sustainability and Co2 emission factors. Hence, less Late deliveries results in customer satisfaction DASCM_Report_Group 05 33 Lastly, it has been observed that the majority of deliveries are getting delayed due to the debit card payment method. This is because it takes time for the payment to be processed in the system. To tackle this issue, businesses can include more payment methods such as Paypal or Klarna, which can increase customer ease of access and ultimately improve business , in turn, improve the business. By doing this, customers can have more faster payment options, thus leading to an improved delivery process and a better customer experience. 2.8 Business Challenges Data Co. can face a variety of challenges when implementing the given solutions to their problems. These challenges can include difficulty in understanding the technical aspects of the solution, a lack of resources to properly implement the solution, or an inability to effectively communicate the solution to stakeholders. Additionally, it may struggle to stay within budget since its revenue is already declining. With fewer resources available, it must make difficult decisions about where to allocate their limited funds. This could mean cutting back on employee benefits, reducing staff, or reducing the amount of money spent on marketing and advertising. It could also mean reducing the amount of money spent on research and development or cutting back on the number of products and services offered. Furthermore, DataCo. must also consider how to best manage their cash flow. In addition, companies must consider how to best use their resources to ensure their long-term success because in order to remain competitive in their industry, as well as to adapt their existing processes to the new solution. Finally, there may be resistance from employees to the new solution, which could impede the success of the implementation. All of these challenges can be difficult to overcome, but with the right preparation and resources, businesses can successfully implement the solutions they need. However, considering the shipment method if Data Co. focus solely on same day delivery to overcome late delivery issues, this might not be beneficial in the long run due to the Sustainable Development Goals (SDGs). It must take them into consideration when making decisions about their operations. Same day delivery may be beneficial in the short-term, but in the long-term, businesses must consider the impact that their operations have on the environment and society. Therefore, focusing only on same day delivery to address late delivery issues may not be the most sustainable solution in the long run but adjusting their internal timeline can be.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "a342d4cb"}
{"text": "the short-term, but in the long-term, businesses must consider the impact that their operations have on the environment and society. Therefore, focusing only on same day delivery to address late delivery issues may not be the most sustainable solution in the long run but adjusting their internal timeline can be. DASCM_Report_Group 05 34 Lastly, company is doing business in many countries in all continents, it is important to consider the payment partners. In some countries, certain payment partners may not be available, such as Paypal. This means that one must look for alternative payment partners in order to ensure that customers in those countries can make payments without any issues. Therefore, it is important to research the payment partners available in each country before deciding which ones to partner with. 2.9 Business Model Canvas Figure 28 Bussiness Model Canvas DASCM_Report_Group 05 35 3 Supply Chain Scenario 3: CNC-Milling Machine_Production data 3.1 Scenario Description The given dataset is a dataset of the power consumption of a programmable CNC milling machine with the corresponding process features. A row represents a process with its properties, timestamps and power consumption 3.2 Dataset Description Table 6 CNC Machine dataset Feature Description start_time Timestamp (UTC) when the process starts. end_time Timestamp (UTC) when the process finishes. processing_time Duration of the process [s]. average_power_consumption Average power consumption during the process [Watt]. number_of_missing_datapoints Number of missing power measurement points during the process. The power data is collected with 1 minute interval. raw_volume Volume of the processed material [mm3]. number_of_lines_of_code Number of lines of G/NC code that execute the process. number_tool_changes Number of tool changes during the process, e.g. drill head -> milling head or larger drill head -> smaller drill head. It does not refer to replacement through wear, just different tools for different tasks. number_of_travels_to_machine _ zero_point_in_rapid_traverse Number of travels/movements of the tools from the initial state (zero point of the machine’s coordinate system) to rapid repetitive/traverse process.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "176a1535"}
{"text": "head -> milling head or larger drill head -> smaller drill head. It does not refer to replacement through wear, just different tools for different tasks. number_of_travels_to_machine _ zero_point_in_rapid_traverse Number of travels/movements of the tools from the initial state (zero point of the machine’s coordinate system) to rapid repetitive/traverse process. DASCM_Report_Group 05 36 number_axis_rotations Number of changes in the position of the axis of rotation of the tool (e.g. drill head). speed Rotation speed of the tool, e.g. drill head [1/min] tool_diameter Diameter of the tool used for a production step [mm]. cutting_length Length of the cutting edges of the tool used for a production step [mm]. number_of_cutting_edges Number of cutting edges of the tool used for a production step. cutting_speed Speed at which the cutting edge of the tool used for a production step cuts into the material [m/s]. feed_per_tooth Portion of distance during feed into the component per cutting tooth of the tool used for a production step per revolution [mm]. feedrate Speed at which the tool cuts through/into the material in [mm/min]. 3.2.1 Dependent Variables: Processing time and average power consumption 3.2.2 Independent Variables: Same as mentioned in the data description except Processing time and average power consumption. 3.3 Exploratory Data Analysis In order to understand the structure of the dataset and make the machine learning process easier and leads to better result we used statistics and visualization to analyze and identify trends in data sets. First looking at the shape of the dataset we have 220 observations and 17 columns. All the variables are numerical values which is convenient reducing on the preprocessing task (Fig 29). DASCM_Report_Group 05 37 Figure 29: Statistic about the data type of the dataset. Going on with the analysis we used different visualization to help us see any relation, trends between the target variables and independent variables. DASCM_Report_Group 05 38 Figure 30: Statistic about missing values Going on with the analysis we used different visualization to help us see any relation, trends between the target variables and independent variables. DASCM_Report_Group 05 39 Figure 31: Correlation heatmap of every variable. Initially we checked the correlation between the attributes using correlation heatmap Looking at the figure (Fig 31) We can observe that start_time and end_time have a perfect positive relationship, same for number_of_travels_to_machine_zero_point_in_rapid_traverse and number_axis_rotations taking into account the dependent variable we can say that processing_time has a strong positive relationship with number_axis_rotations, DASCM_Report_Group 05 40 number_of_travels_to_machine_zero_point_in_rapid_traverse, number_tool_changes and number_of_missing_datapoint. Meanwhile average_power_consumption has a weak correlation with all the other variables. Figure 32: Scatter plot of the target variables against some predictors. Here we notice that there's a linear relationship between processing_time number_axis_rotations and weighed_speed. However, there's no linear relationship between average_power_consumption weighed_speed, processing_time and number_of_lines_of_code. Finally start_time and end_time are completely linear.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "a522140a"}
{"text": "a weak correlation with all the other variables. Figure 32: Scatter plot of the target variables against some predictors. Here we notice that there's a linear relationship between processing_time number_axis_rotations and weighed_speed. However, there's no linear relationship between average_power_consumption weighed_speed, processing_time and number_of_lines_of_code. Finally start_time and end_time are completely linear. DASCM_Report_Group 05 41 Figure 33: Box plot of the independent and dependent variables. It can be seen that some variables present some outliers like weighted_speed, weighted_number_of_cutting_edges, number_of_missing_data_points. Also values in raw_volume doesn't vary that much. Histogram Plot We wanted to see the distribution for some variables so we used histogram plot for that In (Fig 34) some variables are right skewed like number_missing_data_points, weighted_feedrate, processing time. start_time and end_time are left skewed. There are no variables with normal distribution. Weighted_cutting_length is non-symmetric bimodal distribution. DASCM_Report_Group 05 42 Line Plot We intended to see how average_power_consumption would change during those 4 months from April to July. There were no clear patterns but the 2 highest energies consumed was on May and June. Figure 35: line plot to see evolution of average power consumption Bar plot Figure 34: Histogram DASCM_Report_Group 05 43 Figure 36: Bar Plot between average power consumption and processing time With a bar plot we tried to look if the energy would raise with the increasement of time. However, it wasn’t the case as you can see in Fig 36. Figure 37: Average power consumption over time for the same material of 136659600 mm3 around 14 minutes it produced less than 0.5 x 105 Watts and in only 2 minutes it produced more than 1 x 105 Watts which is more in less time. Now we took a different approach by grouping same material with the exact same parameters and same time. Still there was no clear relation between the energy over time as we can see in Fig 37.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "cbdbb835"}
{"text": "it produced more than 1 x 105 Watts which is more in less time. Now we took a different approach by grouping same material with the exact same parameters and same time. Still there was no clear relation between the energy over time as we can see in Fig 37. DASCM_Report_Group 05 44 We have gained enough insights with those plots and our data was ready for modeling. 3.4 Data Preprocessing Being that the data was already pre-processed there wasn’t that much to do in this area however we did some pre-processing step in the next section by dropping some columns after doing feature importance. 3.5 Model Building and Result Description We first decided to do the modeling by considering all the variables. After selecting the independent and target variables. We split the dataset into training set and test set. The first model we used was Random Forest. With RF there was no need for feature scaling, so we went straight on training the model into the training set and selecting 10 for the number of trees. We used mutli-target regression to predict both targets simultaneously. The method was done using scikit learn. # Training the Random Forest Regression model on the whole dataset from sklearn.ensemble import RandomForestRegressor regressor = RandomForestRegressor(n_estimators = 10, random_state = 0) regressor.fit(X_train, y_train) In order to evaluate our model, we used 4 different type of performance metric. Which were R square, Mean square error (MSE) and Root mean square error (RMSE) and Mean absolute error (MAE) R-squared determines how strong the linear relationship is between two variables. It’s represented by value between 0 and 1, where 1.0 indicates a perfect fit for the model meanwhile a value of 0.0 would indicate that it didn’t correctly model the data (Andrew, 2021). 𝑅2 = (1 𝑁) ∑[(𝑥𝑖–x̄) × (𝑦𝑖–ȳ)] (𝜎𝑥× 𝜎𝑦)2 (𝟏) DASCM_Report_Group 05 45 Mean square error (MSE) calculate the average squared distinction between the actual and predicted values. The larger the number the larger the error. (Jim, n.d.) 𝑀𝑆𝐸= 1 𝑁∑ 𝑏 𝑖=1 (𝑦𝑖−ȳ𝑖)2 (𝟐) Root mean square error is calculated as the square root of the average squared distance between the actual and the predicted values (Zack, 2021) 𝑅𝑀𝑆𝐸= 1 𝑁√∑ 𝑏 𝑖=1 (𝑦𝑖−ȳ𝑖)2 (𝟑) Mean absolute error (MAE) score is measured as the average of the absolute error values. 𝑀𝐴𝐸= 1 𝑁∑ 𝑏 𝑖=1 |𝑦𝑖−ȳ𝑖| (𝟒) Using those metrics, we obtained the following result for Random Forest Table 7: Different type of performance Metrics for Random Forest Model R-2- 1 R- 2 MAE 1 MAE 2 MSE 1 MSE 2 RMSE 1 RMSE 2 Random Forest 0.956491 0.139179 181.719697 9.396825e+15 156558.553030 1.515796e+32 156558.553030 1.515796e+32 Next model was Decision Tree. Same as Random Forest we split the data set into training and set and test set and run the multi-target regression on both dependent variables. We obtained the following result after evaluation.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "96602905"}
{"text": "1 RMSE 2 Random Forest 0.956491 0.139179 181.719697 9.396825e+15 156558.553030 1.515796e+32 156558.553030 1.515796e+32 Next model was Decision Tree. Same as Random Forest we split the data set into training and set and test set and run the multi-target regression on both dependent variables. We obtained the following result after evaluation. DASCM_Report_Group 05 46 Table 8: Different type of performance Metrics for Decision Tree Model R- square 1 R- square 2 MAE 1 MAE 2 MSE 1 MSE 2 RMSE 1 RMSE 2 Decision Tree 0.931603 - 0.130642 191.818182 9.419117e+15 246113.636364 1.990918e+32 246113.636364 1.990918e+32 Lastly, we used Support Vector Regression. After splitting into training and test set. We proceed with feature scaling and then run the model in the training set by using a multi-output regression. In the following table we can see the result we obtained. Table 9: Different type of performance Metrics for Support Vector Model R-square 1 R-square 2 MAE 1 MAE 2 MSE 1 MSE 2 RMSE 1 RMSE 2 Support Vector 0.947969 0.117954 212.293465 8.414137e+15 187223.166361 1.553171e+32 187223.166361 1.553171e+32 So far, we can say for processing time Random Forest performed the best among all models and metrics, support vector comes in second place with Decision Tree as worst. But overall, they all give a good performance. For average_power_consumption Random Forest also performed the best among all models however it still a really poor score, same for the other two models. DASCM_Report_Group 05 47 Figure 38: Feature Importance At this stage we did feature importance, we can see that raw_volume doesn’t bring too much to the target variables. So, we decided to drop it. Also end_time and start_time present similar values analyzing the correlation heatmap. Saying that we drop end_time. With those 2 columns dropped we proceed with the whole process of modeling. In table 4 we show the results for the metrics. Table 10: Different type of performance Metrics for all models after cleaning R-square 1 R-square 2 MAE 1 MAE 2 MSE 1 MSE 2 RMSE 1 RMSE 2 Random Forest 0.961658 0.124216 167.436364 9.522955e+15 137965.578182 1.542144e+32 137965.578182 1.542144e+32 Support Vector 0.938621 0.129229 221.311828 8.574613e+15 220859.223265 1.533317e+32 220859.223265 1.533317e+32 Decision Tree 0.931729 - 0.130443 189.545455 9.404719e+15 245659.090909 1.990567e+32 245659.090909 1.990567e+32", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "efff49e4"}
{"text": "models after cleaning R-square 1 R-square 2 MAE 1 MAE 2 MSE 1 MSE 2 RMSE 1 RMSE 2 Random Forest 0.961658 0.124216 167.436364 9.522955e+15 137965.578182 1.542144e+32 137965.578182 1.542144e+32 Support Vector 0.938621 0.129229 221.311828 8.574613e+15 220859.223265 1.533317e+32 220859.223265 1.533317e+32 Decision Tree 0.931729 - 0.130443 189.545455 9.404719e+15 245659.090909 1.990567e+32 245659.090909 1.990567e+32 DASCM_Report_Group 05 48 Table 11: Different type of performance Metrics for all models before cleaning R-square 1 R-square 2 MAE 1 MAE 2 MSE 1 MSE 2 RMSE 1 RMSE 2 Random Forest 0.956491 0.139179 181.719697 9.396825e+15 156558.553030 1.515796e+32 156558.553030 1.515796e+32 Support Vector 0.947969 0.117954 212.293465 8.414137e+15 187223.166361 1.553171e+32 187223.166361 1.553171e+32 Decision Tree 0.931603 - 0.130642 191.818182 9.419117e+15 246113.636364 1.990918e+32 246113.636364 1.990918e+32 Comparing Table 10 and Table 11 we can say that after doing the cleaning only Random Forest gained a considerable improvement. However, Support Vector went down in performance and came at last behind Decision Tree. There was minimum improvement for Decision Tree but not really impactful. Based on that we can say that there isn’t that much difference in performance after cleaning. 3.6 Discussion Using a regression model for a CNC machine can make it easier to predict which product will have a higher processing time. This prediction can help to identify which products may require more resources or more time to process, allowing for better scheduling and planning. By having this information, it can help to reduce the amount of time and resources spent on processing each product, resulting in a more efficient and cost-effective production process. Moreover, with the right data and analysis, predicting power consumption can be a valuable tool for optimizing energy consumption. By understanding the power consumption of CNC machines, their timings can be adjusted to take advantage of dynamic power rates. This can help to reduce energy costs and ensure that the machines are running efficiently. Additionally, understanding the power consumption of CNC machines can help to identify potential problems before they become major issues, allowing for proactive maintenance and repairs.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "ad90992d"}
{"text": "can be adjusted to take advantage of dynamic power rates. This can help to reduce energy costs and ensure that the machines are running efficiently. Additionally, understanding the power consumption of CNC machines can help to identify potential problems before they become major issues, allowing for proactive maintenance and repairs. DASCM_Report_Group 05 49 3.7 Business and Technological Implication In order to decrease processing time, code optimization is required. This means that the number of travels should be reduced, which will result in axis rotation optimization. By optimizing the code and reducing number of line of code for short time, the processing time can be significantly reduced, making the program more efficient. Moreover, when purchasing a machine, it is important to consider the various models available and determine which one is best suited for the task. Applying this type of ML model can help in making an informed decision. Different machines may have different features, so it is important to compare the models to determine which one is the most suitable. By using this type of ML model, one can compare the features of different machines and select the one that is most suitable for the task. 3.7.1 Business Disruptions: CNC machines are becoming increasingly popular in the manufacturing industry, as they can provide a more efficient and cost-effective production process. They are much easier to program than NC (Numerical Control) machines, as changes can be made directly in the program line. This is not possible with NC machines, as editing is not possible. This makes CNC machines much more flexible and easier to use than NC machines. Moreover, they are able to produce parts with a higher level of accuracy and precision than traditional NC machines, which leads to lower production costs. This cost savings is one of the main reasons why CNC machines are replacing standard NC machines in many industries. Also, they offer a higher degree of automation, which makes them more efficient and reduces the need for manual labour. This leads to further cost savings, which makes CNC machines a more attractive option for businesses looking to reduce their production costs.", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "c59f7154"}
{"text": "are replacing standard NC machines in many industries. Also, they offer a higher degree of automation, which makes them more efficient and reduces the need for manual labour. This leads to further cost savings, which makes CNC machines a more attractive option for businesses looking to reduce their production costs. DASCM_Report_Group 05 50 4 Conclusions Future Outlook, and Reflection We have gained a lot of insight and understanding from the combination of supply chain and data analytics. By combining the two, we are able to gain insights into the entire supply chain process, from the problem statement, identify areas of improvement and providing solutions. Through this research we understood that data analytics can help us identify trends, problems, and opportunities for improvement, allowing us to make better decisions and create more efficient supply chain processes. Additionally, it can help us better understand customer needs and preferences, allowing us to create better products and services. Overall, the combination of supply chain and data analytics has been invaluable in helping us gain a better understanding of our operations and how to make them more efficient. Lastly, We have learned through this course that Machine Learning (ML) models are powerful tools that can be used to achieve a variety of results. Depending on the task at hand, different ML models can be used to achieve the desired outcome. For example, if the goal is to classify data, a supervised learning model such as a Support Vector Machine (SVM) or a Decision Tree can be used. If the goal is to predict a continuous value, a regression model such as Linear Regression or a Random Forest can be used. For clustering tasks, an unsupervised learning model such as K-Means or Hierarchical Clustering can be used. Additionally, for tasks such as natural language processing (NLP), a deep learning model such as a Recurrent Neural Network (RNN) or a Convolutional Neural Network (CNN) can be used. No matter the task, there is likely an ML model that can be used to achieve the desired result. DASCM_Report_Group 05 51 Bibliography Constante, F., Silva, F., Pereira, A., 2019. DataCo SMART SUPPLY CHAIN FOR BIG DATA ANALYSIS 5. https://doi.org/10.17632/8gx2fvg2k6.5 Frost, J., 2021. Mean Squared Error (MSE). Stat. Jim. URL https://statisticsbyjim.com/regression/mean-squared-error-mse/ (accessed 12.10.22). Online retail dataset [WWW Document], n.d. URL https://www.kaggle.com/datasets/lakshmi25npathi/online-retail-dataset (accessed 12.10.22). payment not received icon - Google Search [WWW Document], n.d. URL https://www.google.com/search?q=payment+not+received+icon&tbm=isch&chips=q: payment+not+received+icon,online_chips:payment+failure:G8_UhLinejc%3D&hl=en &sa=X&ved=2ahUKEwiE9vmz3tP7AhVthP0HHXcGBhcQ4lYoBHoECAEQLQ&bi w=1519&bih=722#imgrc=kJ0EWH4Kie5iIM (accessed 11.29.22). Schneider, P., Xhafa, F. (Eds.), 2022. Contents, in: Anomaly Detection and Complex Event Processing over IoT Data Streams. Academic Press, pp. vii–xii. https://doi.org/10.1016/B978-0-12-823818-9.00005-5 Schoppe, I., 2020. Heute im Sonder-Livestream: So baust du 2021 dein Online Business auf. Gründer.de. URL https://www.gruender.de/gruendung/sonder-livestream-online- business/ (accessed 11.28.22). Zach, 2021. How to Interpret Root Mean Square Error (RMSE). Statology. URL https://www.statology.org/how-to-interpret-rmse/ (accessed 12.10.22).", "source": "Repo:DASCM:DASCM_Report.pdf", "section": "DASCM", "hash": "22549af6"}
{"text": "## This a unit test CI python practice proj using add and sub function", "source": "Repo:CI_unit_test:README.md", "section": "CI_unit_test", "hash": "c60104cc"}
{"text": "from src.math_operations import add, sub def test_add(): assert add(2,3)==5 assert add(-4,5)==1 assert add(9,1)==10 def test_sub(): assert sub(2,3)==-1 assert sub(-4,5)==-9 assert sub(9,1)==8", "source": "Repo:CI_unit_test:tests/test_operations.py", "section": "CI_unit_test", "hash": "c099c4cf"}
{"text": "name: Python CI on: push: branches: - 'no-ci-branch' pull_request: branches: - 'no-ci-branch' jobs: test: runs-on: ubuntu-latest steps: # Check out code from the repository - name: Check out the code uses: actions/checkout@v4 # Setup python Environment - name: Set up python uses: actions/setup-python@v3 with: python-version: \"3.10\" # Installing dependencies - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt # Run Test - name: Run test run: pytest", "source": "Repo:CI_unit_test:.github/workflows/python-app.yml", "section": "CI_unit_test", "hash": "a29e6257"}
